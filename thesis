\documentclass[11 pt, a4paper]{report}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\usepackage[version=3]{mhchem}
\usepackage{amsmath}
\usepackage{color}
\usepackage{eurosym}
\usepackage{enumitem}
\usepackage{amsfonts}
\usepackage{a4wide}
\usepackage{url}
\usepackage{cite}
\addtolength{\voffset}{-1cm} % Top margin
\addtolength{\textheight}{2cm} % Bottom margin
\addtolength{\hoffset}{-1 cm} % Left margin 
\addtolength{\textwidth}{16 mm} % Right margin
\usepackage{amsmath} 
\usepackage{siunitx}
\usepackage{microtype}

\usepackage{fancyhdr}  % headers
\usepackage{multirow}
\usepackage[table,xcdraw]{xcolor}
\usepackage{lmodern}  % Latin Modern qui supporte small caps



%To make an index:
\usepackage{imakeidx}
\usepackage[acronym]{glossaries}
\setcounter{secnumdepth}{3} % numérotation jusqu'à subsubsection
\setcounter{tocdepth}{3}    % table des matières jusqu'à subsubsection
\usepackage{cite} 
\makeglossaries

\newglossaryentry{fasttext}{
    name=FastText,
    description={Model developed by Facebook AI for word vector representations taking subwords into account}
}

\newglossaryentry{biowordvec}{
    name={BioWordVec},
    description={Specialized word embeddings for the biomedical domain, pretrained on large biomedical corpora by combining Word2Vec and FastText approaches, enabling improved semantic representation of medical and scientific terms}
}

\newacronym{tln}{NLP}{Natural Language Processing}
\newacronym{nlp}{NLP}{Natural Language Processing}
\newacronym{ia}{AI}{Artificial Intelligence}
\newacronym{rnn}{RNN}{Recurrent Neural Network}
\newacronym{lstm}{LSTM}{Long Short-Term Memory}
\newacronym{gru}{GRU}{Gated Recurrent Unit}
\newacronym{pubmedbert}{PubMedBERT}{PubMed BERT, a pretrained language model specific to the biomedical domain}
\newacronym{biobert}{BioBERT}{BioBERT, a BERT variant for the biomedical domain}
\newacronym{smote}{SMOTE}{Synthetic Minority Over-sampling Technique (technique for oversampling minority classes)}
\newacronym{bert}{BERT}{Bidirectional Encoder Representations from Transformers}
\newacronym{glove}{GloVe}{Global Vectors for Word Representation}
\newacronym{w2v}{Word2Vec}{Method for learning word vector representations developed by Mikolov et al.}
\newacronym{fsl}{FSL}{Few-Shot Learning}
\newacronym{auc}{AUC}{Area Under the Curve}
\newacronym{cnn}{CNN}{Convolutional Neural Networks}
\newacronym{tsne}{t-SNE}{t-distributed Stochastic Neighbor Embedding}



\makeindex[name=Tot,title={Index}]
\makeindex
% in the text : \index[Tot]{xxx}
% in the place where you want to put the index: \printindex[Tot]
% we can define a name other than "Tot" it's random, same for the index name we can put another title like {title= TFE Index}



\makeatletter
\def\clap#1{\hbox to 0pt{\hss #1\hss}}%
\def\ligne#1{%
\hbox to \hsize{%
\vbox{\centering #1}}}%
\def\haut#1#2#3{%
\hbox to \hsize{%
\rlap{\vtop{\raggedright #1}}%
\hss
\clap{\vtop{\centering #2}}%
\hss
\llap{\vtop{\raggedleft #3}}}}%
\def\bas#1#2#3{%
\hbox to \hsize{%
\rlap{\vbox{\raggedright #1}}%
\hss
\clap{\vbox{\centering #2}}%
\hss
\llap{\vbox{\raggedleft #3}}}}%
\def\maketitle{%
\thispagestyle{empty}\vbox to \vsize{%
\haut{}{\@blurb}{}
\vfill
\vspace{1cm}
\begin{flushleft}
\usefont{OT1}{ptm}{m}{n}
\vspace{1cm}
\Huge \@title
\end{flushleft}
\par
\hrule height 4pt
\par
\begin{flushleft}
\usefont{OT1}{phv}{m}{n}
\vspace{1cm}
\LARGE \@author
\par
\end{flushleft}
\vspace{0.71cm}
\begin{center}
\begin{LARGE}
\mbox{Thesis presented to obtain the degree of :} \\
\textbf{Master of Data Science and Engineering} \\ 
\vspace{1.3cm}
Thesis supervisor : \\
 \textsc{Ashwin} Ittoo \\
 
 
 
 
 

\end{LARGE}
\end{center}
%\vfill
\vspace{1.3cm}
\begin{flushright}
{\Large Academic year:  \textbf{2024 - 2025}}
\end{flushright}
\vfill





\vspace{2cm}
\bas{}{}{}
}%

\cleardoublepage
}
\def\date#1{\def\@date{#1}}
\def\author#1{\def\@author{#1}}
\def\title#1{\def\@title{#1}}
\def\location#1{\def\@location{#1}}
\def\blurb#1{\def\@blurb{#1}}
\date{Date}
\author{}
\title{}
\location{Liège}\blurb{}




\makeatother
\title{ \centering \normalfont{\Huge{\textbf{Biomedical Text Classification using LSTM, GRU, and Bahdanau Attention}}} \\} 
\author{\normalfont{\textsc{Mvomo Eto} Wilfried}}
\location{Liège}



\makeatletter

\newskip\@bigflushglue \@bigflushglue = -100pt plus 1fil
\def\bigcenter{\trivlist \bigcentering\item\relax}
\def\bigcentering{\let\\\@centercr\rightskip\@bigflushglue%
\leftskip\@bigflushglue
\parindent\z@\parfillskip\z@skip}
\def\endbigcenter{\endtrivlist}

\makeatother



\pagestyle{fancy}



\blurb{%
\vfill
\begin{bigcenter} \includegraphics[scale=0.87]{Logo} \end{bigcenter} %% Logo à changer selon du TFE
\vfill
}%

\makeatletter

\newskip\@bigflushglue \@bigflushglue = -100pt plus 1fil

\def\bigcenter{\trivlist \bigcentering\item\relax}
\def\bigcentering{\let\\\@centercr\rightskip\@bigflushglue%
\leftskip\@bigflushglue
\parindent\z@\parfillskip\z@skip}
\def\endbigcenter{\endtrivlist}

\makeatother

\setlength{\headheight}{14pt}

\begin{document}
\maketitle

\lhead{}
\rhead{}



\newpage
\pagenumbering{roman}
%\setcounter{page}{0}
%\thispagestyle{empty}

\chapter*{Acknowledgments} % star for no numbering in the table of contents

\textit{I sincerely thank Jehovah, the Most Merciful and Full of Wisdom, for granting me the strength and perseverance necessary to pursue my studies and complete this thesis. His infinite mercy has been a constant source of guidance in all my achievements, and I am eternally grateful to Him. I express my deepest gratitude to Professor Ashwin Ittoo for his wise advice, encouragement, and unwavering support throughout this research. His supervision played a crucial role in consolidating my knowledge and the quality of this work. I also thank all the professors and assistants at the University of Liège for their teaching and expertise, which greatly contributed to the success of this thesis. I would also like to express my heartfelt appreciation to my parents for their constant support and for creating an environment conducive to my studies. Their encouragement has always been an essential source of motivation. Finally, I am grateful to my friends for their friendship, shared experiences, and mutual support, which have enriched both my academic and personal journey. The camaraderie and help I received made this adventure truly rewarding, and for that, I am deeply thankful.} 



\newpage



\vspace{2cm}

\section*{Summary}

Biomedical text classification is a complex task due to the specialized terminology and the elaborate linguistic structures inherent to scientific literature. This thesis evaluates the effectiveness of deep learning models — \gls{lstm}, \gls{gru}, and bidirectional \gls{gru} with Bahdanau attention mechanism — for classifying biomedical abstracts according to disease categories. Using datasets extracted from PubMed, experiments were conducted on binary classification tasks (malaria vs. non-malaria) and multiclass classification tasks (9 diseases).

We start by training models with lexical embeddings learned from scratch to establish a baseline. Next, we examine the impact of more advanced approaches, including pretrained static vectors (\gls{glove}, \gls{fasttext}) and specialized transformer-based models — notably \gls{bert} and \gls{pubmedbert}. Although these pretrained representations significantly improve accuracy and contextual understanding, especially when combined with attention mechanisms, they also increase complexity and training time. This work thus analyzes the trade-off between performance and computational cost. Techniques such as \gls{smote}, Borderline-\gls{smote}, and weighted loss functions are employed to address class imbalance.

Finally, few-shot learning (\gls{fsl}) approaches are explored on the best models to evaluate their ability to generalize to rare diseases. The results show that these models maintain good performance even in low-resource scenarios, highlighting their potential for rare disease classification.







%% Tous les 3 ci-dessous se feront automatiquement

\newpage
\lhead{TABLE OF CONTENTS}
\tableofcontents

\newpage
\printglossary[type=\acronymtype]


\newpage
\lhead{LIST OF FIGURES}
\listoffigures
\newpage
\lhead{LIST OF TABLES}
\listoftables


\newpage



\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
\pagenumbering{arabic}

\lhead{INTRODUCTION} %% met le titre au dessus de la page

Automatic analysis and understanding of human language have become key elements of modern \gls{ia}, with \gls{tln} emerging as a central discipline at the intersection of computer science, linguistics, and cognitive science. One of the fundamental tasks in \gls{tln} is text classification, which involves assigning predefined categories or labels to textual data. This task supports a wide range of practical applications, such as spam detection in emails, sentiment analysis in product reviews, topic identification in news articles, and annotation of legal and financial documents. With the continuous proliferation of digital information, the ability to automatically and accurately organize large volumes of unstructured text has become essential across many domains.

Despite significant advances in \gls{tln}, text classification remains challenging due to the inherent ambiguity of natural language, complex contextual dependencies, syntactic variability, and semantic subtleties. In this context, biomedical text classification presents a particularly difficult case. The biomedical domain is characterized by highly specialized vocabulary, intricate grammatical structures, and rapidly evolving terminology driven by ongoing scientific discoveries. Scientific publications, clinical trial reports, and medical records are often rich in domain-specific information, making them difficult to process using general-purpose language models or rule-based systems. Moreover, the scale and speed of biomedical literature production---as exemplified by databases such as PubMed---demand automated solutions capable of efficiently and accurately processing and categorizing millions of abstracts.

This study focuses on two main biomedical text classification tasks extracted from the PubMed database. The first task is a binary classification with the labels Malaria versus Non-Malaria (including subcategories such as Alzheimer’s and Dengue). This dataset contains 29,997 abstracts and exhibits class imbalance. The second task is a multiclass classification involving nine disease categories, including tuberculosis, cholera, lupus, and cystic fibrosis, across 42,879 abstracts, also imbalanced. All documents cover the period from 1950 to 2024.

Effective biomedical text classification could yield significant benefits. Accurate categorization of scientific abstracts by disease, treatment, or clinical outcomes can facilitate systematic literature reviews, support evidence-based clinical decision-making, improve epidemiological surveillance, and accelerate the identification of emerging research trends. However, traditional machine learning methods often struggle with the complexity of biomedical texts and severe class imbalance, particularly for rare diseases that are underrepresented compared to more common ones.

This thesis aims to address these challenges by investigating the effectiveness of advanced deep learning architectures, specifically \gls{lstm}, \gls{gru}, and bidirectional \gls{gru} with Bahdanau Attention, in the context of biomedical text classification. These models, designed to handle sequential data and capture long-range dependencies, are well suited to the linguistic intricacies of biomedical literature. However, their architectural complexity and associated computational costs---notably in terms of training time and resource consumption---raise a critical research question: \textit{Are the predictive performance gains offered by more complex deep learning architectures justified by their additional training time and computational overhead?}

A second key aspect of this study is the examination of textual representations used to train these models. On one hand, traditional embeddings such as \gls{glove} and \gls{fasttext}, pretrained on general corpora, capture broad semantic relationships. On the other hand, contextual embeddings like \gls{biobert} and \gls{pubmedbert}, pretrained on biomedical corpora, incorporate domain-specific knowledge and are expected to improve classification performance. Comparing these two types of embeddings is crucial to evaluate their impact on predictive accuracy, training efficiency, and inference speed, leading to the second research question: \textit{How do \gls{glove} and \gls{fasttext} embeddings (pretrained on general corpora) compare with \gls{biobert} and \gls{pubmedbert} embeddings (pretrained on biomedical corpora) in terms of predictive performance and computational efficiency?}

Finally, this thesis investigates the generalization capabilities of these models in low-resource scenarios, particularly concerning rare diseases with limited labeled data. This challenge is addressed through few-shot learning (\gls{fsl}) and data augmentation strategies aimed at enhancing classification performance under data-scarce conditions. The third research question is thus: \textit{To what extent do the best-performing models generalize to underrepresented disease categories, and can few-shot learning methods effectively improve classification in low-resource scenarios?}

Through these investigations, this thesis not only evaluates the performance of various deep learning approaches for biomedical text classification but also explores the trade-offs involved in their deployment in real-world applications. The ultimate goal is to identify robust, scalable, and interpretable models suitable for biomedical research and healthcare-related tasks.

\newpage

\chapter{Theoretical and Technical Background}
\addcontentsline{toc}{chapter}{Theoretical and Technical Background}

\lhead{\MakeUppercase{Theoretical and Technical Background}}
\section{NLP in the Biomedical Domain}

\gls{nlp} is a subfield of \gls{ia} that aims to enable computers to understand, interpret, and generate human language. While NLP has achieved success in many areas, it presents particular challenges when applied to the biomedical domain. Biomedical texts, such as scientific articles, publication abstracts, research reports, and electronic medical records, are characterized by highly specialized vocabulary, complex grammatical structures, and rapidly evolving terminologies \cite{devlin2019bert}.

Biomedical texts contain a combination of technical, medical, and biological elements. Specific terms such as disease names, treatments, drugs, biological processes, clinical study results, etc., are used in various contexts. These terms are often not present in general language corpora on which NLP models are typically trained. Moreover, these texts frequently include abbreviations, acronyms, and uncommon terms that are not always defined or explained in the text. Consequently, classical NLP models pretrained on general corpora, such as those used in social media or newspapers, may perform poorly when applied to biomedical texts \cite{lee2020biobert}.

The biomedical domain generates an enormous amount of unstructured information in the form of research abstracts, scientific articles, clinical databases, and more. This information must be processed, analyzed, and structured to be useful in practical contexts such as scientific research, clinical decision-making, and epidemiology. However, unstructured data pose additional challenges in terms of noise and linguistic ambiguity. For example, the same term may have multiple meanings depending on the context (e.g., "cell" can refer to a biological cell or a network cell). Furthermore, the lack of labeled data for certain rare or underrepresented diseases complicates the training of robust models \cite{mikolov2018advances}.

Another major challenge in biomedical NLP is the rapid evolution of medical knowledge. New treatments, drugs, technologies, and scientific discoveries are published daily. Therefore, NLP models must be able to quickly adapt to new information and take into account this dynamic evolution. Additionally, specific terms, concepts, and relations are continually introduced, requiring constant adjustment of models and embeddings. This has led to the emergence of contextual learning approaches, where models such as \gls{biobert} and \gls{pubmedbert} are pretrained on large biomedical corpora and fine-tuned to better understand these texts \cite{lee2020biobert, gupta2021pubmedbert}.

With the increasing complexity of the data, approaches based on traditional language models, such as vector models (\gls{fasttext}, \gls{glove}), have proven insufficient to effectively handle domain-specific biomedical language. It has become necessary to leverage more sophisticated architectures such as recurrent models (\gls{rnn}, \gls{lstm}, \gls{gru}) and attention mechanisms to capture complex relationships and dependencies in long text sequences. Furthermore, models like \gls{bert} and its variants such as \gls{biobert} have become preferred tools for biomedical text processing, as they incorporate domain-specific knowledge while extracting rich contextual representations \cite{devlin2019bert, lee2020biobert}.

Automated processing of biomedical texts has profound implications in several key areas. In scientific research, it enables automating the sorting of relevant publications in specific fields, such as rare disease research or specific treatments. In clinical contexts, it can be used to improve medical decision-making by automatically extracting information from medical records or clinical studies. NLP is also used in epidemiological surveillance systems to extract trends and hidden relationships from large volumes of medical data \cite{mikolov2018advances}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Classification-of-Deep-Learning-Models.png}
    \caption{Classification of deep learning models used in natural language processing. Source: \url{https://sl.bing.net/gQNjaTwIKIe}}
    \label{fig:deep_learning}
\end{figure}

\section{Sequential Architectures (RNN, LSTM, GRU)}

\subsection{Recurrent Neural Networks (RNN)}

Recurrent Neural Networks (\gls{rnn}) are a class of models designed for processing sequential data. Unlike feedforward networks, \gls{rnn} incorporate a recurrent loop that allows information to be passed from one time step to the next, making them suitable for modeling temporal dependencies in data. They are distinguished by their ability to remember previous states in a sequence. At each time step \( t \), the hidden state \( h_t \) is computed from the input \( x_t \) and the previous state \( h_{t-1} \), according to the following equation:

\[
h_t = f(Wx_t + Uh_{t-1} + b)
\]

where \( W \) and \( U \) are weight matrices associated respectively with the input and the previous state, \( b \) is a bias term, and \( f \) is an activation function, usually a tanh or sigmoid function. This structure enables \gls{rnn} to accumulate contextual memory over the sequence.

Thanks to this architecture, \gls{rnn} are capable of capturing temporal relationships within a sequence. This makes them particularly suited for processing ordered data streams such as time series, text, or audio signals. Their effectiveness relies on their ability to handle variable-length sequences while maintaining an internal state that summarizes past information.

However, despite their ability to handle sequential data, classical \gls{rnn} have major limitations. They suffer notably from the vanishing and exploding gradient problems \cite{bengio1994learning}, which can occur when training on long sequences. The former makes it difficult to adjust weights in deep layers, while the latter leads to unstable updates of model parameters. These issues reduce the effectiveness of \gls{rnn} in modeling long-term dependencies and have led to the development of improved variants such as \gls{lstm} and \gls{gru}.

To better visualize the working principle of \gls{rnn}, the following figure illustrates the recurrent connections between the input \( x_t \), hidden state \( h_t \), output \( L_t \), and information flow across time steps.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{rnn_image.png}
    \caption{Illustration of the operation of Recurrent Neural Networks (RNN). Source: \url{https://miro.medium.com/v2/resize:fit:660/1*uLTBA8Myf6_IwtpfLr4Xpg.png}}
    \label{fig:rnn_architecture}
\end{figure}

\subsection{Long Short-Term Memory (LSTM)}

Long Short-Term Memory networks (\gls{lstm}) represent a sophisticated extension of traditional Recurrent Neural Networks (\gls{rnn}), designed to overcome their limitations in capturing long-term dependencies, notably the vanishing and exploding gradient problems. Introduced by Hochreiter and Schmidhuber in 1997 \cite{hochreiter1997long}, the \gls{lstm} architecture relies on an internal memory cell capable of retaining information over long sequences, controlled by a system of specialized gates.

At each time step \( t \), the fundamental operations of an \gls{lstm} cell are described by the following equations:

\begin{align*}
f_t &= \sigma(W_f x_t + U_f h_{t-1} + b_f) \quad &\text{(Forget gate)} \\
i_t &= \sigma(W_i x_t + U_i h_{t-1} + b_i) \quad &\text{(Input gate)} \\
\tilde{c}_t &= \tanh(W_c x_t + U_c h_{t-1} + b_c) \quad &\text{(Candidate cell state)} \\
c_t &= f_t \odot c_{t-1} + i_t \odot \tilde{c}_t \quad &\text{(Cell state update)} \\
o_t &= \sigma(W_o x_t + U_o h_{t-1} + b_o) \quad &\text{(Output gate)} \\
h_t &= o_t \odot \tanh(c_t) \quad &\text{(Hidden state)}
\end{align*}

where:
\begin{itemize}[itemsep=0pt, topsep=0pt]
    \item \( x_t \) is the input at time \( t \);
    \item \( h_{t-1} \) is the previous hidden state;
    \item \( f_t \), \( i_t \), and \( o_t \) are the forget, input, and output gate vectors respectively;
    \item \( c_t \) is the memory cell state at time \( t \);
    \item \( \tilde{c}_t \) is the proposed candidate cell state;
    \item \( \sigma \) is the sigmoid function, \( \tanh \) the hyperbolic tangent, and \( \odot \) denotes element-wise multiplication.
\end{itemize}

This structure allows \gls{lstm} to dynamically decide which information to keep, update, or output, offering selective memory capacity very useful for processing complex sequences. Thanks to this flexibility, \gls{lstm} have become a standard in many tasks such as language modeling, machine translation, and time series analysis.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{lstm_image.png}
    \caption{Logic of the LSTM architecture. Source: \url{https://sl.bing.net/bWWTQ1HkC7M}}
    \label{fig:lstm_architecture}
\end{figure}

\bigskip

\noindent Although powerful, \gls{lstm} have considerable computational complexity due to the large number of parameters associated with their multiple gates. To provide a lighter yet effective alternative, the community introduced a simplified architecture: the Gated Recurrent Unit (\gls{gru}).

\subsection{Gated Recurrent Unit (GRU)}

The Gated Recurrent Unit (\gls{gru}) is a variant of \gls{rnn} introduced by Cho et al. in 2014 \cite{cho2014learning}, aiming to reduce model complexity while maintaining comparable performance to \gls{lstm}. Unlike \gls{lstm}, \gls{gru} merge the forget and input gate functions into a single "update gate" and do not use a separate cell state distinct from the hidden state vector. This simplification allows faster convergence and increased efficiency in certain tasks.

The equations governing a \gls{gru} cell are as follows:

\begin{align*}
z_t &= \sigma(W_z x_t + U_z h_{t-1} + b_z) \quad &\text{(Update gate)} \\
r_t &= \sigma(W_r x_t + U_r h_{t-1} + b_r) \quad &\text{(Reset gate)} \\
\tilde{h}_t &= \tanh(W_h x_t + U_h (r_t \odot h_{t-1}) + b_h) \quad &\text{(Candidate state)} \\
h_t &= (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t \quad &\text{(Hidden state)}
\end{align*}

where:
\begin{itemize}[itemsep=0pt, topsep=0pt]
    \item \( z_t \) is the update gate;
    \item \( r_t \) is the reset gate;
    \item \( \tilde{h}_t \) is the candidate hidden state;
    \item \( h_t \) is the final hidden state of the \gls{gru} cell at time \( t \).
\end{itemize}

This compact architecture allows \gls{gru} to effectively adapt to temporal dependencies while limiting computational load.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{gru_image.png}
    \caption{Illustration of the Gated Recurrent Unit (GRU). Source: \url{https://sl.bing.net/bWWTQ1HkC7M}}
    \label{fig:gru_architecture}
\end{figure}

This compact architecture allows \gls{gru} to effectively adapt to temporal dependencies while limiting computational load.

\vspace{0.5cm}

However, despite their ability to capture long-term relationships, sequential architectures such as \gls{lstm} and \gls{gru} process sequences uniformly, without giving more importance to certain parts of the text over others. To overcome this limitation, attention mechanisms have been introduced to enable the model to dynamically focus on the most relevant elements of the sequence.

Among these approaches, the attention mechanism proposed by Bahdanau et al. \cite{bahdanau2015neural} has established itself as an effective extension, initially designed for machine translation. In the context of this thesis, a simplified adaptation of this attention is used to weight the outputs of a bidirectional \gls{gru} in a classification task. The following section details this mechanism.

\section{Convolutional Neural Networks (CNN)}

Convolutional Neural Networks (\gls{cnn}) are architectures widely used in computer vision, but their application to Natural Language Processing (\gls{nlp}) has also proven promising, particularly in text classification tasks \cite{kim2014convolutional}. Their principle relies on the hierarchical extraction of features using sliding convolutional filters applied to a matrix representing a textual sequence.

Mathematically, the discrete convolution operation applied to an input matrix \( I \) and a kernel \( K \) is defined as follows:

\begin{equation}
S(i, j) = (I * K)(i, j) = \sum_m \sum_n I(i + m, j + n) \cdot K(m, n)
\end{equation}

where \( S(i,j) \) denotes the convolved output at position \( (i,j) \), and indices \( m,n \) run over the filter dimensions. This operation allows for local detection of relevant patterns in the structure of the text represented in matrix form.

In the case of a textual document, each word is first represented by a dense vector — either learned during training or pretrained (for example via \gls{glove} or \gls{fasttext}). These vectors are stacked to form an input matrix of size \( L \times d \), where \( L \) is the length of the text (or number of words), and \( d \) is the embedding dimension. This matrix is then passed through a \textit{convolutional layer}: filters (or kernels) slide over the rows of the matrix, capturing local patterns (such as n-grams) of fixed size — for example, windows of 2, 3, or 4 words. Each filter learns to detect a particular structure (term co-occurrences, specific expressions, syntactic patterns, etc.).

Following this convolution, a nonlinear \textit{activation function}, such as the ReLU (\textit{Rectified Linear Unit}) function, is usually applied. This introduces non-linearity into the model and promotes detection of complex patterns.

Next comes a \textit{pooling operation}, often max-pooling, which reduces the dimensionality of the representations while preserving the most salient information. This operation helps make the model invariant to small shifts in the position of detected patterns. In the context of \gls{nlp}, this corresponds to identifying the most informative patterns regardless of their position in the text.

The features extracted by the filters are then concatenated and passed to one or more fully connected layers, leading to an output layer, usually a \textit{softmax} (in the case of multi-class classification) or a \textit{sigmoid} (in the binary case).

\begin{align}
\textit{sigmoid:} \quad \sigma(x) &= \frac{1}{1 + e^{-x}} \\[1em]
\textit{softmax:} \quad \text{softmax}(z_i) &= \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}} \quad \text{for } i = 1, \dots, K
\end{align}

One major advantage of \gls{cnn} in text processing is their ability to extract discriminative representations without relying on explicit modeling of the global order of the sequence, while being faster to train than recurrent networks. However, they sometimes struggle to capture long-range dependencies, which is why they are often combined with architectures such as \gls{gru} or \gls{lstm}, as is the case in this thesis with the hybrid \gls{cnn}-\gls{gru} and \gls{cnn}-\gls{lstm} models \cite{zhou2015clstm}.

These combined architectures benefit both from the capacity of \gls{cnn} to quickly extract local patterns, and from the ability of recurrent networks to model temporal and contextual dynamics of sequences (see Figure~\ref{fig:cnn_architecture}).This makes them particularly suitable candidates for biomedical text classification, where documents are short, information-dense, and rely on specialized linguistic structures.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{cnn_image.png}
    \caption{Convolutional Neural Network. Source: \url{https://sl.bing.net/g171Tl7lF48}}
    \label{fig:cnn_architecture}
\end{figure}

\subsection{Bahdanau Attention Mechanism}

The Bahdanau attention mechanism, introduced in the context of neural machine translation \cite{bahdanau2015neural}, aims to overcome the limitations of classical sequential models that compress all the information of a sequence into a fixed-length vector. Such compression can be suboptimal, especially when sequences are long or contain dispersed information.

Attention allows the model to dynamically weight the elements of the sequence according to their relevance to the task at hand. In other words, instead of treating all parts of a text equally, the model learns to focus more on the parts that are relevant for prediction.

In this thesis, a simplified version of Bahdanau's mechanism is applied on the outputs of a bidirectional \gls{gru}. Specifically, each time-step output of the \gls{gru} is transformed via a feedforward network to produce an importance score (or attention score). These scores are then normalized through a softmax function to obtain attention weights. The context vector is then computed as a weighted average of the hidden states of the sequence, where the weights reflect the relevance of each word.

The hidden vector at time \( t \) in a bidirectional \gls{gru} is defined as the concatenation of the forward and backward vectors:

\[
h_t = [\overrightarrow{h_t}; \overleftarrow{h_t}]
\]

where \( \overrightarrow{h_t} \) and \( \overleftarrow{h_t} \) are respectively the hidden states generated by the forward and backward passes of the \gls{gru}.

The attention score for each hidden state at time \( t \) is computed as follows:

\[
\text{score}_t = \mathbf{v}^\top \tanh(\mathbf{W} [\overrightarrow{h_t}; \overleftarrow{h_t}])
\]

The relative importance of each element in the sequence is then calculated by a softmax function:

\[
\alpha_t = \frac{\exp(\text{score}_t)}{\sum_{t'} \exp(\text{score}_{t'})}
\]

Finally, the context vector, summarizing the relevant information from the hidden states of the sequence, is obtained by a weighted sum of the hidden states:

\[
\text{context} = \sum_t \alpha_t [\overrightarrow{h_t}; \overleftarrow{h_t}]
\]

In these equations:

\begin{itemize}[label=--, itemsep=0pt, topsep=0pt]

    \item \( h_t \) is the hidden vector at time \( t \), which is the concatenation of the forward and backward hidden states;
    \item \( \alpha_t \) is the attention weight associated with the hidden state \( h_t \);
    \item \text{context} is the resulting vector, which summarizes the sequence by emphasizing the most relevant elements.
\end{itemize}

This mechanism enables the model to leverage both preceding and succeeding contextual information in a sequence, which is particularly useful for tasks such as biomedical text classification, where a term may have different meanings depending on the surrounding context.

After exploring Bahdanau’s attention mechanism and its application in sequential models such as the bidirectional \gls{gru}, it is important to turn to another essential component in natural language processing architectures: the vector representation of words and sentences. These representations play a fundamental role in capturing semantic and contextual relationships between terms.

In this regard, several vector representation techniques have emerged to better understand and model language. Among the most popular are \gls{glove}, \gls{fasttext}, as well as larger-scale models specifically designed for biomedical domains, such as \gls{pubmedbert} and \gls{biobert}. These approaches have helped to overcome the limitations of classical representations by integrating both global information (such as word co-occurrences) and finer contextual information.

The next section presents these various vector representation techniques, with a particular focus on their application in the biomedical field, where terms are often highly specialized and context-dependent.

\vspace{0.5cm}
\section{Vector Representations}

The performance of natural language processing models largely depends on how words are numerically represented. Indeed, even before applying sequential models or attention mechanisms, texts must be transformed into vectors that can be processed by machine learning algorithms.

Two main families of vector representations stand out: static \textit{word embeddings}, which assign a fixed vector to each word regardless of its context, and \textit{contextual embeddings}, which dynamically adjust representations based on the word’s context within the sentence.

In this section, we first explore the classical approaches based on \textit{word embeddings}, notably \gls{glove} and \gls{fasttext}, which marked a significant advance in capturing semantic relationships between words. Then, we present more recent and powerful methods based on Transformer models, such as \gls{biobert} and \gls{pubmedbert}, which produce contextual representations particularly suited for biomedical texts.

\subsection{Word Embeddings: GloVe and FastText}

Static \textit{word embeddings} represent each word as a dense vector in a continuous low-dimensional space. These representations are learned from large text corpora by exploiting word co-occurrences. Two of the most influential approaches in this area are \gls{glove} and \gls{fasttext}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{we.png}
    \caption{Illustration of the principle of \textit{word embeddings}: similar words are projected into nearby regions of the vector space. Source: \url{https://sl.bing.net/jumSq5RbE1Q}}
    \label{fig:word_embeddings}
\end{figure}

\subsubsection{FastText}

\gls{fasttext} \cite{bojanowski2017enriching}, developed by Facebook AI Research, is an extension of \gls{w2v}, a word vector representation learning method developed by Mikolov et al. Unlike \gls{glove} or \gls{w2v}, which treat each word as an atomic unit, \gls{fasttext} represents a word as the sum of vectors of its subunits called character \textit{n-grams}. Formally, if a word \( w \) consists of n-grams \( g_1, g_2, \dots, g_K \), then its vector representation \( \mathbf{v}_w \) is defined as:

\[
\mathbf{v}_w = \sum_{k=1}^{K} \mathbf{z}_{g_k}
\]

where \( \mathbf{z}_{g_k} \) is the vector associated with the n-gram \( g_k \).

For example, the word “cat” with trigrams (n=3) would be represented by the following n-grams: \_ca, cat, at\_. \gls{fasttext} also adds word boundary markers (“\_” at the beginning and end) to distinguish positions within the word.

This representation allows the model to share information among words with similar morphology, making it robust to rare or out-of-vocabulary words. Additionally, FastText can produce vectors for words missing from the training vocabulary as long as their subunits were observed.

By preserving the semantic properties of \gls{w2v} while capturing morphological regularities (prefixes, suffixes, roots), FastText is especially well-suited for specialized domains like biomedicine, where terms are often long, compound, and rare.

\subsubsection{GloVe}

\gls{glove} \cite{pennington2014glove} is an unsupervised learning method that combines the advantages of local context-window based approaches (like \gls{w2v}) with global statistical methods. Unlike Word2Vec, which optimizes a local objective function, \gls{glove} takes into account global information about word distribution across the entire corpus. It relies on constructing a word co-occurrence matrix, where each entry \( X_{ij} \) corresponds to the number of times word \( j \) appears in the context of word \( i \), i.e., a measure of word co-occurrence frequency.

The goal of the model is to learn vectors \( w_i \) and \( \tilde{w}_j \) for each word \( i \) and context \( j \) such that their dot product approximates the logarithm of \( X_{ij} \). This relationship can be expressed as:

\[
w_i^\top \tilde{w}_j + b_i + \tilde{b}_j \approx \log(X_{ij})
\]

where \( b_i \) and \( \tilde{b}_j \) are learned biases for each word and context respectively. The model is trained to optimize this relationship over all frequent word pairs in the corpus. Optimizing this objective captures semantic and syntactic relationships between words. In particular, vectors learned by \gls{glove} can capture analogies such as:

\[
\vec{\text{king}} - \vec{\text{man}} + \vec{\text{woman}} \approx \vec{\text{queen}}
\]

The embeddings produced by \gls{glove} thus represent complex relationships between words, not only semantic but also analogical, making them especially useful in NLP applications such as text classification, information retrieval, and text generation.

A key advantage of \gls{glove} is that it combines local co-occurrence information with a global corpus representation, enabling learning word vectors that capture both close semantic relations and more abstract relationships like analogies. Unlike local models focused solely on word relations within a fixed window, \gls{glove} integrates the entire global co-occurrence structure.

However, a limitation of \gls{glove} lies in its inability to handle contextual ambiguities of words. For example, the word “bank” can have very different meanings depending on context, but \gls{glove} produces a single vector representation that does not reflect these variations. This lack of contextualization has motivated the emergence of more advanced models, such as contextual word representation models (e.g., \gls{bert}), which address this problem by generating different embeddings depending on the context in which the word appears.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{glove.png}
    \caption{Architecture of the GloVe model, illustrating vector construction from co-occurrences. The input is a one-hot representation of a word at a given time. Word embedding matrices are used as weight matrices, and the model output is a vector obtained by the dot product between word vectors. Source: \url{https://www.researchgate.net/figure/The-model-architecture-of-GloVe-The-input-is-a-one-hot-representation-of-a-word-The_fig1_337461648}}
    \label{fig:glove_architecture}
\end{figure}

% Here, develop BioBERT and PubMedBERT
\subsection{Contextual Embeddings: BioBERT and PubMedBERT}

Although static \textit{word embeddings} such as \gls{glove} and \gls{fasttext} have significantly improved the quality of lexical representations, they present a major limitation: each word is associated with a single vector regardless of its meaning in context. This poses a problem for polysemous or ambiguous words, which are frequent in natural language and particularly critical in biomedicine.

To overcome this limitation, recent models rely on Transformer-based architectures capable of producing dynamic contextual representations. In these models, the same word can be represented by different vectors depending on the context in which it appears.

In this section, we explore two contextual representations specifically trained on biomedical corpora: \gls{biobert} and \gls{pubmedbert}. These models, based on the \gls{bert} architecture, capture essential semantic nuances for complex tasks such as biomedical information extraction, scientific document classification, or medical concept normalization.

\subsection{BioBERT as a Contextual Embedding Vector}

\gls{biobert} is a variant of the \gls{bert} model specially pretrained on biomedical corpora, such as PubMed abstracts and PMC full-text articles \cite{lee2020biobert}. Like BERT, it generates \textit{contextual word embeddings}: each word is represented by a dense vector that depends not only on the word itself but also on its surrounding context.

Unlike static embedding methods like \gls{glove} or \gls{w2v}, which assign a single vector to each word regardless of context, \gls{biobert} can dynamically modulate representations based on the other tokens present in the same sentence. This is enabled by its Transformer-based architecture that exploits the \textit{self-attention} mechanism.

The self-attention mechanism computes dependencies among all tokens in a sequence, allowing each word to be represented in relation to the others:

\begin{equation}
\textit{Attention}(Q, K, V) = \text{softmax} \left( \frac{QK^{T}}{\sqrt{d_k}} \right) V
\end{equation}

where \(Q\), \(K\), and \(V\) are the query, key, and value matrices derived from input representations, and \(d_k\) is the dimension of the key vectors. This operation allows the model to weight interactions between each pair of words, effectively capturing semantic and syntactic relationships even at long distances.

The process starts with tokenization via WordPiece (Fig.~\ref{fig:wordpiece}), which splits words into subunits, followed by positional and segment encoding. Tokens are then processed through multiple Transformer layers, where the self-attention mechanism computes, for each token, a vector representation that takes into account all other tokens in the sequence. The output of these layers is a series of rich contextual vectors suited for specific tasks such as classification, biomedical named entity recognition, or relation extraction.

\gls{biobert} retains the bidirectional architecture of \gls{bert}—each layer receives both left and right context of each word—but is initialized with the base \gls{bert} weights and then fine-tuned on large biomedical corpora. This specialization makes \gls{biobert} particularly effective in health domain tasks: biomedical text classification, named entity recognition, clinical question answering, and more.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{tokenization_bert.png}
    \caption{Illustration of the WordPiece tokenization process followed by encoding and Transformer layers. Source: \url{https://research.googleblog.com/2021/12/a-fast-wordpiece-tokenization-system.html}}
    \label{fig:wordpiece}
\end{figure}

\subsection{PubMedBERT as a Contextual Embedding Vector}

\gls{pubmedbert} is a variant of the \gls{bert} model that has been pretrained specifically on biomedical texts extracted from the PubMed database, one of the largest corpora of scientific literature in the medical domain. PubMedBERT stands out because it was designed to better capture the lexical specificities and complex relationships present in medical and biological texts, particularly in biomedical research and clinical publications \cite{gu2020domain}.

Like \gls{biobert}, \gls{pubmedbert} generates \textit{contextual word embeddings} that vary depending on the context in which words appear. However, PubMedBERT is distinguished by its exclusive pretraining on PubMed articles, allowing it to better understand the terminology and semantics specific to biomedicine. Indeed, \gls{pubmedbert} can generate dense vectors that incorporate information specific to medical terminology, thus enabling improved performance on tasks such as biomedical named entity recognition, scientific publication classification, and analysis of relationships between medical concepts.

The model relies on the Transformer architecture and the self-attention mechanism to produce context-dependent vector representations of words, similar to \gls{biobert}. Initial tokenization is performed using WordPiece, a process that splits words into subunits to better handle rare or compound words. This allows the model to efficiently process the wide variety of technical terms encountered in biomedical literature.

The token processing pipeline in \gls{pubmedbert} follows an approach similar to that of \gls{biobert}: tokenization via WordPiece (see Fig.~\ref{fig:wordpiece}) splits complex medical terms into subunits, followed by positional and segment encoding. Tokens are then passed through multiple Transformer layers where each token is represented by a contextual vector computed through the self-attention mechanism, taking into account all tokens in the sequence. The output of these layers is a series of contextual vectors used for specific tasks such as biomedical text classification, clinical information extraction, or named entity recognition in scientific publications.

Thus, \gls{pubmedbert} is a powerful solution for biomedical applications where a fine understanding of medical and scientific terminology is essential. Its pretraining on PubMed, combined with the Transformer architecture, enables it to achieve strong results in complex tasks such as biomedical entity recognition, scientific document classification, and clinical information extraction, while accounting for the linguistic particularities of the medical domain.

\section{Visualization of Data Representations with t-SNE}
\label{sec:tsne}

Visualizing vector representations is an important step to better understand how a natural language processing model learns to distinguish different classes from textual data. In this project, we use the \gls{tsne} algorithm~\cite{van2008visualizing} to project high-dimensional vectors into a two-dimensional space for intuitive visualization.

\gls{tsne} (t-distributed Stochastic Neighbor Embedding) is based on a probabilistic modeling of proximity between points. In the original high-dimensional space, the similarity between two points $x_i$ and $x_j$ is defined as a conditional probability $p_{j|i}$, given by:

\begin{equation}
p_{j|i} = \frac{\exp(-\|x_i - x_j\|^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-\|x_i - x_k\|^2 / 2\sigma_i^2)}
\end{equation}

The joint symmetric probability is then defined as:

\begin{equation}
p_{ij} = \frac{p_{j|i} + p_{i|j}}{2n}
\end{equation}

where $n$ is the total number of points. In the low-dimensional projected space, a distribution $q_{ij}$ is defined on the projected points $y_i$ and $y_j$ using a Student’s t-distribution with one degree of freedom (which has heavier tails than a Gaussian):

\begin{equation}
q_{ij} = \frac{(1 + \|y_i - y_j\|^2)^{-1}}{\sum_{k \neq l} (1 + \|y_k - y_l\|^2)^{-1}}
\end{equation}

The objective of \gls{tsne} is then to minimize the Kullback-Leibler (KL) divergence between these two similarity distributions:

\begin{equation}
\mathcal{L} = KL(P \| Q) = \sum_{i \neq j} p_{ij} \log \left( \frac{p_{ij}}{q_{ij}} \right)
\end{equation}

This minimization is performed by gradient descent, and allows preservation of the local structures of the data — pairs of points that are close in the high-dimensional space remain close in the projected 2D space.

Applied at different stages of the pipeline, \gls{tsne} provides a visual representation of the data as perceived by the model — whether at the input (for example, \gls{glove} or \gls{biobert} embeddings) or through intermediate activations after passing through model layers (\gls{lstm}, attention, etc.).

The interpretation of \gls{tsne} projections relies on the preservation of local data structure. In other words, if two points are close in the projected space, they were likely also close in the original high-dimensional space. Thus, a clear visual separation between groups (or classes) in the \gls{tsne} map may indicate that the model has learned discriminative representations. Conversely, significant overlap may signal that the classes are not well separable in the learned space, which can result in classification errors.

In this work, \gls{tsne} allowed us to visualize clusters of data from different classes, identify potential zones of confusion between classes, and observe the evolution of the representations learned by the model throughout training.

This visualization is not merely illustrative: it offers a qualitative insight into the model’s behavior and can guide methodological choices, for example by suggesting the introduction of rebalancing techniques or adjustments to the architecture if classes remain difficult to separate.

\section{Few-Shot Learning in NLP: Definitions, Methods, and Challenges}

Few-shot learning (\gls{fsl}) represents an approach where models must learn effectively from only a few examples per class, a scenario commonly encountered in contexts where obtaining a sufficient amount of annotated data is difficult. This approach is particularly useful in \gls{tln}, where datasets can be scarce or hard to manually annotate, such as in domain-specific applications like biomedicine or law.

\subsection{Definitions and Concepts}

Few-shot learning is based on the idea that a model can learn complex tasks using very few examples. This capability is essential to overcome the limitations of traditional methods that require large amounts of annotated data to achieve good performance. In this context, a k-shot refers to the number of examples used per class to train a model. For example, in a 5-shot learning problem, the model is trained with five examples per class.

A key concept in \gls{fsl} is the ability of models to generalize from few examples. This requires strong abstraction and knowledge transfer capabilities gained from pretraining on larger datasets, as demonstrated by works on architectures based on \gls{bert} and its variants \cite{devlin2019bert, lee2020biobert, lu2020pubmedbert}.

\subsection{Methods and Techniques}

\gls{fsl} methods in \gls{tln} include approaches such as meta-learning, which enables models to quickly adapt to new tasks with few examples. Techniques like Matching Networks \cite{vinyals2016matching}, Prototypical Networks \cite{snell2017prototypical}, and MAML (Model-Agnostic Meta-Learning) \cite{finn2017model} are commonly used. These methods rely on mechanisms allowing the model to learn from few examples by optimizing its ability to quickly adapt to new tasks.

Transformers such as \gls{bert} \cite{devlin2019bert} and its derivatives, like \gls{biobert} \cite{lee2020biobert} or \gls{pubmedbert} \cite{lu2020pubmedbert}, have proven particularly effective in \gls{fsl} tasks in NLP, as they can capture rich, adaptive contextual representations. These models are often pretrained on large corpora and then fine-tuned for specific tasks with few examples.

\subsection{Challenges in Few-Shot Learning}

One major challenge in few-shot learning is generalization. The model must be able to make reliable predictions despite the limited number of training examples. This difficulty is heightened in specific domains such as biomedicine, where semantic richness and term polysemy make the model prone to errors if the context is not well understood \cite{jin2019recurrent, zhang2020biowordvec}. Techniques like multitask training and transfer learning can help overcome these obstacles by leveraging information from related tasks or domains.

Class contrast in a Few-Shot dataset can also be challenging. Models need to distinguish very similar examples while being robust to variations in input data. Approaches such as data augmentation and regularization techniques can be useful to improve model robustness in \gls{fsl}.

\section{Class Imbalance Issues and Adaptation Techniques}
\label{sec:class-imbalance}

Class imbalance is a common problem in biomedical text classification, where some classes are overrepresented while others, often rarer but clinically important, are underrepresented. This disparity can heavily bias model learning, which tends to favor the majority class at the expense of accuracy on minority classes. To address this, several adaptation techniques have been developed, including resampling methods and class weighting strategies in the loss function. These techniques are particularly relevant in low-data or highly imbalanced contexts, such as in biomedicine.

\subsection{Resampling Methods}

Resampling methods aim to modify the data distribution to compensate for class imbalance. There are mainly two approaches:

\begin{itemize}[label=--, itemsep=0pt, topsep=0pt]
    \item \textbf{Oversampling}, which artificially increases the proportion of minority class examples;
    \item \textbf{Undersampling}, which reduces the size of majority classes to balance the distribution.
\end{itemize}

Although both types of methods are commonly used in the literature, our project focuses exclusively on \textbf{oversampling} techniques. This choice is based on a review of specialized articles on learning from imbalanced data, particularly in the biomedical domain, where preserving majority class data is often crucial to maintain the richness of clinical cases. Moreover, oversampling methods enrich the minority class without altering the overall structure of the dataset.

Among oversampling approaches, we selected two widely recognized techniques: \gls{smote} (Synthetic Minority Over-sampling Technique)~\cite{chawla2002smote} and its variant Borderline-\gls{smote}~\cite{han2005borderline}. These methods have demonstrated their effectiveness in several studies to improve model performance on highly imbalanced datasets.

\subsubsection{SMOTE — \textbf{Synthetic Minority Over-sampling Technique}}

\gls{smote}~\cite{chawla2002smote} is an oversampling method designed to address class imbalance issues in datasets, particularly where minority classes are underrepresented. Unlike simpler approaches such as random duplication of minority examples, \gls{smote} generates new synthetic examples by interpolation, introducing greater diversity in the data.

The fundamental principle of \gls{smote} consists in randomly selecting one of the $k$ nearest neighbors of a minority sample, then creating a new example along the line segment connecting these two points in feature space. Formally, if $x_i$ is a minority example and $x_i^{(NN)}$ one of its nearest neighbors, a new synthetic example $x_{\text{new}}$ is generated as:

\begin{equation}
x_{\text{new}} = x_i + \delta \cdot (x_i^{(NN)} - x_i), \quad \text{where } \delta \sim \mathcal{U}(0,1)
\end{equation}

where $\delta$ is a random weighting factor drawn from a uniform distribution. This linear interpolation produces instances close to existing data, enriching the minority class feature space more smoothly than exact duplications.

This technique has several advantages: it reduces the risk of overfitting, improves decision boundary definition, and allows classifiers to better generalize in low-density areas. It is particularly relevant in biomedical contexts where rare conditions or specific diseases are underrepresented.

However, \gls{smote} also has limitations. It may create non-representative synthetic instances if neighbors used are near class boundaries, which can introduce noise. Additionally, by artificially extending the minority class, it can exacerbate class overlap if the original problem is not linearly separable. Variants such as Borderline-\gls{smote} have been proposed to address these issues by focusing synthetic example generation on the most ambiguous regions.

The principle of \gls{smote} is illustrated in Figure~\ref{fig:smote_algo}, which presents the algorithm as pseudocode. It details the steps to generate synthetic examples via random interpolation between a minority instance and its nearest neighbors. This method can be combined with other techniques, such as majority class undersampling or loss function weighting, to optimize performance on imbalanced datasets.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{smote.png}
    \caption{SMOTE algorithm: generation of synthetic examples from nearest neighbors of a minority instance. Adapted from \cite{chawla2002smote}.}
    \label{fig:smote_algo}
\end{figure}

\subsubsection{Borderline-SMOTE}

Borderline-\gls{smote}~\cite{han2005borderline} is a variant of \gls{smote} that specifically targets examples near decision boundaries—areas most likely to cause classification errors. In sensitive applications such as medical diagnosis, where errors on ambiguous cases can be critical, this method enhances the model’s ability to correctly discriminate classes in the most complex situations. It works by identifying minority samples surrounded by many majority class examples (so-called "borderline" situations) and generating new examples from these, thereby densifying these critical regions of the decision space.

\section{Class Weighting in the Loss Function}

Class weighting in the loss function is a method used to compensate for class imbalance by assigning higher weights to minority classes. This approach helps to correct the bias induced by imbalanced classes without altering the data itself. It is particularly useful in domains where certain classes are clinically important but underrepresented, such as biomedicine \cite{Chawla2002SMOTE} or political science \cite{King2001Logit}.

Let $y_i$ be the true label of sample $i$, and $\hat{y}_i$ the model's prediction for that sample. The standard loss function for binary classification (e.g., cross-entropy) is given by:

$$
\mathcal{L}_\text{standard} = - \sum_{i=1}^{N} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
$$

However, in the case of imbalanced classes, this loss function may lead the model to favor the majority class. To address this issue, the loss function is modified by assigning weights $w_1$ and $w_0$ to classes 1 and 0, respectively. The weighted loss function then becomes:

$$
\mathcal{L}_\text{weighted} = - \sum_{i=1}^{N} \left[ w_1 \cdot y_i \log(\hat{y}_i) + w_0 \cdot (1 - y_i) \log(1 - \hat{y}_i) \right]
$$

where $w_1$ and $w_0$ are the weights associated with classes 1 and 0, respectively. These weights can be defined based on the class frequencies in the dataset.

\vspace{1em}
In a multi-class problem, each label $y_i$ belongs to one of the possible classes, say $C = \{1, 2, \dots, K\}$, where $K$ is the total number of classes. The model predicts a probability $\hat{y}_i$ for each class $k$, i.e., $\hat{y}_i = P(\hat{y} = k | x_i)$ \cite{Han2005Borderline}.

The standard loss function for multi-class classification, such as cross-entropy, is given by:

$$
\mathcal{L}_\text{standard} = - \sum_{i=1}^{N} \sum_{k=1}^{K} y_{ik} \log(\hat{y}_{ik})
$$

where $y_{ik}$ is the binary label for sample $i$ and class $k$ (1 if $x_i$ belongs to class $k$, otherwise 0), and $\hat{y}_{ik}$ is the predicted probability for that class.

To adapt the loss function to class imbalance in the multi-class setting, a weight $w_k$ is assigned to each class $k$. The weighted loss function becomes:

$$
\mathcal{L}_\text{weighted} = - \sum_{i=1}^{N} \sum_{k=1}^{K} w_k \cdot y_{ik} \log(\hat{y}_{ik})
$$

where $w_k$ is the weight associated with class $k$.

As in the binary case, these weights can be defined inversely proportional to the class frequencies in the dataset. Weights can be computed in various ways depending on the class imbalance.

The focal loss, proposed by Lin et al. \cite{Lin2017Focal}, introduces a modulation factor to focus learning on hard-to-classify examples while retaining class weighting (where $\gamma$ is a focusing parameter):

$$
\mathcal{L}_\text{focal} = - \sum_{i=1}^{N} \sum_{k=1}^{K} w_k \cdot (1 - \hat{y}_{ik})^\gamma \cdot y_{ik} \log(\hat{y}_{ik})
$$

\chapter{Related Work}
\addcontentsline{toc}{chapter}{Related Work}

\lhead{\MakeUppercase{Related Work}}
\section{Overview of Biomedical Text Classification Methods}

Automatic classification of biomedical texts is a central field in natural language processing (\gls{nlp}), particularly crucial for applications such as the analysis of scientific publications, clinical information extraction, and decision support systems. This field has evolved alongside advances in machine learning, especially deep learning. The objective of this section is to present the evolution of biomedical text classification approaches, focusing on the major works that have influenced this domain, while highlighting the motivations that led to this thesis work.

In the early years of automatic text classification development, models were mainly based on simple representations such as the \textit{bag-of-words}. Traditional approaches also included methods like support vector machines (SVM) and Naïve Bayes classifiers. Although these techniques were robust, they struggled to capture the complex contextual relationships between terms, which are essential in the biomedical domain. Biomedical corpora, often characterized by highly specialized vocabulary and significant semantic ambiguity, required a more sophisticated approach capable of understanding these subtle relations.

The shift toward sequential models began in the 2010s with the emergence of recurrent neural networks (\gls{rnn}), particularly the \gls{lstm} and \gls{gru} variants. These models allowed better modeling of temporal and contextual dependencies of words within a text sequence. Indeed, in fields like biology and medicine, the relationships between words are not independent but depend on the context in which they appear. \glspl{rnn}, introduced by Elman (1990)~\cite{elman1990finding}, were the first models able to process sequential data by capturing these dependencies, but they suffered from a major issue: the \textit{vanishing gradient} problem, which made learning over long sequences difficult. It was in this context that \glspl{lstm}, proposed by Hochreiter and Schmidhuber (1997)~\cite{hochreiter1997long}, emerged, introducing memory mechanisms that greatly improved the handling of long-range dependencies in data.

\glspl{gru}, more recently introduced by Cho et al. (2014)~\cite{cho2014learning}, simplified the \gls{lstm} architecture while maintaining similar performance, with the advantage of requiring fewer parameters. These advances enabled better handling of contextual dependencies and thus naturally found application in the biomedical field, where capturing relationships between different medical entities present in a text is crucial.

At the same time, static word representations such as \gls{w2v} and \gls{glove}, as well as domain-specific embeddings like BioWordVec~\cite{zhang2020biowordvec}, improved the semantic representation of terms. These models helped overcome the limitation of purely count-based representations by providing dense vectors that capture semantic relations between words.

However, it is with the rise of Transformer-based models that biomedical text classification experienced a major breakthrough. Models such as \gls{biobert}~\cite{lee2020biobert} and \gls{pubmedbert}~\cite{gupta2021pubmedbert}, pre-trained on specific biomedical corpora, enabled the capture of much finer contextual relationships by considering the entire context around a term. These models have achieved significant advances in medical text classification, particularly in complex tasks such as medical entity extraction and relation recognition between different entities.

From 2018 onward, research also turned towards more complex scenarios like few-shot learning (\gls{fsl}), where models can adapt to sparsely annotated or imbalanced datasets. Domain adaptation techniques and meta-learning have also been explored to improve generalization and robustness of models against language variations or specific domains.

Building upon these various advances, our work fits within this research continuum. We propose to combine these techniques innovatively to overcome the specific challenges of biomedical text classification, notably the management of specialized vocabulary, term ambiguity, and class imbalance in corpora. This work draws heavily from sequential models, particularly \glspl{lstm}, \glspl{gru}, and recent architectures like \gls{biobert}, to address classification in this domain with adapted and efficient solutions.

\subsection{Classical Approaches}

Before the emergence of deep neural networks, biomedical text classification mainly relied on traditional machine learning methods. These methods were based on sparse vector representations such as the \textit{Bag-of-Words} model or TF-IDF weighting (\textit{Term Frequency–Inverse Document Frequency}), which completely ignore word order as well as syntactic or semantic structure. Classical models like Naïve Bayes classifiers, support vector machines (SVM), or decision trees were frequently used, taking as input these vectors derived from simple lexical representations.

For example, Japkowicz and Stephen (2002) \cite{japkowicz2002class} highlighted the limitations of these approaches in contexts of class imbalance, a common phenomenon in biomedical data. Similarly, King and Zeng (2001) \cite{King2001Logit} studied the degraded performance of logistic regression models when dealing with rare events, posing a challenge in classifying underrepresented medical documents. While these methods have the advantage of being interpretable and fast to train, they do not capture semantic relationships between words nor their contextual usage within a sentence.

Thus, biomedical text classification models have significantly evolved with advances in natural language processing and deep learning. We will focus next on specific works that influenced how current models are applied in the biomedical domain. In the following sections, we will explore particular studies, especially those on the application of \glspl{lstm}, \glspl{rnn}, and more complex architectures, to better understand their contributions to text classification in the biomedical context. These works from the literature will serve as the foundation for the design and implementation of the method proposed in this thesis.

We begin by examining in detail research that incorporated recurrent architectures to handle specific biomedical data, before turning to recent hybrid approaches combining multiple types of networks to maximize classification performance. These perspectives will pave the way for presenting the specific methodology developed in this work.

\subsection{Finding Structure in Time}

The seminal article by Jeffrey L. Elman titled \textit{Finding Structure in Time}~\cite{elman1990finding} marks a decisive step in the evolution of sequential models for natural language processing. In this work, Elman introduces a simple recurrent neural network architecture (SRN, \textit{Simple Recurrent Network}) designed to learn temporal regularities in linguistic sequences. This was not merely a technical development but an empirical demonstration that implicit grammatical and lexical structures can emerge from exposure to unannotated sequential data.

The SRN operates according to a fundamental principle: at each time step $t$, the input $x_t$ is combined with a context state $h_{t-1}$ (coming from the hidden layer at the previous step) to produce a new hidden state $h_t$. Formally, this can be represented as:

\begin{equation}
h_t = \sigma(W_{xh} x_t + W_{hh} h_{t-1} + b_h), \quad y_t = \text{softmax}(W_{hy} h_t + b_y)
\end{equation}

where $\sigma$ is a nonlinear activation function (such as $\tanh$), $W_{xh}$, $W_{hh}$, and $W_{hy}$ are weight matrices, and $y_t$ is the prediction made at time $t$. The network's ability to retain temporal information relies solely on the recurrent passage of the hidden state, which introduces a form of memory.

Elman tested his model on tasks of increasing complexity, demonstrating that even a simple recurrent network can develop internal representations capturing deep linguistic structures:

\paragraph{Structure in letter sequences.} The SRN was first trained on sequences of letters generated according to simple probabilistic rules. The network quickly learned to predict the next letters with significant accuracy, showing it had implicitly learned orthographic regularities. This result suggests that predictive learning can lead to the emergence of structured patterns in continuous symbol streams.

\paragraph{Implicit discovery of the notion of word.} A landmark experiment in the paper involved presenting the SRN with a continuous stream of characters without explicit word boundaries. Remarkably, the network implicitly identified lexical boundaries by capturing letter transition statistics. This structural induction capacity without supervision foreshadows modern unsupervised learning models.

\paragraph{Identification of lexical classes from word order.} In a more complex task, Elman exposed the network to word sequences following an artificial grammar. The SRN not only learned to predict following elements but organized its internal representations so that words were grouped according to their grammatical role (nouns, verbs, determiners, etc.). Thus, latent representations revealed the spontaneous acquisition of syntactic categories.

This work highlights several key concepts: distributed representation learning, the importance of memory in sequential processing, and the ability of networks to discover implicit hierarchical structures. It also offers developmental perspectives: Elman hypothesized that temporal constraints and limited memory capacity play a role in the progressive structuring of language in children. This hypothesis has led to work in computational psycholinguistics, suggesting convergence between machine learning and cognitive processes.

However, the SRN suffers from significant limitations, notably the vanishing gradient problem during backpropagation through time, which hampers learning of long-term dependencies. These limitations motivated the emergence of more robust architectures such as LSTM networks~\cite{hochreiter1997long}, specifically designed to overcome these difficulties.

Ultimately, Elman’s article stands as a cornerstone in the development of neural networks for natural language processing, showing that simple predictive learning can induce structural understanding of language. This paradigm remains central in contemporary architectures, whether recurrent, convolutional, or transformer-based.

\begin{figure}[H]
\centering
\includegraphics[width=0.4\linewidth]{rnn_elman.png}
\caption{Simple recurrent network according to Elman~\cite{elman1990finding}. The hidden layer activations are passed to the context layer with a fixed weight of 1. The dashed connections represent the network’s trainable weights.}
\label{fig:elman_rnn}
\end{figure}

Finally, the article emphasizes that effective learning of hierarchical structures imposes a constraint on the network’s short-term memory. Elman even proposed a solution inspired by human development: start learning with simple sequences and progressively increase their complexity, an idea now known as \textit{curriculum learning}.

\subsection{Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling}

The article by Zhou et al.~\cite{zhou2015text} proposes an innovative architecture combining a bidirectional \gls{lstm} (BiLSTM) network with a two-dimensional max pooling operation, explicitly aimed at improving text classification.

The model first relies on bidirectional encoding via a BiLSTM, which enables capturing both past and future dependencies in a sequence simultaneously. Unlike traditional \glspl{rnn}, the BiLSTM preserves contextual information from both directions, which is particularly useful in text processing where semantic relationships may depend on words located before and after a target term.

The authors use pretrained \gls{glove} word vectors, constructed from 6 billion tokens from Wikipedia 2014 and the Gigaword 5 corpus. Words absent from this vocabulary are randomly initialized using a uniform distribution over the interval $[-0.25, 0.25]$. These embeddings are then fine-tuned during model training to adapt lexical representations to the classification task.

The major innovation introduced by the authors lies in the use of \textit{2D max pooling} applied to the BiLSTM outputs. This technique, inspired by convolutional neural networks, allows extracting the most salient features across the entire encoded sequence, thus reducing noise and increasing the robustness of the final classifier. Thanks to this combination, the model can capture not only temporal dependencies but also finer local interactions between hidden representations generated by the BiLSTM.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{bilstm.png}
    \caption{Architecture of the BLSTM-2DCNN model for a sequence of seven words. Word vectors have a dimension of 3, and the BiLSTM has 5 hidden units. The height and width of convolutional filters and max pooling operations are fixed at 2~\cite{zhou2015text}.}
    \label{fig:bilstm-arch}
\end{figure}

Experiments conducted on six text classification datasets (including SST-1, SST-2, TREC, etc.) show that the BLSTM-2DCNN model outperforms not only classic \glspl{rnn}, \glspl{cnn}, and RecNN (Recursive Neural Networks) models but also closely related variants such as BLSTM-2DPooling and the DSCNN (Deep Structured Convolutional Neural Network) proposed by Zhang et al.~(2016). Notably, it achieves the best accuracy on the SST-1 and SST-2 datasets, which are often used as benchmarks for fine-grained sentiment classification tasks.

A sensitivity analysis conducted on SST-1 also demonstrates that using larger convolutional filters enables capturing more complex semantic patterns, which can significantly improve performance. These results highlight the effectiveness of combining BiLSTM with 2D dimensionality reduction strategies, preserving both temporal information and the vector dimensions derived from word representations.

Thus, the BLSTM-2DCNN approach represents a notable advance in text classification, offering an efficient trade-off between representational richness and architectural simplicity, making it particularly promising for complex domains such as biomedicine or legal texts.

\subsection{BioBERT: A Pretrained Language Model for the Biomedical Domain}

The emergence of pretrained language models such as \gls{bert} has revolutionized natural language processing (\gls{nlp}), notably by reducing the need for task-specific architectures through a bidirectional attention mechanism. However, despite its general performance, \gls{bert} shows limitations in specialized domains such as biomedicine, where vocabulary, terminology, and syntactic structures differ significantly from general language.

To address this issue, Lee et al. (2020) introduced \gls{biobert}, a \gls{bert} variant specifically pretrained on large biomedical corpora, including PubMed abstracts (4.5 billion words) and full-text PMC articles (13.5 billion words). The goal was to adapt \gls{bert}'s semantic representations to the biomedical domain to improve performance on tasks such as Named Entity Recognition (NER), Relation Extraction (RE), and Question Answering (QA).

\paragraph{BioBERT Architecture.} \gls{biobert} fully inherits the \gls{bert}\textsubscript{BASE} architecture, composed of \textit{12 Transformer layers}, \textit{768 hidden dimensions}, \textit{12 attention heads}, and approximately \textit{110 million parameters}. There is therefore no structural modification compared to \gls{bert}; the improvement comes solely from the pretraining corpus and specialized vocabulary. \gls{biobert} is initialized with \gls{bert}\textsubscript{BASE} weights and continues pretraining with a masked language modeling (MLM) objective on biomedical texts.

\space{0.5cm}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{biobert.png}
    \caption{Overview of the pretraining and fine-tuning phases of BioBERT~\cite{lee2020biobert}.}
    \label{fig:biobert_architecture}
\end{figure}

Figure~\ref{fig:biobert_architecture} illustrates the overall process of building \gls{biobert}, structured in two main phases: pretraining and fine-tuning. The first phase consists of continuing to train the \gls{bert}\textsubscript{BASE} model on large specialized corpora such as PubMed and PMC using the masked language modeling task. This step aims to adapt the model’s linguistic representations to the biomedical domain specifics without modifying the original \gls{bert} architecture. The second phase, fine-tuning, corresponds to adapting the model to specific biomedical NLP tasks such as NER, RE, and QA. This visualization highlights that the performance gains of \gls{biobert} do not arise from architectural changes but exclusively from the type and quality of the corpus used during training, emphasizing the importance of specialized data in successfully adapting pretrained language models.

Lee et al. (2020) compared different corpus combinations for pretraining, demonstrating that the more biomedical the corpus, the better the model’s performance~\cite{lee2020biobert}:

\begin{table}[H]
\centering
\caption{BioBERT pretraining according to the textual corpora used~\cite{lee2020biobert}}
\scriptsize
\resizebox{0.8\textwidth}{!}{%
\begin{tabular}{|l|l|}
\hline
\textbf{Model} & \textbf{Corpus Used} \\
\hline
BERT (baseline) & Wiki + Books \\
BioBERT (+PubMed) & Wiki + Books + PubMed \\
BioBERT (+PMC) & Wiki + Books + PMC \\
BioBERT (+PubMed + PMC) & Wiki + Books + PubMed + PMC \\
\hline
\end{tabular}%
}
\end{table}

BioBERT evaluation was conducted on several benchmark datasets in biomedical natural language processing. These benchmarks are essential for testing model capabilities on specific tasks and allow measuring BioBERT’s performance on fundamental tasks for biomedical text analysis and interpretation. Among the main evaluated tasks are Named Entity Recognition (NER), Relation Extraction (RE), and Question Answering (QA).

The Named Entity Recognition (NER) task consists of identifying and classifying named entities present in a text. These entities may include terms such as diseases, drugs, genes, etc. Several datasets have been used for this task to test BioBERT’s ability to recognize medical entities. These include the NCBI Disease corpus, which contains biomedical articles annotated for diseases and is used to evaluate BioBERT’s ability to identify disease-related entities. The BC5CDR dataset focuses on extracting disease and chemical entities, emphasizing the relationship between diseases and chemicals. The JNLPBA dataset includes entities such as proteins and genes, which are essential in molecular biology. Lastly, the BioNLP13CG corpus, from the BioNLP-2013 competition, groups entities related to genomics, drugs, and other biology domains.

Regarding Relation Extraction (RE), another crucial biomedical NLP task, BioBERT was evaluated on datasets such as ChemProt and GAD. The RE task aims to identify and classify relationships between different entities in a text, such as connections between diseases and treatments or between genes and proteins. The ChemProt corpus contains relations between chemical substances and biomolecular targets, allowing evaluation of BioBERT’s ability to extract biological relations. The GAD (Genetic Association Database) dataset is used to detect relationships between genetic mutations and human diseases, an essential area in biomedical genetics.

Finally, in the Question Answering (QA) task, BioBERT was tested on the BioASQ benchmark, an annual competition dedicated to extracting medical answers from PubMed and other biomedical documents. Questions posed in BioASQ cover a wide range of medical and bioinformatics topics, testing BioBERT’s ability to extract relevant and precise information from complex scientific documents. This task is particularly important in contexts where physicians and researchers need rapid access to reliable answers based on vast amounts of biomedical information.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{result_biobert.png}
    \caption{(a) Effect of PubMed corpus size on pretraining. (b) BioBERT NER performance at different checkpoints. (c) Improvement achieved by BioBERT v1.0 (PubMed + PMC) compared to BERT~\cite{lee2020biobert}.}
    \label{fig:biobert_results}
\end{figure}

The authors of \gls{biobert} conducted a series of experiments to assess the impact of the volume of biomedical data used during pretraining. The results, shown in Figure~\ref{fig:biobert_results}(a), indicate that progressively increasing the size of the PubMed corpus leads to significant improvements in model performance on multiple Named Entity Recognition (NER) tasks. More precisely, F1 scores notably increase for the NCBI Disease, BC4CHEMD, and BC5CDR datasets. However, beyond a certain threshold — approximately 4.5 billion words — performance tends to plateau, indicating that the model reaches saturation in terms of benefits from additional data. This highlights that pretraining on a large biomedical corpus is beneficial, although marginal returns diminish as the corpus grows.

Figure~\ref{fig:biobert_results}(b) shows the evolution of \gls{biobert}'s performance during pretraining as a function of training steps. Performance generally improves with more steps, reflecting the model’s capacity to capture increasingly relevant biomedical representations. This trend is especially notable in tasks like NCBI Disease, where initial fluctuations stabilize over time, confirming the benefit of extended domain-specific pretraining.

Figure~\ref{fig:biobert_results}(c) compares \gls{biobert} with the general \gls{bert}, with performance gains plotted against fine-tuning dataset sizes (log scale). The largest improvements are seen in QA tasks (e.g., BioASQ), exceeding 6\%, followed by NER tasks with gains of 2--4\%. RE tasks show smaller yet consistent improvements. Notably, gains are most pronounced on smaller datasets, highlighting \gls{biobert}’s effectiveness when annotated data is scarce.

\newpage

\chapter{Architectures and Experiments}
\addcontentsline{toc}{chapter}{Architectures and Experiments}

\lhead{\MakeUppercase{Architectures and Experiments}}

\section{Data preprocessing and exploratory data analysis}

Data preprocessing is a key step in preparing texts for analysis. It includes several essential sub-steps, notably text cleaning, tokenization, lemmatization, and vectorization.

First, the abstracts are cleaned by removing irrelevant special characters while preserving certain mathematical symbols such as comparison operators. Then, the text is converted to lowercase and unnecessary spaces are eliminated to standardize the data.

Once the text is cleaned, it is tokenized, i.e., broken down into individual words. At this stage, lemmatization is applied. Each word is reduced to its base form (or lemma), which helps minimize term variability while preserving their fundamental meaning. For example, \textit{would eat}, \textit{we eat}, and \textit{to eat} would all be reduced to \textit{eat}. Additionally, stop words — terms without significant contextual meaning (such as \textit{and}, \textit{the}, \textit{of}) — are removed to retain only the words relevant for analysis.

The analyzed datasets contain several textual variables , described in Table~\ref{tab:variables}. Among these, the variable \texttt{Abstract} was selected as the sole source of information for the classification tasks. It contains a rich and comprehensive summary of the article, integrating most of the essential information — including content often redundant with keywords or MeSH terms. Consequently, no automatic feature selection procedure was necessary. This choice allows focusing the processing on a single coherent text sequence while ensuring standardization of the preprocessing pipeline.

This section presents an exploratory analysis of two datasets extracted from PubMed, containing scientific article abstracts related to various diseases. The goal is to better understand the data structure, identify class imbalances, and detect textual patterns that could influence model performance.

Both datasets share the same main variables, notably: a unique identifier (\texttt{PMID}), the \texttt{Title} of the article, the \texttt{Abstract}, the \texttt{Keywords}, the year of publication (\texttt{PublicationYear}), the \texttt{MeSH Terms}, and the class label (\texttt{Label}). A full description of these variables is available in the appendix (see Table~\ref{tab:variables}).

The first task concerns binary classification between articles related to malaria (class 1) and those concerning other diseases such as Alzheimer’s or Dengue (class 0). This dataset contains \textit{29,997} abstracts (see Figure~\ref{fig:word_distribution_binary_class}), collected between 1950 and 2024. The class distribution is imbalanced, with an overrepresentation of malaria-related articles. Regarding word counts, the number of words per abstract ranges from a minimum of 1 to a maximum of 4,985 words, with an average of about 243.86 words per abstract.

The second task is a multiclass classification involving \textit{42,879} abstracts, distributed across nine disease categories, ranging from infectious diseases (tuberculosis, cholera, leprosy, etc.) to non-infectious diseases (leukemia, asthma, Parkinson’s, etc.) (see Figure~\ref{fig:word_distribution_multi_class}). These data also cover the period from 1950 to 2024 and exhibit a marked class imbalance. For this task, the word counts per abstract vary from 0 (empty cells) to 3,814 words, with an average of 219.25 words per abstract.

It should be noted that infectious diseases are generally caused by external pathogens (bacteria, viruses, parasites), whereas non-infectious diseases often result from internal factors such as chronic, autoimmune, or genetic mechanisms.

Histograms of the word count distributions in the abstracts for these two tasks are shown in Figure~\ref{fig:class_distributions}. This analysis helps to better understand the variations in word counts, notably the differences between very short and more detailed abstracts, which could potentially affect classification model performance. The histograms of word distributions for each task are presented respectively in Figures~\ref{fig:word_distribution_binary_class} and~\ref{fig:word_distribution_multi_class} in the appendix.

\begin{center}
\begin{figure}[H]
\centering
\begin{minipage}{0.48\textwidth}
  \includegraphics[width=0.8\linewidth]{binary_class_distribution.png}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
  \includegraphics[width=0.8\linewidth]{multiclass_class_distribution.png}
\end{minipage}
\caption{Class distribution in the binary classification (left) and multiclass classification (right) datasets.}
\label{fig:class_distributions}
\end{figure}
\end{center}

Once texts are cleaned and standardized, the corpus is split into three distinct subsets to structure the training process. The dataset is divided into 70\% for training, 15\% for validation, and 15\% for testing. This separation ensures an objective evaluation of the developed models. The training set is used to fit model parameters, the validation set allows monitoring performance during training and detecting potential overfitting, while the test set provides a final estimate of the model’s ability to generalize to unseen data.

To better visualize the distribution of dominant terms in each classification task, word clouds were generated to illustrate the most frequent terms. In the binary classification word cloud, terms like \textit{parasite}, \textit{patient}, \textit{infection}, and \textit{disease} appear large for class 0, indicating these terms are particularly frequent and significant in abstracts related to this class. Conversely, words such as \textit{schistosomiasis}, \textit{prevalence}, and \textit{young} appear smaller, suggesting they are less frequent but still important in this context.

For class 1, words like \textit{malaria}, \textit{plasmodium}, and \textit{mosquito} are displayed prominently, highlighting their importance and frequency in malaria-related abstracts. Other terms such as \textit{prevalence}, \textit{disease}, and \textit{patient} appear smaller, indicating they are also present but to a lesser extent.

A similar reasoning applies to the multiclass classification. Here, the word clouds reflect unique characteristics of each disease category. For example, infectious diseases are represented by large terms like \textit{tuberculosis}, \textit{cholera}, and \textit{leprosy}, while non-infectious diseases are dominated by words such as \textit{leukemia}, \textit{asthma}, and \textit{Parkinson}. General terms such as \textit{disease}, \textit{prevalence}, and \textit{patient} are present in both categories, but their size varies according to their frequency in the different classes.

The results of the word clouds for each classification task are available in the appendix, respectively in Figures~\ref{fig:binary_word_cloud} and~\ref{fig:multi_word_cloud}. These visualizations help better understand the distribution of the most significant terms in both classification types and provide interesting insights into the semantic features of the textual data.

For experiments involving models trained without pre-existing embeddings (learn from scratch), as well as those using static vector representations such as \gls{glove} and \gls{fasttext}, a specific data preparation pipeline was developed. This pipeline includes several key steps: text tokenization, vocabulary creation from the training corpus, encoding sequences into numerical indices, and padding to ensure uniform sequence length. A minimum frequency threshold is applied to remove rare words, while out-of-vocabulary terms are replaced by a special \texttt{<UNK>} token. The sequences are then converted into tensors, making them compatible with neural network training. This approach aims to provide a normalized and consistent text representation, particularly suited for training lightweight models or recurrent architectures (\gls{lstm}/\gls{gru}), whether with embeddings learned during training or initialized with pretrained matrices like \gls{glove} or \gls{fasttext}.

For experiments involving models using contextual embeddings, a distinct pipeline is implemented. It relies on using an appropriate tokenizer that converts each raw text into a sequence of tokens, then generates input IDs and attention masks as required. The tokenizer also handles padding and truncation of sequences to a maximum length of 400 tokens, thus ensuring uniform input size. These data are then wrapped into a custom dataset class, preparing the texts for models using contextual embeddings. Classification labels are added as tensors, and each input is thus represented by token IDs, attention masks, and corresponding labels.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{binary_word_cloud.png}
\caption{Word cloud for binary classification, showing the most frequent terms associated with each class.}
\label{fig:binary_word_cloud}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{multi_word_cloud.png}
\caption{Word cloud for multi-class classification, illustrating dominant terms for each disease category (for 8 classes).}
\label{fig:multi_word_cloud}
\end{figure}

\section{Presentation of Architectures and Types of Embeddings}

To address the two classification tasks (binary and multi-class), we experimented with several neural network architectures, each combined with different textual representations. These combinations aim to capture varied linguistic and semantic features while considering constraints of complexity and computational time.

Three main families of approaches were explored in terms of text representation:

\begin{itemize}[label=--, itemsep=0pt, topsep=0pt]
    \item \textit{Without pre-trained embeddings} (also referred to as \textit{no embeddings}): word vectors are learned \textit{from scratch} during training. This approach is well-suited to domain-specific corpora, such as biomedical texts, but generally requires more data and training time to reach competitive performance.
    
    \item \textit{Static word embeddings}: use of pre-trained vectors such as \texttt{GloVe} (300 dimensions) and \texttt{FastText} (300 dimensions). These representations provide enriched semantics but remain fixed regardless of the context in which the word appears.
    
    \item \textit{Contextual embeddings}: use of models like \gls{bert}, particularly \gls{biobert} and \gls{pubmedbert}, which are specifically trained on biomedical texts. These models produce dynamic representations sensitive to the context of each word.
\end{itemize}

Regarding model architectures, the choices were based on theoretical considerations (language modeling capacity), practical aspects (inference and training time), and empirical evidence (performance observed in literature and on similar datasets). The tested combinations for each task are summarized below:

\begin{itemize}
    \item \textbf{Binary classification task}:
    \begin{itemize}
        \item \gls{lstm}: capable of capturing long-term sequential dependencies.
        \item \gls{gru}: a more efficient alternative to \gls{lstm}, with fewer parameters to train.
        \item Bidirectional \gls{gru} with Bahdanau attention: an attention mechanism allowing the model to dynamically focus on relevant parts of the abstract, improving interpretability and accuracy.
    \end{itemize}
    
    \item \textbf{Multi-class classification task}:
    \begin{itemize}
        \item \gls{lstm}, \gls{gru};
        \item \gls{cnn}-\gls{gru} and \gls{cnn}-\gls{lstm}: combinations that capture both local patterns (via \gls{cnn}) and global dependencies (via \gls{gru} or \gls{lstm});
        \item Bidirectional \gls{gru} with Bahdanau attention.
    \end{itemize}
\end{itemize}

Besides classification performance, evaluation also considered complexity criteria: training time, inference time, and memory load. These factors are especially critical in a biomedical context, where resources may be limited and responsiveness requirements high.

A detailed comparison between the different combinations of architectures and embeddings will be presented in the following sections, linked to the performance results and task-specific evaluation metrics.

\section{Training Methodology}

The model training followed a multi-step methodology inspired by standard practices for evaluating machine learning models, as described by Geurts and Wehenkel~\cite{geurts2006}. This approach includes the following steps:

\begin{enumerate}
    \item \textit{Model training}: All models were trained on the training dataset using different algorithms and/or varying complexity values. To enhance stability during training—particularly for recurrent architectures such as LSTM and GRU—\textbf{gradient clipping} was applied to constrain the norm of the gradients, effectively preventing exploding gradient issues. In addition, \textbf{dropout} regularization was employed within the architectures to reduce the risk of overfitting by randomly deactivating a subset of neurons during each training iteration.

    \item \textit{Best model selection}: After the initial training, the model that achieved the best performance on the validation set was selected. This step enables choosing the most effective model while avoiding overfitting by using a separate validation set.

    \item \textit{Hyperparameter optimization with Grid Search}: Following model selection, a hyperparameter optimization phase was conducted using the \textit{Grid Search} method. This method performs an exhaustive search over a predefined set of hyperparameters for each model to maximize performance. Using the validation set, Grid Search explores all possible hyperparameter combinations to find the best configuration, thus ensuring optimal model performance.

    \item \textit{Retraining on the full dataset}: Once the best hyperparameters were identified via Grid Search, the model was retrained using the entire available dataset, i.e., the combination of the training and validation sets. This step maximizes data utilization to improve model performance.

    \item \textit{Testing on the test set}: After retraining, the model was tested on a separate test set to evaluate its ability to generalize to new data, providing an accurate estimate of model performance.

    \item \textit{Final retraining}: Finally, after the final evaluation, the model was retrained one last time using all available data. This last step aims to make the best use of the data before applying the model to new datasets or real-world scenarios.
\end{enumerate}

This methodology ensures that the model is not only performant but also robust and capable of generalizing to unseen data by following a rigorous training and validation process. The use of gradient clipping and dropout further strengthens the training procedure by enhancing numerical stability and reducing overfitting, respectively.


\section{Evaluation Metrics for Biomedical Text Classification}

In the context of text classification, particularly in specialized domains such as biomedicine, the choice of evaluation metrics is crucial for reliably measuring model performance. As highlighted by Saito et al.~\cite{saito2015precision}, traditional metrics like \textbf{accuracy} can be misleading in cases of class imbalance, which is common in biomedicine or in low-data learning scenarios.

\textbf{Accuracy}, defined as the ratio of correct predictions to the total number of predictions, remains the most widely used metric. However, its effectiveness significantly decreases when classes are imbalanced. As noted by Japkowicz and Stephen~\cite{japkowicz2002class}, a model can achieve high accuracy simply by predicting the majority class, without truly learning to discriminate minority classes.

To address these limitations, \textbf{balanced accuracy} or ACSA (Average Class-Scaled Accuracy) was introduced as a fairer alternative. It is defined as the average of the recalls for each class, thus allowing robust evaluation even when some classes are heavily underrepresented~\cite{brodersen2010balanced}. The formula for balanced accuracy in the binary case is given by:

\[
\textit{Balanced Accuracy} = \frac{1}{2} \left( \frac{TP}{TP + FN} + \frac{TN}{TN + FP} \right)
\]

where \( TP \) is the number of true positives, \( FN \) the number of false negatives, \( TN \) the number of true negatives, and \( FP \) the number of false positives.

In the multi-class classification case, balanced accuracy is computed by averaging the recall for each class:

\[
\textit{Balanced Accuracy} = \frac{1}{C} \sum_{i=1}^{C} \frac{TP_i}{TP_i + FN_i}
\]

where \( C \) is the total number of classes and \( TP_i \), \( FN_i \) are respectively the true positives and false negatives for class \( c_i \).

\textbf{Precision} and \textbf{recall} are two complementary metrics: the former measures the proportion of true positives among positive predictions, while the latter measures the proportion of true positives correctly identified among all actual positives. The \textbf{F1-score}, their harmonic mean, is recommended in recent works on medical text classification, notably by Jin et al.~\cite{jin2019recurrent}, as it penalizes imbalances between precision and recall.

Finally, the \gls{auc} (Area Under the Curve), more specifically the AUC-ROC (Receiver Operating Characteristic), is a metric used to evaluate a model's ability to distinguish between positive and negative classes. It measures the area under the ROC curve, which plots the true positive rate against the false positive rate. An \gls{auc} close to 1 indicates excellent discriminatory power, whereas an AUC of 0.5 corresponds to random performance. As shown by Saito and Rehmsmeier~\cite{saito2015precision}, in contexts with strong class imbalance—such as biomedicine—the AUC-PR (Precision-Recall) is often more informative than AUC-ROC, since it focuses more on performance regarding the rare positive class.

\section{Experiments}

The study is divided into two main categories: \textit{binary} classification and \textit{multi-class} classification. This separation is based on the nature of the tasks, prediction objectives, model output architectures, and loss functions used.

In \textbf{binary classification}, each example belongs to one of two possible classes: \texttt{Malaria} or \texttt{Non-Malaria} (\autoref{tab:binary_classes}). The model uses a single output \( \hat{y} \in [0,1] \), interpreted as the probability that the input belongs to the positive class. The \textit{Binary Cross-Entropy (BCE) loss function} is used, defined as:

\begin{equation}
\mathcal{L}_{\text{BCE}}(y, \hat{y}) = - \left( y \cdot \log(\hat{y}) + (1 - y) \cdot \log(1 - \hat{y}) \right),
\end{equation}

where \( y \in \{0,1\} \) is the true class and \( \hat{y} \) is the model prediction.

Conversely, in \textbf{multi-class classification}, each input belongs to exactly one class among \( K > 2 \) classes (e.g., 9 different diseases, see \autoref{tab:model_performance_multiclass}). The model produces a vector of scores \( \hat{\mathbf{y}} = (\hat{y}_1, \hat{y}_2, \ldots, \hat{y}_K) \) normalized by a \textit{softmax function}:

\begin{equation}
\hat{y}_k = \frac{\exp(z_k)}{\sum_{j=1}^{K} \exp(z_j)},
\end{equation}

where \( z_k \) is the unnormalized output (logit) for class \( k \). The \textit{Cross-Entropy (CE) loss function} is then:

\begin{equation}
\mathcal{L}_{\text{CE}}(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{k=1}^{K} y_k \log(\hat{y}_k),
\end{equation}

where \( \mathbf{y} \) is the one-hot vector of the true class.

This structural distinction influences several aspects of the experimental pipeline:
\begin{itemize}[label=--, itemsep=0pt, topsep=0pt]
    \item \textbf{Output architecture}: a single output with sigmoid activation for the binary case, versus a \( K \)-neuron layer with softmax for the multi-class case.
    \item \textbf{Loss function}: BCE for binary, CE for multi-class.
    \item \textbf{Evaluation metrics}: metrics such as AUC, F1-score, or precision are used differently depending on the case.
\end{itemize}

For these reasons, we present separately the experiments related to binary classification and those concerning multi-class classification.

Moreover, we apply early stopping to prevent overfitting, as commonly done in deep learning practice~\cite{prechelt1998early, yao2020automl}.

\subsection{Binary classification experiments}

\subsubsection{Experiment 0 : LSTM and GRU models with embeddings learned from scratch}
\subsubsection*{a. t-SNE visualization before training (Embeddings learned from scratch)}
\addcontentsline{toc}{subsubsection}{t-SNE visualization before training (Embeddings learned from scratch)}

Before training the models, we visualized the raw embeddings using t-SNE (t-distributed Stochastic Neighbor Embedding) to assess whether any initial semantic structure was present.

Since these embeddings are initialized randomly and learned from scratch, we expect the word representations to be uniformly distributed in the vector space without any meaningful clusters. This is confirmed by the t-SNE projection: words are scattered without forming clearly separable groups corresponding to different classes.

This visualization validates that the network starts from an unstructured representation and must learn relevant semantic relationships entirely through training. It also highlights the capacity of recurrent models to organize such raw representations into structured, class-discriminative feature spaces over time.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{bi_gru_tsne_before.png} % Replace with your actual image filename
    \caption{t-SNE projection of embeddings before training (randomly initialized). No meaningful separation between classes is observed.}
    \label{fig:tsne_before_training_scratch}
\end{figure}

\paragraph{b. Validation phase}
\addcontentsline{toc}{paragraph}{Validation phase}

Figure~\ref{fig:binary_validation_phase_results} presents the validation curves — loss, accuracy (balanced accuracy or ACSA), and F1-score — for binary classification using GRU and LSTM models trained with embeddings learned from scratch.

Both GRU and LSTM models demonstrate a clear learning progression. For the \textbf{GRU model}, the validation loss drops sharply after epoch 5, stabilizing near 0.1, while the validation ACSA (accuracy) rises rapidly to $\sim$0.95. The F1-score also increases steeply, reaching over 0.85 by epoch 7, indicating good precision-recall balance and strong class separation.

The \textbf{LSTM model} exhibits a similar performance trajectory, with validation loss decreasing to $\sim$0.2 and ACSA nearing 0.94. Its F1-score also stabilizes around 0.85 after epoch 8, confirming its effectiveness in binary classification tasks. However, minor fluctuations in validation loss suggest slight sensitivity to training dynamics.

Both models show \textbf{no significant signs of overfitting}, with validation curves closely following training trends. The GRU model shows slightly faster convergence, while LSTM offers comparable peak performance with a marginally slower training dynamic.

\begin{table}[H]
\centering
\small
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{0.8}
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{|l|c|c|c|l|}
\hline
Model     & Max F1-score & Stability  & Speed    & Recommendation       \\
\hline
GRU       & $\sim$0.87   & Very good  & Fast     & Strong choice for binary classification \\
LSTM      & $\sim$0.85   & Good       & Moderate & Reliable alternative with stable results \\
\hline
\end{tabular}
}
\caption{Summary of GRU and LSTM performance on binary classification task.}
\label{tab:binary_validation_summary}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{bi_lstm_gru_val.png}
\caption{Validation metrics (loss, accuracy, F1-score) for GRU and LSTM models on binary classification with embeddings learned from scratch.}
\label{fig:binary_validation_phase_results}
\end{figure}

\paragraph{c. Optimization phase: hyperparameter selection}
\addcontentsline{toc}{paragraph}{Optimization phase: hyperparameter selection}

This experiment investigates optimal hyperparameters for binary biomedical text classification using GRU and LSTM models trained with embeddings learned from scratch. Hyperparameter tuning was conducted using grid search over various configurations.

Optimizers evaluated include Adam, RMSprop, and SGD, with Adam demonstrating superior convergence and performance. Learning rates between $10^{-4}$ and $10^{-3}$ were tested, with $10^{-3}$ selected for both models due to a good balance of speed and stability. The embedding dimension was set to 200 for optimal trade-off between expressiveness and computation. Batch sizes of 16 and 32 were tested, with 32 selected for improved gradient estimates. Both models employed a single-layer architecture for efficient training and lower overfitting risk.

As shown in Figure~\ref{fig:optimization_binary}, GRU consistently outperformed LSTM in terms of both training and validation loss reduction, as well as faster and more stable accuracy gains. The GRU model achieved >90\% validation accuracy in fewer epochs, with smaller variance across folds. LSTM, while slightly slower to converge, achieved comparable validation performance with slightly higher fluctuation.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{bi_gru_lstm_opt.png}
\caption{Training and validation loss and accuracy during hyperparameter tuning for binary classification using GRU and LSTM models with embeddings learned from scratch. Shaded areas represent standard deviation across folds.}
\label{fig:optimization_binary}
\end{figure}

These results confirm that both architectures can be effectively optimized for binary classification with raw embeddings. The GRU model showed slightly superior generalization and convergence properties under the tested settings.

\subsubsection*{c. Test Phase: Model Performances}
\addcontentsline{toc}{subsubsection}{Test Phase: Model Performances}

The performance of the LSTM and GRU models trained with embeddings learned from scratch is summarized in Table~\ref{tab:model_performance_scratch}, and performance evolution over epochs is shown in Figure~\ref{fig:bi_test_performance}. The optimal hyperparameters selected for each model were identified during the tuning phase.

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.2}
\resizebox{1.05\textwidth}{!}{%
\begin{tabular}{|l|l|c|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Embedding Type} & \textbf{Balanced Acc} & \textbf{F1 Score} & \textbf{Precision} & \textbf{Recall} & \textbf{AUC} & \textbf{Inf. Time (s)} & \textbf{Train Time (min)} \\
\hline
LSTM & Learned (from scratch, Raw Text, 200d) & 89.50\% & 84.60\% & 75.66\% & \textbf{95.94\%} & 0.92 & 0.05 & 0.44 \\
\hline
GRU & Learned (from scratch, Raw Text, 200d) & \textbf{90.69\%} & \textbf{87.11\%} & \textbf{82.92\%} & 91.74\% & \textbf{0.96} & \textbf{0.04} & \textbf{0.37} \\
\hline
\end{tabular}
}
\caption{Test performance of LSTM and GRU models with embeddings learned from scratch (binary classification).}
\label{tab:model_performance_scratch}
\end{table}

\paragraph{Evaluation of Model Generalization on the Test Set with Embeddings Learned from Scratch:}

Figure~\ref{fig:bi_test_performance} presents the test performance curves for both LSTM and GRU models, including metrics such as test loss, F1-score, balanced accuracy, precision, and recall.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{bi_test_lstm_gru.png}
    \caption{Test performance curves for LSTM and GRU trained with embeddings learned from scratch. Metrics shown include loss, F1-score, balanced accuracy, precision, and recall.}
    \label{fig:bi_test_performance}
\end{figure}

The GRU model exhibits rapid convergence within the first few epochs and maintains strong, stable performance across all key metrics. It achieves a peak balanced accuracy of 90.69\% and an F1-score of 87.11\%, indicating effective handling of class imbalance. Precision and recall curves are well-aligned, suggesting robust and reliable classification across both classes. The low inference time (0.04 s) further supports its practicality in real-time or resource-constrained clinical applications.

The LSTM model follows a similar trajectory but displays more pronounced variation in precision throughout training. Despite achieving a very high recall (95.94\%), its lower precision (75.66\%) leads to a lower overall F1-score (84.60\%). This indicates a higher rate of false positives, which may be acceptable or even desirable in certain diagnostic contexts where missing a positive case has severe consequences.

In terms of computational efficiency, the GRU again demonstrates superiority, with both faster training (0.37 min vs. 0.44 min) and inference. The compact structure of GRU contributes to its faster learning dynamics and slightly better generalization under the same conditions.

Hence, the comparison reveals that both models are capable of learning meaningful class-discriminative features when trained from scratch. GRU stands out as a balanced performer across all metrics, combining high accuracy with computational efficiency. LSTM, on the other hand, is advantageous in high-recall scenarios, making it a suitable candidate for safety-critical biomedical applications where sensitivity is paramount.

These findings underscore the importance of evaluating a range of performance metrics—particularly precision, recall, and balanced accuracy—when developing models for imbalanced binary classification tasks in biomedical text analysis.

Figure~\ref{fig:bi_test_perf} presents the test performance curves for LSTM and GRU models across key metrics such as test loss, F1-score, balanced accuracy, precision, and recall.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{bi_lstm_gru_perf.png}
    \caption{Test performance of LSTM and GRU models trained with embeddings learned from scratch. Plots show the evolution of loss, F1-score, balanced accuracy, precision, and recall over training epochs.}
    \label{fig:bi_test_perf}
\end{figure}

\paragraph{Latent Space Visualization with t-SNE}
\addcontentsline{toc}{paragraph}{Latent Space Visualization with t-SNE}

To qualitatively evaluate the hidden representations learned by GRU and LSTM models during binary classification, we performed t-SNE visualization of the hidden states after training. This provides insight into how well each model separates the two classes in its latent space.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{bi_gru_lstm_tsne_after.png}
\caption{t-SNE projections of the hidden states for GRU (left) and LSTM (right) models after training on binary classification. Each point represents a document’s hidden state during the forward pass, colored by class label.}
\label{fig:tsne_binary_gru_lstm}
\end{figure}

As shown in Figure~\ref{fig:tsne_binary_gru_lstm}, both GRU and LSTM models form discernible class-specific clusters in the latent space after training. The GRU model (left) achieves compact, well-separated clusters, indicating its effectiveness in encoding class-distinctive features. The LSTM model (right) also shows separation, though its latent space is more elongated and curved, suggesting a different internal representation strategy.

These t-SNE plots visually support the quantitative test results, where GRU outperformed LSTM in most metrics. The stronger class separability in the GRU’s latent space confirms its superior capacity to learn discriminative features for binary biomedical text classification.

\vspace{1em}
\noindent
The results of Experiment 0 demonstrate that GRU and LSTM models trained from scratch can learn meaningful document representations for binary classification, with GRU showing superior efficiency and generalization. However, the performance may be further enhanced by leveraging pretrained word embeddings, which provide rich semantic priors and improve convergence. Therefore, in the next experiment, we assess the impact of integrating pretrained embeddings (GloVe and FastText) into the LSTM and GRU architectures.


\subsubsection{Experiment 1: LSTM and GRU models with word embeddings: GloVe and FastText}
\addcontentsline{toc}{subsubsection}{Experiment 1: LSTM and GRU models with word embeddings: GloVe and FastText}

\subsubsection*{a. t-SNE visualization before training (with word embeddings)}
\addcontentsline{toc}{subsubsection}{t-SNE visualization of GloVe and FastText embeddings before training}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{merged_tsne_horizontal.png}
    \caption{t-SNE projection of biomedical text inputs using GloVe (left) and FastText (right) pre-trained embeddings before training. Points are colored by binary class label.}
    \label{fig:tsne_binary_pretrained_before}
\end{figure}

Figure~\ref{fig:tsne_binary_pretrained_before} presents the t-SNE projections of biomedical text inputs represented by pre-trained GloVe and FastText word embeddings, in the context of binary classification, prior to any supervised model training. Each point corresponds to a document projected into two dimensions, and is colored according to its binary class label (0 or 1).

The left panel shows the distribution for GloVe embeddings, while the right panel displays FastText. Although both are pre-trained on general language corpora, they exhibit slightly different clustering behaviors. GloVe embeddings result in a more uniform spatial spread with subtle class concentration in certain areas, while FastText shows more pronounced and tighter local groupings, albeit still with considerable class overlap.

These projections highlight that pre-trained embeddings already contain latent semantic structures that partially align with the classification task. However, the overlapping regions demonstrate that these embeddings alone are not sufficient for strong class separation. This justifies the use of downstream neural models such as LSTM or GRU, which further refine these representations using task-specific supervision and sequential context.

\subsubsection*{b. Validation phase}
\addcontentsline{toc}{subsubsection}{Validation phase}

\paragraph{Validation phase (with FastText embeddings):}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{bi_gru_lstm_glove_fasttext_validation.png}
    \caption{Validation curves (loss, ACSA, F1-score) for GRU and LSTM models trained with 300d FastText embeddings on binary classification.}
    \label{fig:validation_phase_binary_fasttext}
\end{figure}

Figure~\ref{fig:validation_phase_binary_fasttext} presents the validation curves of the GRU and LSTM models during training for the binary biomedical classification task, using 300-dimensional FastText embeddings.

The GRU model converges swiftly in under 8 epochs. Validation loss decreases sharply, and both ACSA and F1-score improve significantly after epoch 5, reaching a peak F1-score around 0.84. The training appears stable, indicating a good synergy between FastText embeddings and the GRU's sequential modeling capacity.

Similarly, the LSTM model shows strong and consistent performance. The F1-score peaks at approximately 0.86, with ACSA and loss trends confirming good generalization. Though training continues slightly longer than GRU (up to 12 epochs), convergence remains smooth and efficient.

Both models demonstrate that FastText embeddings, enriched with subword information, enable rapid and stable learning for binary classification tasks. The LSTM model edges out slightly in performance, though GRU offers faster convergence with lower complexity.

\begin{table}[H]
\centering
\small
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{0.8}
\resizebox{0.7\textwidth}{!}{
\begin{tabular}{|l|c|c|c|l|}
\hline
Model     & Max F1-score & Stability  & Speed    & Remarks                    \\
\hline
GRU       & $\sim$0.84   & Very good  & Very Fast & Strong early convergence   \\
LSTM      & $\sim$0.86   & Excellent  & Fast      & Best performance overall    \\
\hline
\end{tabular}
}
\caption{Summary of GRU and LSTM performance during validation phase (FastText embeddings, binary classification).}
\label{tab:validation_binary_summary_fasttext}
\end{table}

\subsubsection*{c. Optimization phase : hyperparameter selection}
\addcontentsline{toc}{subsubsection}{Optimization phase : hyperparameter selection}


\subsubsection*{d. Test Phase : model performances}
\addcontentsline{toc}{subsubsection}{Test Phase : model performances}























\subsubsection{Experiment 2 : Bidirectional GRU models with Bahdanau attention with word embeddings : GloVe and FastText }




\subsubsection{Experiment 3 : Bidirectional GRU models with Bahdanau attention with contextual embeddings : PubMedBERT and BioBERT}



\subsubsection{Experiment 4 : Impact of class imbalance: resampling and Loss Weighting}



\subsubsection{Experiment 5 : Few-shot learning}




\subsection{Multi-class classification experiments}

\subsubsection{Experiment 0: LSTM, GRU, CNN-GRU and CNN-LSTM models with Embeddings Learned from Scratch}

\subsubsection*{a. t-SNE visualization before training (Embeddings learned from scratch)}
\addcontentsline{toc}{subsubsection}{t-SNE visualization before training (Embeddings learned from scratch)}

Figure~\ref{fig:tsne_before_training} shows a \textit{t-distributed stochastic neighbor embedding (t-SNE)} projection of textual embeddings before training the GRU and LSTM models. The embeddings are learned from scratch, without pre-trained vectors such as word2vec or BERT. This visualization provides insight into the initial latent structure of raw text data and the distribution of classes prior to training.

Since embeddings are randomly initialized or minimally structured, the t-SNE plot reveals no meaningful semantic or categorical separation. Points of different colors, each representing a distinct class, are distributed rather homogeneously. This indicates the input vectors at this stage carry little discriminative information.

The strong overlap and color mixing confirm that the model must learn to differentiate classes during training by simultaneously constructing semantic representations and classification boundaries. This explains why hybrid architectures like CNN-GRU may need longer training or show more variability.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{tsne_before_training_gru_no_emb.png}
    \caption{t-SNE visualization of text embeddings before training GRU and LSTM models (embeddings learned from scratch) before training.}
    \label{fig:tsne_before_training}
\end{figure}

\subsubsection*{b. Validation phase}
\addcontentsline{toc}{subsubsection}{Validation phase}

Figure~\ref{fig:validation_phase_results} presents validation results — loss, accuracy, and F1-score — for GRU, LSTM, CNN-LSTM, and CNN-GRU models trained on multi-class biomedical text classification with embeddings learned from scratch.

The GRU model shows a steady decrease in loss, stabilizing around 0.5 by epoch 13, while accuracy settles near 0.8 and F1-score rises to approximately 0.75. The performance is stable with no signs of overfitting.

The LSTM model follows a similar loss trend and accuracy level, but the F1-score fluctuates more, suggesting less consistency in class prediction.

The CNN-LSTM model decreases loss more gradually, reaching 0.5 by epoch 30. Accuracy and F1-score both improve steadily to around 0.85, indicating the benefit of combining convolutional and recurrent layers.

The CNN-GRU model shows slower initial improvements but ends with the best accuracy (~0.88) and F1-score (~0.9). However, its training is less stable and requires careful tuning.

\begin{table}[H]
\centering
\small % ou \scriptsize pour encore plus petit
\setlength{\tabcolsep}{4pt} % réduit l'espacement horizontal
\renewcommand{\arraystretch}{0.8} % réduit l'espacement vertical
\resizebox{0.8\textwidth}{!}{ % adapte à 80% de la largeur du texte
\begin{tabular}{|l|c|c|c|l|}
\hline
Model     & Max F1-score & Stability  & Speed    & Recommendation       \\
\hline
GRU       & $\sim$0.75 & Good       & Fast     & Reliable baseline    \\
LSTM      & $\sim$0.75 & Moderate   & Fast     & Acceptable choice    \\
CNN-LSTM  & $\sim$0.85 & Very good  & Moderate & Best overall         \\
CNN-GRU   & $\sim$0.90 & Low        & Slow     & Needs careful tuning \\
\hline
\end{tabular}
}
\caption{Summary of model performance during validation phase.}
\label{tab:validation_summary}
\end{table}

The validation curves are illustrated for all models in the figure~\ref{fig:validation_phase_results} .

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Validation_phase_no_emb.png}
    \caption{Validation curves (loss, accuracy, F1-score) for GRU, LSTM, CNN-LSTM, and CNN-GRU models trained with embeddings learned from scratch.}
    \label{fig:validation_phase_results}
\end{figure}

\subsubsection*{c. Optimization phase : hyperparameter selection}
\addcontentsline{toc}{subsubsection}{Optimization phase : hyperparameter selection}

This experiment assesses GRU, LSTM, CNN-GRU, and CNN-LSTM architectures for biomedical text classification, with embeddings learned from scratch. Hyperparameters were tuned through grid search.

Optimizers Adam, AdamW, and RMSprop were compared; Adam was chosen for its robustness and convergence speed. Learning rates were tested within the interval from $10^{-4}$ to $10^{-3}$, balancing speed and stability. Embedding dimensions were selected between 100 and 500, with larger dimensions generally improving the model’s capacity to represent the data. Model depth varied from 1 to 3 recurrent layers, with deeper models providing enhanced representational power. Batch sizes of 8, 16, and 32 were tested, with 32 providing smoother training and better generalization. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Opti_phase_no_emb.png}
    \caption{Training and validation performance during hyperparameter optimization phase for GRU, LSTM, CNN-GRU, and CNN-LSTM models with embeddings learned from scratch.}
    \label{fig:optimization_no_embeddings}
\end{figure}

As shown in Figure~\ref{fig:optimization_no_embeddings}, hybrid models CNN-GRU and CNN-LSTM outperform standalone recurrent architectures, highlighting the advantage of convolutional feature extraction for biomedical text. The CNN-LSTM model achieved a validation F1-score above 0.85 under optimal settings.

These results demonstrate that high classification performance is achievable when embeddings are learned from scratch, provided thorough hyperparameter optimization is performed, which is especially critical in complex biomedical NLP tasks.

\subsubsection*{d. Test Phase : model performances}
\addcontentsline{toc}{subsubsection}{Test Phase : model performances}

The performance of the CNN-LSTM, CNN-GRU, LSTM, and GRU models trained with embeddings learned from scratch is summarized in Table~\ref{tab:performances_multiclass} and illustrated in figure~\ref{fig:multi_test_performance}. The optimal hyperparameters selected for each model are detailed in Table~\ref{tab:hyperparams_multiclass}.

\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|c|c|c|c|c|c|c|}
\hline
Model & Embedding Type & Accuracy & F1 Score & Balanced Accuracy & Precision & Recall & Training Time (min) & Inference Time (s) \\ \hline
CNN-LSTM & Learned (from scratch, 250d) & \textbf{89.79\%} & \textbf{89.92\%} & \textbf{90.00\%} & \textbf{90.32\%} & \textbf{89.79\%} & 16.5 & 0.04 \\ \hline
CNN-GRU & Learned (from scratch, 250d) & 89.67\% & 89.83\% & 89.95\% & 90.29\% & 89.67\% & 10.4 & \textbf{0.02} \\ \hline
LSTM & Learned (from scratch, 250d) & 82.30\% & 82.28\% & 83.05\% & 83.82\% & 82.30\% & \textbf{9.0} & 0.07 \\ \hline
GRU & Learned (from scratch, 250d) & 88.62\% & 88.69\% & 88.84\% & 89.07\% & 88.62\% & 9.0 & 0.03 \\ \hline
\end{tabular}
}
\caption{Performance of models with embeddings learned from scratch on the test set for multiclass classification.}
\label{tab:performances_multiclass}
\end{table}

These results show that the CNN+LSTM and LSTM models achieve the highest accuracy and F1 scores, close to 88\%. The GRU model, although slightly less performant, remains competitive due to shorter training and inference times. The CNN+GRU model is the fastest at inference, albeit with a minor drop in performance.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{test_no_embed.png}
    \caption{Test performance curves for LSTM, GRU, CNN+GRU, and CNN+LSTM models trained with embeddings learned from scratch. Metrics shown include loss, F1-score, balanced accuracy, precision, and recall.}
    \label{fig:multi_test_performance}
\end{figure}

\paragraph{Evaluation of Model Generalization on the Test Set with Embeddings Learned from Scratch :}

Figure~\ref{fig:multi_test_performance} illustrates the evolution of key evaluation metrics on the test set for all four models. These include test loss, F1-score, balanced accuracy, precision, and recall, providing a comprehensive view of their generalization capabilities.

The GRU model shows rapid convergence within the first 10 epochs. It achieves strong and stable performance in terms of F1-score and balanced accuracy, both exceeding 0.9 by the end of training. Precision and recall are well-aligned, indicating robust generalization. This model proves particularly effective when embeddings are trained from scratch.

The LSTM model follows a similar trajectory, reaching high recall and balanced accuracy (around 0.9). However, precision fluctuates more significantly, suggesting some inconsistency in predicting specific classes. While the loss curve remains smooth, the variability in precision may reflect sensitivity to class imbalance or borderline examples.

The CNN-LSTM model demonstrates a slower start but ultimately outperforms the others across nearly all metrics. After epoch 12, a sharp improvement is observed in F1-score, precision, and recall, indicating that the convolutional layers enhance LSTM’s representational capacity. Its final performance is both strong and consistent.

The CNN-GRU model converges more slowly and initially exhibits lower performance across all metrics. A substantial improvement appears after epoch 18, with F1-score and accuracy approaching the levels achieved by other models. This late-stage progress suggests potential, but also highlights the need for careful hyperparameter tuning or extended training to unlock its full performance.

Conclusion: The CNN-LSTM architecture stands out as the most effective configuration when embeddings are learned from scratch. GRU serves as a solid and reliable baseline, while CNN+GRU, despite eventual improvement, requires more careful optimization. These findings highlight the importance of monitoring multiple evaluation metrics—especially precision and recall—rather than relying solely on loss or accuracy when assessing model performance in imbalanced biomedical text classification tasks.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{No_Embed_Performances.png}
\caption{Performance of GRU, LSTM, CNN-GRU, and CNN-LSTM models on the test set (embeddings learned from scratch).}
\label{fig:Multi_no}
\end{figure}

\paragraph{Latent Space Visualization with t-SNE :}

To gain qualitative insights into the learned representations, we visualized the document embeddings before and after training using t-SNE with model-based GRU.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{gru_tsne_after_training.png}
\caption{t-SNE projection of the document embeddings after training the GRU model (embeddings learned from scratch).}
\label{fig:tsne_gru_after_training}
\end{figure}

As shown in Figure~\ref{fig:tsne_gru_after_training}, after training the GRU model, the latent space becomes more structured: class-specific clusters emerge more distinctly, although some overlap remains. This demonstrates that even starting from untrained embeddings, the GRU can build a moderately discriminative feature space. This supports the observed quantitative performance and shows the model’s ability to structure the semantic space during training.


\subsubsection{Experiment 1 : LSTM, GRU, CNN-GRU and CNN-LSTM models with Word Embeddings: GloVe and FastText}

\subsubsection*{a. t-SNE visualization before training (with word embeddings)}
\addcontentsline{toc}{subsubsection}{t-SNE visualization Visualization of word embeddings before Training (with GloVe and FastText)}

Figure~\ref{fig:tsne_pretrained_before} displays the t-SNE projection of biomedical text inputs represented by pre-trained word embeddings (GloVe and FastText) prior to any model training. This visualization highlights the initial semantic distribution captured by the embeddings.

Despite the absence of task-specific fine-tuning, we observe an early separation tendency between some class clusters, particularly for classes 0 and 4, which tend to occupy denser regions. However, the global distribution remains largely overlapping, indicating that the embeddings encode general semantic similarity rather than task-specific discriminative features at this stage.

This observation confirms that while pre-trained embeddings provide a solid semantic foundation, their discriminative power for multi-class classification tasks requires further refinement through supervised training. It also justifies the integration of recurrent or convolutional layers capable of contextualizing and enhancing these initial representations for improved class separability.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{word_embedding_before_training_tsne.png}
    \caption{t-SNE projection of pre-trained word embeddings (GloVe and FastText) before training. Each point represents a text input projected from its word embedding space into two dimensions, colored by class label.}
    \label{fig:tsne_pretrained_before}
\end{figure}


\subsubsection*{b. Validation phase}
\addcontentsline{toc}{subsubsection}{Validation phase}

\paragraph{Validation phase (with GloVe embeddings) : }

Figure~\ref{fig:validation_phase_glove} presents the validation performance of GRU, LSTM, CNN-LSTM, and CNN-GRU models on the biomedical classification task, this time using 300-dimensional GloVe embeddings as input representations.

The GRU model rapidly converges within 8 epochs, with validation loss dropping below 0.5 and F1-score approaching 0.85. Accuracy follows a similar trend, suggesting effective utilization of pre-trained embeddings and efficient learning.

The LSTM model performs similarly, achieving a validation F1-score of ~0.87. The training is stable, and both loss and accuracy curves indicate robust generalization across classes.

The CNN-LSTM model displays more gradual convergence but achieves a strong F1-score nearing 0.9. The combined use of convolutional filters and recurrent memory appears beneficial, especially in extracting local and sequential semantic patterns from the pre-trained embeddings.

The CNN-GRU model also reaches a high F1-score of ~0.88. However, as observed previously, the early training phases show fluctuations, indicating sensitivity to initial conditions or learning rate schedules. Despite this, its final performance is competitive and promising.

\begin{table}[H]
\centering
\small
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{0.8}
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{|l|c|c|c|l|}
\hline
Model     & Max F1-score & Stability  & Speed    & Recommendation       \\
\hline
GRU       & $\sim$0.85 & Excellent  & Very Fast & Best lightweight option \\
LSTM      & $\sim$0.87 & Very good  & Fast      & Balanced and robust     \\
CNN-LSTM  & $\sim$0.90 & Excellent  & Moderate  & Strongest overall       \\
CNN-GRU   & $\sim$0.88 & Moderate   & Moderate  & Competitive with tuning \\
\hline
\end{tabular}
}
\caption{Summary of model performance during validation phase using GloVe embeddings.}
\label{tab:validation_summary_glove}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{we_val_glove.png}
    \caption{Validation curves (loss, accuracy, F1-score) for GRU, LSTM, CNN-LSTM, and CNN-GRU models trained with pre-trained GloVe (300d) word embeddings.}
    \label{fig:validation_phase_glove}
\end{figure}

\paragraph{Validation phase (with FastText embeddings):}

Figure~\ref{fig:validation_phase_fasttext} presents the validation performance of GRU, LSTM, CNN-LSTM, and CNN-GRU models on the biomedical classification task, this time using 300-dimensional FastText embeddings as input representations.

The GRU model converges quickly, with loss stabilizing below 0.5 within the first 8 epochs. Its accuracy exceeds 0.85 early in training, and the F1-score also reaches approximately 0.85, suggesting strong and efficient learning when initialized with FastText representations. The model maintains stable performance throughout the training.

The LSTM model displays comparable results, achieving an F1-score near 0.87 by epoch 12. Loss and accuracy curves show smooth convergence, and the model generalizes well across categories, confirming its effectiveness with FastText embeddings.

The CNN-GRU model exhibits a rapid learning phase in the first five epochs, with the F1-score rising quickly to around 0.87. The loss and accuracy curves reflect stable training, with only minor oscillations, indicating improved stability compared to its performance with GloVe or learned-from-scratch embeddings.

The CNN-LSTM model shows a slightly slower convergence pattern, but ultimately achieves the best performance across the board. The F1-score peaks near 0.9, and accuracy remains consistently high. The loss curve fluctuates more than others, particularly in later epochs, but this does not negatively affect its classification quality.

\begin{table}[H]
\centering
\small
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{0.8}
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{|l|c|c|c|l|}
\hline
Model     & Max F1-score & Stability  & Speed    & Recommendation         \\
\hline
GRU       & $\sim$0.85 & Excellent  & Very Fast & Best lightweight option \\
LSTM      & $\sim$0.87 & Very good  & Fast      & Robust and efficient    \\
CNN-LSTM  & $\sim$0.90 & Good       & Moderate  & Best overall choice     \\
CNN-GRU   & $\sim$0.87 & Good       & Fast      & Efficient and competitive \\
\hline
\end{tabular}
}
\caption{Summary of model performance during validation phase using FastText embeddings.}
\label{tab:validation_summary_fasttext}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{we_val_fasttext.png}
    \caption{Validation curves (loss, accuracy, F1-score) for GRU, LSTM, CNN-LSTM, and CNN-GRU models trained with pre-trained FastText (300d) word embeddings.}
    \label{fig:validation_phase_fasttext}
\end{figure}

\subsubsection*{d. Optimization phase : hyperparameter selection}
\addcontentsline{toc}{subsubsection}{Optimization phase: hyperparameter selection (with pre-trained embeddings)}

This experiment investigates the optimization process for GRU, LSTM, CNN-GRU, and CNN-LSTM architectures when initialized with pre-trained word embeddings (GloVe or FastText). Hyperparameter tuning was conducted via grid search.

The choice of optimizer was critical, with Adam again emerging as the most effective due to its fast convergence and stable gradients. Learning rates in the range of $10^{-4}$ to $10^{-3}$ were tested, with $5 \times 10^{-4}$ providing a favorable trade-off between learning speed and generalization. Embedding dimensions were fixed at 300 to match the pre-trained vectors. Model depth was tested from 1 to 3 recurrent layers, with optimal results typically obtained using 1 layer. Batch sizes of 16 and 32 were preferred, with 32 giving slightly smoother learning curves.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{we_opt_Glove.png}
    \caption{Training and validation performance during hyperparameter optimization phase for GRU, LSTM, CNN-GRU, and CNN-LSTM models using pre-trained GloVe embeddings.}
    \label{fig:optimization_pretrained_embeddings}
\end{figure}

As depicted in Figure~\ref{fig:optimization_pretrained_embeddings}, all models benefit significantly from pre-trained embeddings. With GloVe, both CNN-based hybrids (CNN-GRU and CNN-LSTM) demonstrate rapid convergence and low validation loss. The CNN-LSTM model stands out, with the smoothest training dynamics and highest accuracy and F1-scores.

With FastText embeddings, performance trends are similar, though convergence appears slightly more stable, particularly for the CNN-GRU model. This may be due to FastText’s subword-level representations, which improve handling of rare or domain-specific biomedical terms. The CNN-LSTM model once again achieves the best performance across most metrics, highlighting the synergy between convolutional and recurrent mechanisms.

These findings underscore that, even with strong pre-trained embeddings, thorough hyperparameter tuning remains essential. The CNN-LSTM model consistently emerges as the most effective architecture, combining strong representational capacity with high generalization performance in biomedical text classification.

\subsubsection*{e. Test Phase: model performances (with pre-trained embeddings)}
\addcontentsline{toc}{subsubsection}{Test Phase: model performances (with pre-trained embeddings)}

All models evaluated in this phase used pre-trained word embeddings (GloVe or FastText) and were trained with an early stopping strategy to prevent overfitting. Training was halted when either (1) the validation loss exceeded the training loss by more than 25\%, or (2) the training accuracy surpassed validation accuracy by more than 5 percentage points. This ensured robust generalization and avoided excessive fitting to training data.

Figure~\ref{fig:multi_test_we_performance} shows the evolution of key evaluation metrics during test phase: test loss, F1-score, balanced accuracy, precision, and recall. Results are shown separately for GloVe (top row) and FastText (bottom row) embeddings.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{we_test.png}
    \caption{Test performance curves for LSTM, GRU, CNN-GRU, and CNN-LSTM using pre-trained embeddings: GloVe (top) and FastText (bottom). Metrics include loss, F1-score, balanced accuracy, precision, and recall.}
    \label{fig:multi_test_we_performance}
\end{figure}

\paragraph{Evaluation of Model Generalization on the Test Set with Pre-trained Embeddings:}

\begin{table}[H]
\centering
\caption{Test set metrics of all models using pre-trained embeddings (GloVe and FastText). Best results per metric are in bold.}
\label{tab:test_metrics_we}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Embedding} & \textbf{F1-score} & \textbf{Bal. Acc.} & \textbf{Precision} & \textbf{Recall} & \textbf{Test Acc.} & \textbf{Training Time (min)} & \textbf{Inference Time (s).} \\
\hline
\textbf{CNN-LSTM} & FastText & \textbf{90.50\%} & \textbf{90.49\%} & \textbf{90.73\%} & \textbf{90.58\%} & \textbf{90.50\%} & 5.12 & 0.06 \\
\textbf{CNN-GRU} & FastText & 88.91\% & 88.96\% & 89.15\% & 89.15\% & 88.91\% & 8.25 & 0.03 \\
\textbf{LSTM} & FastText & 80.49\% & 80.65\% & 81.67\% & 89.21\% & 80.49\% & $\sim$11.0 & 0.09 \\
\textbf{GRU} & FastText & \textbf{92.26\%} & \textbf{92.26\%} & \textbf{92.46\%} & \textbf{92.31\%} & \textbf{92.26\%} & \textbf{2.67} & \textbf{0.06} \\
\hline
\textbf{CNN-LSTM} & GloVe & 87.19\% & 87.60\% & 87.43\% & 89.16\% & 87.19\% & 19.28 & 0.06 \\
\textbf{CNN-GRU} & GloVe & 87.62\% & 87.72\% & 87.89\% & 88.33\% & 87.62\% & 12.74 & 0.03 \\
\textbf{LSTM} & GloVe & 89.17\% & 89.25\% & 89.51\% & 89.51\% & 89.17\% & 41.73 & 0.05 \\
\textbf{GRU} & GloVe & \textbf{92.15\%} & \textbf{92.14\%} & \textbf{92.35\%} & \textbf{92.13\%} & \textbf{92.15\%} & \textbf{2.33} & 0.05 \\
\hline
\end{tabular}
} % end of resizebox
\end{table}

\paragraph{Summary of Quantitative Results:}

Table~\ref{tab:test_metrics_we} confirms the strong generalization ability of models trained with pre-trained embeddings. With FastText, both CNN-LSTM and GRU models reach top-tier performance, exceeding 90\% across all metrics. Notably, the GRU achieves the highest F1-score and fastest training time (2.67 minutes), indicating an excellent trade-off between efficiency and effectiveness.

The CNN-LSTM model with FastText shows particularly stable and high performance, reinforcing the advantage of hybrid architectures. On the other hand, models trained with GloVe embeddings, while still performant, show slightly lower metrics and longer training times (especially LSTM).

The loss variation remains low across all settings, suggesting stable training. CNN-GRU and GRU models maintain good consistency, though FastText clearly improves robustness across all configurations.

Inference time remains low and stable across models, with GRU and CNN-LSTM being particularly efficient during prediction.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{we_perf.png}
    \caption{Performance of GRU, LSTM, CNN-GRU, and CNN-LSTM models on the test set using pre-trained embeddings: GloVe (left) and FastText (right)}
    \label{fig:we_perf}
\end{figure}

\paragraph{Latent Space Visualization with t-SNE (Pre-trained Embeddings):}

Figure~\ref{fig:tsne_we_after_training_we} visualizes the latent space of the test set after training using pre-trained embeddings. Compared to embeddings learned from scratch, the t-SNE plot shows more coherent and separated class clusters. This confirms that pre-trained embeddings facilitate faster convergence and richer semantic representations during training, thereby improving both classification performance and interpretability.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{we_after_training_after_training_tsne.png}
\caption{t-SNE projection of the document embeddings after training (pre-trained embeddings used). Clusters appear more distinct and better separated, reflecting learned semantic structure.}
\label{fig:tsne_we_after_training_we}
\end{figure}

\subsubsection{Experiment 2 : Bidirectional GRU and Bahdanau Attention with: GloVe and FastText}

\subsubsection*{a. Validation phase}
\addcontentsline{toc}{subsubsection}{Validation phase}

\paragraph{Validation phase (with GloVe embeddings):}

As previously illustrated through t-SNE projections, word embeddings offer a foundational semantic structure (figure~\ref{fig:tsne_pretrained_before}). We now analyze how these embeddings, combined with a bidirectional GRU and Bahdanau attention, perform during the validation phase.

Figure~\ref{fig:ce_val_we} displays training and validation metrics over 10 epochs. The model was trained using RMSprop with a learning rate of $1 \times 10^{-5}$ and cross-entropy loss. It contains approximately 13.6 million trainable parameters.

The validation loss converges rapidly, stabilizing around 0.29, while the F1-score reaches $\sim$0.91. The training and validation accuracy curves closely follow each other, peaking near 91\%, suggesting strong generalization and stable learning.

\paragraph{Validation phase (with FastText embeddings):}

Figure~\ref{fig:ce_val_we} shows the same BiGRU with Bahdanau attention architecture trained on FastText embeddings. Training was conducted over 12 epochs using identical optimization settings.

The validation F1-score reaches $\sim$0.85, and accuracy stabilizes around 84\%. The convergence is slower compared to GloVe, and the final validation loss remains slightly higher at 0.58. Nonetheless, the model demonstrates good learning dynamics and consistency across epochs.

\begin{table}[H]
\centering
\small
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{0.8}
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{|l|c|c|c|l|}
\hline
Model (Embedding)            & Max F1-score & Stability  & Speed     & Recommendation              \\
\hline
BiGRU + Bahdanau (GloVe)     & $\sim$0.91   & Excellent  & Fast      & Highest performance          \\
BiGRU + Bahdanau (FastText)  & $\sim$0.85   & Very Good  & Moderate  & Reliable and efficient       \\
\hline
\end{tabular}
}
\caption{Summary of validation phase performance for BiGRU + Bahdanau Attention using GloVe and FastText embeddings.}
\label{tab:validation_summary_attention}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{ce_val_we.png}
    \caption{Validation curves (loss, accuracy, F1-score) for BiGRU + Bahdanau Attention using GloVe (top row) and FastText (bottom row) embeddings.}
    \label{fig:ce_val_we}
\end{figure}

\subsubsection*{b. Optimization phase : hyperparameter selection}
\addcontentsline{toc}{subsubsection}{Optimization phase: hyperparameter selection (with pre-trained embeddings)}

This experiment investigates the optimization behavior of the BiGRU + Bahdanau attention architecture using pre-trained word embeddings (GloVe and FastText). The aim is to determine the optimal combination of hyperparameters through systematic experimentation.

As shown in Figure~\ref{fig:optimization_attention_embeddings}, the optimization curves reveal the model’s sensitivity to different learning rates and optimizers. Experiments tested Adam and RMSprop optimizers, with learning rates ranging from $1 \times 10^{-4}$ to $1 \times 10^{-5}$. The embedding dimension was fixed at 300, consistent with the pre-trained GloVe and FastText vectors. All configurations used a single BiGRU layer, with 64 hidden units and batch sizes of 16 or 32.

With FastText embeddings (top row), RMSprop with a learning rate of $1 \times 10^{-5}$ delivers the most stable training and validation dynamics. Models trained with this setup show a consistent decline in loss and steady improvement in accuracy over epochs. Although convergence is slower compared to Adam, RMSprop leads to better generalization and avoids overfitting.

In contrast, the GloVe-based models (bottom row) converge much more rapidly. All three tested configurations demonstrate swift reduction in training and validation loss, with RMSprop again yielding the smoothest and highest accuracy curves. The model achieves near-optimal performance within the first 5 epochs under most configurations, showcasing excellent compatibility between the BiGRU-attention architecture and GloVe embeddings.

\textit{Compared to the architectures in Experiments 0 and 1}, the optimization curves of BiGRU + Attention are smoother and more predictable. Although CNN-LSTM previously had the fastest convergence, the BiGRU-Attention model with RMSprop shows better long-term stability, especially with GloVe. This indicates that attention mechanisms can enhance robustness during training.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{ce_opt_we.png}
    \caption{Training and validation performance during hyperparameter optimization for BiGRU + Bahdanau Attention using FastText (top row) and GloVe (bottom row) embeddings.}
    \label{fig:optimization_attention_embeddings}
\end{figure}

These results confirm the importance of optimizer selection and learning rate tuning when working with attention-based architectures. RMSprop with a conservative learning rate emerges as the most reliable choice, particularly when combined with GloVe embeddings. The BiGRU + Bahdanau model thus offers a compelling balance of accuracy, stability, and generalization capacity for biomedical sequence classification tasks.

\subsubsection*{e. Test Phase: model performances}
\addcontentsline{toc}{subsubsection}{Test Phase: model performances}

All models evaluated in this phase used pre-trained word embeddings (GloVe or FastText), with early stopping criteria applied to ensure optimal generalization. Training was terminated when (1) the validation loss exceeded the training loss by more than 25\%, or (2) training accuracy exceeded validation accuracy by over 5 percentage points—thus minimizing overfitting risk.

Figure~\ref{fig:test_bahdanau_we} shows test-time performance curves of the BiGRU + Bahdanau Attention model using both GloVe and FastText embeddings. The metrics tracked over epochs include test loss, F1-score, balanced accuracy, precision, and recall.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{ce_test_we1.png}
    \caption{Test performance metrics for BiGRU + Bahdanau Attention model using GloVe and FastText embeddings.}
    \label{fig:test_bahdanau_we}
\end{figure}

\paragraph{Evaluation of Generalization and Robustness of the BiGRU + Bahdanau Model:}

As illustrated in Figure~\ref{fig:test_bahdanau_we}, both GloVe and FastText embeddings enable the BiGRU + Bahdanau model to reach strong and stable performance across all metrics. The loss curves converge rapidly within the first few epochs, with negligible overfitting signs. All evaluation metrics—F1-score, balanced accuracy, precision, and recall—plateau near or above 90\%, indicating high generalization capacity.

\begin{table}[H]
\centering
\caption{Test set results for BiGRU + Bahdanau using pre-trained embeddings (GloVe and FastText). Best results per metric are in bold.}
\label{tab:test_metrics_bahdanau}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Embedding Type} & \textbf{Accuracy} & \textbf{F1 Score} & \textbf{Balanced Acc} & \textbf{Precision} & \textbf{Recall} & \textbf{Training Time (min)} & \textbf{Inference Time (s)} \\
\hline
\textbf{BiGRU + Bahdanau} & GloVe (300d)   & 90.17\% & 90.17\% & 90.21\% & 90.35\% & 90.40\% & 10.56 & 0.21 \\
\textbf{BiGRU + Bahdanau} & FastText (300d) & \textbf{91.80\%} & \textbf{91.80\%} & \textbf{91.89\%} & \textbf{91.04\%} & \textbf{91.14\%} & \textbf{9.96} & \textbf{0.22} \\
\hline
\end{tabular}
}
\end{table}

\paragraph{Interpretation and Comparison:}

The FastText-based model slightly outperforms its GloVe counterpart across most test metrics, particularly in accuracy, F1-score, and balanced accuracy. This aligns with earlier observations in the optimization phase, where FastText's subword-level representations led to more stable convergence. Notably, FastText also offers a marginal reduction in training time, indicating faster convergence.

Compared to the models in Experiment 1 (e.g., CNN-LSTM with FastText), BiGRU + Bahdanau achieves comparable or superior results in accuracy and F1-score while remaining efficient in both training and inference. The attention mechanism likely contributes to improved handling of complex sequences, further supported by FastText’s robust embeddings for rare or biomedical-specific terms.

In summary, BiGRU + Bahdanau with FastText emerges as a high-performing architecture, delivering a strong balance between accuracy, recall, and computational efficiency—making it particularly well-suited for biomedical text classification tasks.

Figure~\ref{fig:ce_perf_bigru_glove_fasttext} presents a side-by-side evaluation of the best-performing BiGRU + Bahdanau Attention models trained using GloVe and FastText embeddings. Each model was saved based on its peak validation performance and then evaluated on the unseen test set across four critical metrics: F1-score, Balanced Accuracy, Recall, and Precision.

Both models exhibit near-identical performance, with all scores clustered around the 0.90 mark. Notably:
\begin{itemize}
    \item The \textbf{FastText-based model} slightly outperforms the GloVe-based one in overall recall (91.14\% vs. 90.40\%), indicating better sensitivity to true positives.
    \item The \textbf{FastText model} also maintains an edge in F1-score, balanced accuracy, and training efficiency.
\end{itemize}

These results reinforce the earlier quantitative findings in Table~\ref{tab:test_metrics_bahdanau}, where both models demonstrated high generalization capacity. The consistent performance across all metrics also validates the use of early stopping and the selected optimization strategies during training.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{ce_perf_bigru_glove_fasttext.png}
    \caption{Comparison of test set performance metrics for BiGRU + Bahdanau Attention models using GloVe and FastText embeddings.}
    \label{fig:ce_perf_bigru_glove_fasttext}
\end{figure}

\subsubsection{Experiment 3 : Bidirectional GRU models with Bahdanau attention with  contextual embeddings : PubMedBert and BioBert }

\subsubsection*{a. t-SNE visualization before training (contextual embeddings)}  
\addcontentsline{toc}{subsubsection}{t-SNE visualization before training (contextual embeddings)}

Figure~\ref{fig:tsne_contextual_before} presents the t-SNE visualization of the contextual embeddings generated by \textbf{PubMedBERT} (left) and \textbf{BioBERT} (right) prior to any training with the BiGRU-Attention model.

At this stage, the embeddings reflect solely the pre-trained representations without any fine-tuning or task-specific adaptation. The visualization aims to qualitatively evaluate the initial class separability and semantic clustering achieved by each language model.

\textbf{PubMedBERT (left):}  
The embeddings produced by PubMedBERT exhibit relatively compact and well-defined clusters, especially for certain classes such as Class 6 and Class 7. This suggests a richer and more discriminative semantic representation within the biomedical domain. This observation aligns with PubMedBERT’s training strategy, which involves pretraining from scratch on PubMed abstracts and full texts, allowing it to capture more specialized biomedical knowledge compared to BioBERT.

\textbf{BioBERT (right):}  
In contrast, BioBERT’s embeddings show a more dispersed distribution with less distinct clustering. Although some local grouping by class is apparent, considerable overlap across many classes indicates limited initial class discriminability. This pattern is consistent with BioBERT’s initialization from the general BERT-base model followed by adaptation to biomedical texts, which may result in less pronounced task-specific structures at this stage.

Overall, PubMedBERT provides more structured and class-informative embeddings at initialization, which may facilitate more efficient model convergence and improved performance in downstream classification tasks after fine-tuning.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{tsne.png}
    \caption{t-SNE visualization of contextual embeddings before training: PubMedBERT (left) and BioBERT (right).}
    \label{fig:tsne_contextual_before}
\end{figure}

\subsubsection*{b. Validation Phase: BiGRU-Attention with PubMedBert and BioBert}
\addcontentsline{toc}{subsubsection}{Validation Phase: BiGRU-Attention with PubMedBert and BioBert}

Figure~\ref{fig:ce_val_phase} presents the validation phase performance of the BiGRU-Attention architecture using two different contextual embedding sources: PubMedBERT (top row) and BioBERT (bottom row). The metrics tracked include training and validation loss, accuracy, and F1-score over 10 epochs.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{ce_val.png} % Replace with your actual filename
    \caption{Validation phase performance of the BiGRU-Attention model using PubMedBERT (top row) and BioBERT (bottom row) contextual embeddings. Metrics shown include training and validation loss, accuracy, and F1-score over 10 epochs.}
    \label{fig:ce_val_phase}
\end{figure}

\paragraph{PubMedBERT + BiGRU-Attention (Top Row):}

Both training and validation loss drop rapidly within the first few epochs, converging below 0.2. The close alignment of training and validation loss indicates stable generalization with minimal overfitting. The model achieves over 90\% validation accuracy by epoch 4, sustaining this performance thereafter. The validation F1-score rises steadily and stabilizes near 0.97 after epoch 5, reflecting strong and consistent classification across classes.

This behavior suggests that PubMedBERT embeddings facilitate rapid convergence and high generalization, likely due to their domain-specific, fine-grained semantic representations.

\paragraph{BioBERT + BiGRU-Attention (Bottom Row):}

Validation loss plateaus around 0.3 after an initial sharp decline, while training loss continues to decrease, hinting at mild overfitting. Validation accuracy fluctuates between 85\% and 90\% without clear convergence. The F1-score remains relatively high (around 0.91–0.92) but shows instability across epochs, indicating some inconsistency in classification performance.

These results imply that although BioBERT embeddings support good classification, they exhibit less stability and may benefit from additional regularization or adaptive learning strategies.

\vspace{0.3cm}
\noindent 
Hence, the PubMedBERT + BiGRU-Attention model outperforms the BioBERT counterpart during validation, delivering higher and more stable metrics. This highlights the advantage of pretraining from scratch on biomedical corpora (PubMedBERT) compared to adapting from a general-domain model (BioBERT) for downstream biomedical tasks.

\subsubsection*{b. Optimization Phase: BiGRU-Attention with PubMedBERT and BioBERT}
\addcontentsline{toc}{subsubsection}{Optimization Phase: BiGRU-Attention with PubMedBERT and BioBERT}

\paragraph{Overview}  
Since visualizing learning curves through images is not possible within this document, we present the tuning results using tables for clarity and precision. Although detailed training curves and plots are available in the WandB reports, the tables here concisely summarize the performance of the BiGRU-Attention model combined with PubMedBERT and BioBERT embeddings under varying hyperparameters, including model depth, learning rate, and optimizer choice.

\paragraph{Hyperparameter Tuning with PubMedBERT}  
To optimize the BiGRU-Attention model with PubMedBERT embeddings, hyperparameters such as model depth and learning rate were systematically tuned. The model was trained with depths of 1, 2, and 3 layers, using the Adam optimizer, a learning rate of \(1 \times 10^{-4}\), weight decay of \(1 \times 10^{-3}\), and CrossEntropy loss over 5 epochs. At \textit{depth 1} with 1.71 million parameters, the model achieved a final validation accuracy of 91.86\% and an F1-score of 91.81\%. Increasing the depth to \textit{2} (2.89 million parameters) improved performance, with validation accuracy reaching 92.03\% and an F1-score of 92.02\%. Further increasing to \textit{depth 3} (4.07 million parameters) resulted in a slight decline to 91.70\% accuracy and 91.62\% F1-score, indicating diminishing returns with increased model complexity.

For learning rate tuning, the depth was fixed at 2, and learning rates of \(9 \times 10^{-5}\), \(7 \times 10^{-5}\), and \(5 \times 10^{-5}\) were tested. The best results were obtained with a learning rate of \(9 \times 10^{-5}\), yielding a validation accuracy of 92.14\% and an F1-score of 92.09\%. Lower learning rates of \(7 \times 10^{-5}\) and \(5 \times 10^{-5}\) achieved slightly reduced performance, with validation accuracies of 91.12\% and 91.57\% respectively, and corresponding F1-scores of 91.14\% and 91.53\%. These findings highlight that a depth of 2 with a learning rate of \(9 \times 10^{-5}\) balances model complexity and generalization optimally.

\paragraph{Hyperparameter Tuning with BioBERT}  
When extending the tuning to BioBERT embeddings, similar considerations were applied. Models with depths of 1, 2, and 3 layers were trained over 4 epochs using a learning rate of \(1 \times 10^{-4}\) and weight decay of \(1 \times 10^{-3}\). Despite a substantial increase in trainable parameters—approximately 110 million across depths—the validation accuracy at \textit{depth 1} was 90.98\% with an F1-score of 90.95\%. Increasing to \textit{depth 2} improved accuracy to 91.58\% and F1 to 91.58\%, while \textit{depth 3} further increased accuracy to 91.91\% and F1-score to 91.89\%, demonstrating that deeper architectures continue to benefit performance in BioBERT’s larger model.

Focusing on learning rate selection with the model fixed at depth 2, learning rates of \(1 \times 10^{-4}\), \(7 \times 10^{-5}\), and \(5 \times 10^{-4}\) were evaluated. The optimal learning rate was \(1 \times 10^{-4}\), yielding the highest validation accuracy of 92.05\% and F1-score of 92.04\%. The learning rate \(7 \times 10^{-5}\) showed a slight drop in performance, with validation accuracy around 91.72\% and F1-score of 91.76\%, while the higher learning rate of \(5 \times 10^{-4}\) resulted in weaker performance with accuracy near 91.84\% and F1-score of 91.81\%. Additionally, among the optimizers tested—Adam, AdamW, and RMSprop—Adam consistently delivered superior results for BioBERT-based models, confirming its suitability for fine-tuning large transformer architectures.

\paragraph{Training Time Analysis}  
Throughout all experiments, training time per epoch remained consistently around 14 minutes, despite significant differences in parameter counts and depth across configurations. This consistency suggests well-optimized hardware utilization and batch size settings, enabling fair performance comparisons without interference from computational overhead.

\paragraph{Presentation of Results}  
The tables below summarize the validation accuracy and F1-scores corresponding to the hyperparameter tuning experiments described above.

\begin{table}[H]
\centering
\caption{Validation accuracy and F1-score for PubMedBERT BiGRU-Attention with varying depths (learning rate \(1 \times 10^{-4}\)) over 5 epochs}
\begin{tabular}{ccc}
\hline
\textit{Depth} & \textit{Validation Accuracy (\%)} & \textit{F1-score (\%)} \\
\hline
1 (1.71M params) & 91.86 & 91.81 \\
2 (2.89M params) & \textit{92.03} & \textit{92.02} \\
3 (4.07M params) & 91.70 & 91.62 \\
\hline
\end{tabular}
\end{table}

\vspace{0.5cm}

\begin{table}[H]
\centering
\caption{Validation accuracy and F1-score for PubMedBERT BiGRU-Attention with fixed depth = 2 and varying learning rates over 5 epochs}
\begin{tabular}{ccc}
\hline
\textit{Learning Rate} & \textit{Validation Accuracy (\%)} & \textit{F1-score (\%)} \\
\hline
\(9 \times 10^{-5}\) & \textit{92.14} & \textit{92.09} \\
\(7 \times 10^{-5}\) & 91.12 & 91.14 \\
\(5 \times 10^{-5}\) & 91.57 & 91.53 \\
\hline
\end{tabular}
\end{table}

\vspace{1cm}

\begin{table}[H]
\centering
\caption{Validation accuracy and F1-score for BioBERT BiGRU-Attention with varying depths (learning rate \(1 \times 10^{-4}\)) over 4 epochs}
\begin{tabular}{ccc}
\hline
\textit{Depth} & \textit{Validation Accuracy (\%)} & \textit{F1-score (\%)} \\
\hline
1 (110M params) & 90.98 & 90.95 \\
2 (110M params) & 91.58 & 91.58 \\
3 (110M params) & \textit{91.91} & \textit{91.89} \\
\hline
\end{tabular}
\end{table}

\vspace{0.5cm}

\begin{table}[H]
\centering
\caption{Validation accuracy and F1-score for BioBERT BiGRU-Attention with fixed depth = 2 and varying learning rates over 4 epochs}
\begin{tabular}{ccc}
\hline
\textit{Learning Rate} & \textit{Validation Accuracy (\%)} & \textit{F1-score (\%)} \\
\hline
\(1 \times 10^{-4}\) & \textit{92.05} & \textit{92.04} \\
\(7 \times 10^{-5}\) & 91.72 & 91.76 \\
\(5 \times 10^{-4}\) & 91.84 & 91.81 \\
\hline
\end{tabular}
\end{table}

\vspace{0.5cm}

\begin{table}[H]
\centering
\caption{Validation accuracy and F1-score for BioBERT BiGRU-Attention with different optimizers over 4 epochs (depth = 2, learning rate \(1 \times 10^{-4}\))}
\begin{tabular}{ccc}
\hline
\textit{Optimizer} & \textit{Validation Accuracy (\%)} & \textit{F1-score (\%)} \\
\hline
Adam & \textit{91.77} & \textit{92.00} \\
AdamW & 91.25 & 91.33 \\
RMSprop & 90.98 & 90.90 \\
\hline
\end{tabular}
\end{table}

Hence, the optimization phase indicates that for PubMedBERT embeddings, a BiGRU depth of 2 combined with a learning rate of \(9 \times 10^{-5}\) achieves the best generalization. For BioBERT, deeper models with up to 3 layers and a learning rate of \(1 \times 10^{-4}\) yield superior results, with Adam optimizer proving most effective. These insights underscore the critical role of tailored hyperparameter tuning aligned with the scale and architecture of the embedding models.

\subsubsection*{e. Test Phase: model performances}
\addcontentsline{toc}{subsubsection}{Test Phase: model performances}

All models evaluated in this phase used contextual embeddings from PubMedBERT or BioBERT, combined with a Bidirectional GRU architecture enhanced by Bahdanau attention. Early stopping was applied to prevent overfitting, with criteria analogous to previous experiments.

Figure~\ref{fig:test_contextual} shows the test-time performance curves for the Bidirectional GRU + Bahdanau model using both PubMedBERT and BioBERT embeddings. Metrics tracked over epochs include test loss, F1-score, balanced accuracy, precision, and recall.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{ce_test_loss_balanced_bio_pub.png}
    \caption{Loss and balanced accuracy curves of bidirectional GRU + Bahdanau attention + PubMedBert/BioBert during training and evaluation .}
    \label{fig:test_contextual}
\end{figure}

\paragraph{Evaluation of Generalization and Robustness of the BiGRU + Bahdanau Model:}

As shown in Figure~\ref{fig:test_contextual}, both PubMedBERT and BioBERT embeddings enable the Bidirectional GRU + Bahdanau model to achieve strong and stable performance across all metrics. Loss curves converge steadily within 10 epochs with minimal overfitting signs. All evaluation metrics—F1-score, balanced accuracy, precision, and recall—hover around or above 91\%, indicating robust generalization.

\begin{table}[H]
\centering
\caption{Test set results for Bidirectional GRU + Bahdanau attention using contextual embeddings (PubMedBERT and BioBERT). Best results per metric are in bold.}
\label{tab:test_metrics_contextual}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Test Loss} & \textbf{Accuracy (\%)} & \textbf{F1 Score (\%)} & \textbf{Balanced Acc. (\%)} & \textbf{Recall (\%)} & \textbf{Training Time (min)} & \textbf{Inference Time (s)} \\
\hline
PubMedBERT + BiGRU + Bahdanau & 0.271 & 91.34 & 91.31 & 91.52 & 91.34 & 157 & \textbf{145} \\
BioBERT + BiGRU + Bahdanau & 0.258 & \textbf{91.81} & \textbf{91.84} & \textbf{92.00} & \textbf{91.81} & \textbf{160} & 204 \\
\hline
\end{tabular}
}
\end{table}

\paragraph{Interpretation and Comparison:}

The BioBERT-based model slightly outperforms its PubMedBERT counterpart across all key metrics, including accuracy, F1-score, and balanced accuracy. This advantage likely stems from BioBERT’s more extensive domain-specific pretraining.

Training times are comparable, though BioBERT requires a slightly longer inference time. Both models demonstrate excellent stability and rapid convergence.

Compared with previous experiments using static embeddings, these contextual embedding models show improved classification performance, benefiting from richer semantic representations and the attention mechanism’s ability to focus on relevant sequence parts.

In summary, Bidirectional GRU + Bahdanau models with PubMedBERT and BioBERT embeddings provide state-of-the-art performance, with BioBERT offering a marginal edge for biomedical text classification tasks.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{ce_perf_biobert_pubmed.png}
    \caption{Comparison of test set performance metrics for Bidirectional GRU + Bahdanau Attention models using PubMedBERT and BioBERT embeddings.}
    \label{fig:ce_perf_biobert_pubmed_compare}
\end{figure}

\paragraph{Latent Space Visualization:}

To better understand how the models differentiate between classes, Figure~\ref{fig:tsne_contextual} presents the t-SNE projections of the learned document embeddings after training. Both PubMedBERT and BioBERT-based models produce well-separated clusters, indicating that the latent representations capture meaningful semantic distinctions relevant to the classification task.

Notably, the BioBERT embeddings exhibit slightly tighter and more distinct clusters, reflecting its superior domain adaptation and richer feature encoding. This enhanced separability in the latent space likely contributes to the improved classification performance observed.

These visualizations provide qualitative confirmation that the combination of contextual embeddings with attention mechanisms effectively encodes discriminative features, facilitating robust downstream classification.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{ce_tsne_after_training_biobert_pubmed.png}
    \caption{t-SNE visualization of the latent space after training Bidirectional GRU + Bahdanau attention using PubMedBERT and BioBERT embeddings. Each color represents a different class.}
    \label{fig:tsne_contextual}
\end{figure}


\subsubsection*{f. Results Analysis: Confusion Matrices and AUC Curves}
\addcontentsline{toc}{subsubsection}{Results Analysis: Confusion Matrices and AUC Curves}

The performance of each model was assessed using two complementary evaluation tools: confusion matrices and ROC-AUC curves. These visualizations provide deep insights into model behavior across all classes and go beyond aggregate metrics such as accuracy or F1-score.

\subsubsection*{Confusion Matrices}
\label{sec:confusion_matrices}

Figure~\ref{fig:confusion_matrices} presents the confusion matrices for all evaluated model configurations across the multi-class classification task. These visualizations provide insights into each model’s discriminative capacity and highlight systematic patterns of misclassification, especially between clinically or semantically related categories.

For the baseline models trained from scratch—namely GRU, LSTM, CNN-GRU, and CNN-LSTM—without any pretrained embeddings, the matrices reveal a high degree of confusion among several classes. The diagonals, which represent correct predictions, appear less prominent, indicating limited class separability and generalization ability. This underperformance can be attributed to the models’ reliance on shallow representations that lack semantic understanding. Among these, CNN-LSTM demonstrates a modest improvement, likely due to its capacity to capture both local and sequential dependencies via convolutional and recurrent layers.

The introduction of GloVe embeddings leads to a visible enhancement in classification accuracy. The diagonals in the corresponding confusion matrices become more pronounced, signaling better alignment between predicted and actual classes. Particularly, the GRU-GloVe and LSTM-GloVe configurations show strong gains, although misclassifications persist between classes such as 3, 4, and 6—categories that often share terminological overlaps and medical co-occurrences in clinical narratives. These residual errors suggest that word-level semantics, while helpful, may not always suffice for fine-grained biomedical disambiguation.

FastText embeddings yield even more robust results. Their ability to capture subword information enables improved handling of morphological variations, rare biomedical terms, and abbreviations, which are frequent in domain-specific text. Models such as CNN-GRU and CNN-LSTM, when combined with FastText, generate significantly cleaner confusion matrices with sharper diagonals and reduced noise, reflecting improved robustness to lexical variation and rare token sparsity.

The addition of Bahdanau attention amplifies these benefits across architectures. By allowing the model to assign dynamic importance weights to input tokens, the attention mechanism helps prioritize clinically informative terms and ignore less relevant context. This results in fewer off-diagonal errors and greater class focus, particularly when attention is used in conjunction with GloVe or FastText embeddings.

The highest-performing configurations involve contextual embeddings—PubMedBERT and BioBERT—paired with bidirectional GRU layers and Bahdanau attention. These models achieve near-perfect confusion matrices, characterized by sharp, uninterrupted diagonals and minimal inter-class confusion. Such performance reflects the models’ ability to leverage rich contextual knowledge specific to biomedical language, capturing both domain semantics and clinical subtleties that simpler embedding schemes miss. These results underscore the importance of contextualized representations in tackling complex multi-class classification tasks in the biomedical NLP field.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{merged_confusion_matrices_multi_class.png}
    \caption{Confusion matrices for all model configurations on the multi-class classification task. Top left: models with GloVe embeddings; top right: models trained from scratch (no embeddings); bottom left: models with FastText embeddings; bottom right: models with contextual embeddings (PubMedBERT and BioBERT) combined with Bahdanau attention.}
    \label{fig:confusion_matrices}
\end{figure}

\subsubsection*{AUC Curves}
\label{sec:auc_curves}

The ROC-AUC curves for all evaluated models are shown in Figure~\ref{fig:auc_curves}. These curves illustrate each model’s ability to discriminate between classes independent of the decision threshold, with the Area Under the Curve (AUC) offering a threshold-free measure of performance. A higher AUC score indicates stronger confidence in positive vs. negative class distinction.

Models trained from scratch, using no pretrained embeddings, achieve average AUC values ranging from approximately 0.86 to 0.89. Among them, LSTM and CNN-GRU exhibit slightly stronger performance, but some variability is observed across classes. Notably, lower AUCs are recorded for classes 1 and 4, suggesting challenges in separating these categories using purely learned token-level features. This indicates limited expressiveness and generalization of shallow representations when dealing with biomedical text complexity.

With the introduction of GloVe and FastText word embeddings, a clear leap in performance is observed. AUC values consistently exceed 0.90 across most classes, with several approaching or surpassing 0.99. The LSTM-CNN and CNN-GRU architectures paired with these embeddings are particularly effective, benefiting from both the semantic richness of pretrained word vectors and architectural depth. These results emphasize how pretrained word-level semantics help models generalize across complex and nuanced clinical language.

Attention mechanisms further amplify this improvement. By allowing the models to dynamically prioritize informative terms, attention leads to a noticeable reduction in false positives and false negatives, thereby improving class-level AUC. For most attention-based configurations, AUC scores exceed 0.95, with some classes achieving perfect separation (AUC = 1.0). This demonstrates how attention contributes to interpretability and sharpens class boundaries even in overlapping biomedical narratives.

The most significant performance gains are observed when contextual embeddings—namely PubMedBERT and BioBERT—are introduced in conjunction with BiGRU and Bahdanau attention. These models achieve near-perfect ROC curves across all classes, with AUC scores closely approaching or reaching 1.0. Such results highlight their exceptional capacity to encode complex, domain-specific context at the sentence and token levels, making them ideally suited for high-resolution biomedical classification tasks.

These findings confirm the cumulative value of integrating pretrained knowledge and attention mechanisms, and demonstrate the superiority of contextual models in modeling semantic and clinical subtleties across multi-class biomedical datasets.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{auc_merged.png}
    \caption{ROC-AUC curves of all models across different embeddings.From top to bottom: models without embeddings, with word embeddings, and models with contextual embeddings (PubMedBERT and BioBERT) combined with Bahdanau attention.}
    \label{fig:auc_curves}
\end{figure}

\subsubsection*{g. Error Analysis: Understanding Model Misclassifications}
\addcontentsline{toc}{subsubsection}{Results Analysis: Confusion Matrices and AUC Curves}

In order to deepen the understanding of the limitations of our classification models, we conducted a qualitative error analysis by examining the instances that were misclassified across different architectures and embedding schemes, as illustrated in Figure~\ref{fig:misclassifications}. The figure displays the number of incorrect predictions for each disease class, comparing models trained from scratch, with pretrained word embeddings, and with contextual embeddings using Bahdanau attention.

One particularly revealing example involves a misclassified instance originally labeled as Cystic Fibrosis (class 8). The input text contains several highly indicative terms such as \textit{“Pulmonary hemorrhage,” “hemoptysis,” “bronchiectasis”}, and even explicitly mentions \textit{“mucoviscidose”} (the French term for cystic fibrosis). These terms are semantically and clinically linked to class 8 and serve as strong lexical cues. However, the model erroneously classified the text as Tuberculosis (class 0). This suggests that the model, specifically the BiGRU with Bahdanau attention, may have failed to properly weigh the most relevant terms, possibly being misled by the contextual mention of “tuberculosis” as a comparative condition rather than the central topic.

This misclassification highlights several potential underlying issues. First, it appears that the attention mechanism did not fully capture or prioritize the discriminative terminology despite its presence. Secondly, the co-occurrence of multiple diseases in a single document may introduce noise that overwhelms the model’s decision-making process. Finally, such errors may stem from insufficient training examples that reflect the multi-disease contexts commonly encountered in biomedical corpora.

This case reveals a critical weakness in models relying solely on standard attention mechanisms when applied to specialized domains like biomedical literature. Enhancing class-specific representation during training, leveraging domain-specific ontologies, or incorporating hierarchical and contextual attention mechanisms could improve the ability to correctly interpret complex documents with overlapping terminologies and improve classification robustness in high-stakes clinical NLP applications.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{merged_missclass.png}
    \caption{Distribution of misclassified instances per class across different models and embedding strategies.From top to bottom: models without embeddings, with FastText, with GloVe, and with contextual embeddings (PubMedBERT and BioBERT).}
    \label{fig:misclassifications}
\end{figure}

\subsubsection*{Conclusion}

Taken together, the confusion matrices and ROC-AUC curves reveal a consistent and compelling pattern: integrating pretrained embeddings and attention mechanisms leads to substantial improvements in classification performance. In particular, models leveraging contextual embeddings such as BioBERT, especially when combined with Bahdanau attention, demonstrate superior discriminative capabilities in handling complex biomedical narratives. These results underscore the importance of embedding richness and attention-guided feature focusing in capturing subtle linguistic cues relevant to disease classification. However, despite these improvements, some classes remain disproportionately misclassified, suggesting that architectural advances alone may not fully overcome the challenges posed by data imbalance. This motivates a deeper investigation into class distribution effects and the application of resampling and loss-weighting techniques to mitigate such biases.

\subsection{Impact of Class Imbalance: Resampling and Loss Weighting Strategies}
\addcontentsline{toc}{subsubsection}{Impact of Class Imbalance: Resampling and Loss Weighting Strategies}

In biomedical text classification, class imbalance can significantly affect prediction quality, particularly by reducing the model's ability to generalize well to minority classes. This issue is even more critical given that the most performant models often rely on heavy architectures, such as contextual encoders like BERT, whose computational cost limits their applicability in resource-constrained environments.

Before reporting the results, it is important to emphasize that applying techniques such as resampling or class weighting requires redoing the entire model selection pipeline. Specifically, each strategy was accompanied by a full re-execution of the validation and hyperparameter optimization processes described in previous sections. Only after completing these steps were the final models evaluated on the test set.

In this context, we explore the effectiveness of standard imbalance-handling strategies: data resampling (using SMOTE and Borderline-SMOTE) and class weighting in the loss function (weighted training). The objective is twofold: to assess how well these approaches improve performance across all classes, and to determine whether they offer a viable and less resource-intensive alternative to using large-scale pre-trained models.

To this end, we evaluated the performance of the \textit{BiGRU} model with Bahdanau attention, combined with various embeddings (GloVe, FastText, PubMedBERT, and BioBERT), under four training strategies: no rebalancing, SMOTE, Borderline-SMOTE, and loss weighting. Table~\ref{tab:performance_multiclass} summarizes the results obtained in terms of accuracy, balanced accuracy, F1-score, precision, and recall for each tested configuration.

\begin{table}[H]
    \centering
    \scriptsize
    \caption{Performance comparison of models using SMOTE, Borderline-SMOTE, and Weighted Training (Multi-Class Classification)}
    \label{tab:performance_multiclass}
    \begin{tabular}{|l|l|l|c|c|c|c|c|}
    \hline
    \textbf{Training Strategy} & \textbf{Model} & \textbf{Embedding} & \textbf{Accuracy} & \textbf{Balanced Acc} & \textbf{F1-score} & \textbf{Precision} & \textbf{Recall} \\
    \hline
    No resampling & BiGRU+Bahdanau & GloVe (300d)     & 92.17\% & 92.35\% & 92.21\% & 92.40\% & 92.17\% \\
    No resampling & BiGRU+Bahdanau & FastText (300d)  & \textbf{92.80\%} & 93.04\% & \textbf{92.89\%} & \textbf{93.14\%} & \textbf{92.80\%} \\
    No resampling & BiGRU+Bahdanau & PubMedBERT       & 91.89\% & 92.16\% & 91.90\% & 91.96\% & 91.89\% \\
    No resampling & CNN+LSTM       & -                & 88.48\% & 88.70\% & 88.61\% & -       & 88.48\% \\
    No resampling & GRU            & GloVe (300d)     & 91.67\% & 91.99\% & 91.71\% & -       & 91.67\% \\
    \hline
    SMOTE         & BiGRU+Bahdanau & GloVe (300d)     & 90.60\% & 90.88\% & 90.58\% & 90.58\% & 90.60\% \\
    SMOTE         & BiGRU+Bahdanau & FastText (300d)  & 91.45\% & 91.71\% & 91.49\% & 91.59\% & 91.45\% \\
    SMOTE         & BiGRU+Bahdanau & PubMedBERT       & 92.38\% & 92.56\% & 92.36\% & 92.48\% & 92.38\% \\
    SMOTE         & BiGRU+Bahdanau & BioBERT          & 92.00\% & 92.00\% & 92.00\% & \textbf{93.00\%} & 92.00\% \\
    SMOTE         & CNN+LSTM       & -                & 89.93\% & 90.26\% & 89.91\% & -       & 89.93\% \\
    SMOTE         & GRU            & GloVe (300d)     & 91.58\% & 91.90\% & 91.59\% & -       & 91.58\% \\
    \hline
    Borderline-SMOTE & BiGRU+Bahdanau & GloVe (300d)   & 90.84\% & 91.09\% & 90.84\% & 90.86\% & 90.84\% \\
    Borderline-SMOTE & BiGRU+Bahdanau & FastText (300d)& 91.89\% & 92.16\% & 91.90\% & 91.96\% & 91.89\% \\
    Borderline-SMOTE & BiGRU+Bahdanau & PubMedBERT     & 92.07\% & 92.18\% & 92.02\% & 92.19\% & 92.07\% \\
    Borderline-SMOTE & BiGRU+Bahdanau & BioBERT        & 92.00\% & 92.00\% & 92.00\% & \textbf{93.00\%} & 92.00\% \\
    Borderline-SMOTE & CNN+LSTM       & -              & 89.46\% & 89.76\% & 89.58\% & -       & 89.46\% \\
    Borderline-SMOTE & GRU            & GloVe (300d)   & 91.32\% & 91.55\% & 91.42\% & -       & 91.32\% \\
    \hline
    Weighted Training & BiGRU+Bahdanau & GloVe (300d)  & 90.29\% & 90.54\% & 90.35\% & 90.51\% & 90.29\% \\
    Weighted Training & BiGRU+Bahdanau & FastText (300d)& 92.17\% & 92.43\% & 92.19\% & 92.32\% & 92.17\% \\
    Weighted Training & BiGRU+Bahdanau & PubMedBERT    & 92.33\% & \textbf{92.57\%} & 92.31\% & 92.32\% & 92.33\% \\
    Weighted Training & BiGRU+Bahdanau & BioBERT       & 92.00\% & \textbf{93.00\%} & 92.00\% & \textbf{93.00\%} & 92.00\% \\
    Weighted Training & CNN+LSTM       & -             & 77.70\% & 78.27\% & 74.07\% & -       & 77.70\% \\
    Weighted Training & GRU            & GloVe (300d)  & 92.22\% & 92.36\% & 92.20\% & -       & 92.22\% \\
    \hline
    \end{tabular}
    \vspace{0.5em}
    \caption*{\textit{Note: Batch size = 16 was used for all experiments. Some precision values are omitted when not provided in logs.}}
\end{table}


The comparative analysis reveals several key trends. First, the results obtained without any rebalancing method remain highly competitive, particularly with static embeddings. The \textit{BiGRU + FastText} model stands out with an F1-score of 92.89\% and a precision of 93.14\%, making it the best baseline configuration without balancing strategies.

The application of SMOTE yields mixed effects. While it slightly improves the performance of models using contextual embeddings—e.g., PubMedBERT sees its F1-score rise from 91.90\% to 92.36\%—it tends to degrade the results with static embeddings. For instance, the \textit{BiGRU + GloVe} model experiences a drop in F1-score from 92.21\% to 90.58\%. This suggests that SMOTE, which relies on linear interpolation in feature space, is less suited for densely correlated representations like those produced by GloVe or FastText.

This hypothesis is supported by the t-SNE visualizations presented earlier. They show that static embeddings, particularly with GloVe, produce compact clusters with somewhat blurred class boundaries, making synthetic example injection potentially disruptive. In contrast, contextual embeddings such as those from PubMedBERT yield a more diffuse structuring of the semantic space, with clearer class separations. In such cases, SMOTE and Borderline-SMOTE can generate more coherent and useful synthetic samples, especially when focused on decision boundaries.

Borderline-SMOTE, which restricts synthetic generation to regions near class boundaries, proves slightly more effective than standard SMOTE, especially in conjunction with FastText or PubMedBERT. However, the gains remain marginal and do not surpass the best results obtained without resampling.

On the other hand, class weighting in the loss function (weighted training) stands out for its robustness and consistency. This approach not only maintains the performance of lighter models but also optimizes that of contextual encoder-based models. The \textit{BiGRU + PubMedBERT} configuration, for instance, achieves a balanced accuracy of 92.57\% (the highest for this model) and an F1-score of 92.31\%. Likewise, the \textit{BiGRU + BioBERT} configuration reaches an impressive precision of 93.00\% while maintaining overall stable performance.

These results indicate that class weighting is a simple, stable, and effective solution for improving minority class detection without resorting to artificial data generation. It involves no significant additional training time or architectural complexity, making it particularly suitable for constrained environments. Figure~\ref{fig:resampling_impact} illustrates this trend for some models, showing how different resampling strategies affect the classification performance across various model-embedding combinations.


In conclusion, we have some keys observations:

\textbf{No Resampling Remains Competitive, Especially with Static Embeddings:} \\
Among the unbalanced models, the \textit{BiGRU + FastText} configuration achieved the highest F1-score (92.89\%) and precision (93.14\%), outperforming some balanced variants. Similarly, the \textit{GRU + GloVe} and \textit{CNN+LSTM} baselines performed competitively, with F1-scores of 91.71\% and 88.61\%, respectively.

\vspace{0.5em}
\textbf{SMOTE Offers Limited or Negative Gains with Static Embeddings:} \\
Applying SMOTE to models with static embeddings often led to diminished performance. For example, F1-scores for \textit{BiGRU + GloVe} dropped from 92.21\% to 90.58\%. The linear interpolation of synthetic samples may interfere with tight embedding clusters, leading to less useful training signals.

\vspace{0.5em}
\textbf{Contextual Embeddings Benefit More from SMOTE and Borderline-SMOTE:} \\
PubMedBERT and BioBERT showed marginal improvements with SMOTE (e.g., PubMedBERT F1 increased to 92.36\%). Borderline-SMOTE further improved consistency near decision boundaries, particularly for \textit{BiGRU + FastText} and \textit{GRU + GloVe}, although gains remained incremental and dataset-dependent.

\vspace{0.5em}
\textbf{Loss Weighting (Class-Weighted Training) Is Most Effective Overall:} \\
Across all architectures, class weighting consistently improved or maintained performance. Notably:
\begin{itemize}
    \item \textit{BiGRU + PubMedBERT} achieved a balanced accuracy of 92.57\%, the highest among all configurations.
    \item \textit{GRU + GloVe} and \textit{CNN+LSTM} models also saw notable improvements, with F1-scores of 92.20\% and 74.07\%, respectively.
    \item \textit{BioBERT} combined with class weighting reached a precision of 93.00\%, the highest of any configuration.
\end{itemize}

These findings are further supported by t-SNE visualizations, which indicate that contextual embeddings form more clearly delineated semantic clusters. As a result, class boundaries are better represented, enabling weighted losses or targeted sampling (Borderline-SMOTE) to have a more meaningful impact.

\vspace{1em}

While resampling techniques like SMOTE and Borderline-SMOTE can occasionally improve performance---especially for contextual embeddings---their effectiveness is inconsistent and often counterproductive for static embeddings. In contrast, class weighting offers a reliable and computationally efficient solution for handling imbalance. It enhances model robustness across all architectures and embeddings without requiring synthetic data generation or increased training time.

All aforementioned findings are derived from experiments conducted in this project and reflect the specific dynamics observed in biomedical text classification under class imbalance.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{imbalanced_rs1.png}
    \caption{Effect of Resampling Techniques and Weighting Class on CNN-LSTM and BiGRU models}
    \label{fig:resampling_impact}
\end{figure}

\subsubsection{Experiment 5 : Few-Shot Learning}
\addcontentsline{toc}{subsubsection}{Experiment 5 : Few-Shot Learning}

% Goal reminder: assessing model generalization in limited data regimes for multi-class tasks
% Introduction to the experiment and rationale
The primary goal of this experiment is to assess how well different models can generalize from very limited labeled data per class — a common challenge in real-world multi-class classification problems, especially in domains like biomedicine where annotated data are scarce. 
Just as was done in the case of \textit{binary} classification, we here explore the performance of models in a \textit{few-shot learning} context applied to \textit{multi-class} classification.

% Description of the process: fixed hyperparameters, multiple k values, dataset usage
To conduct this evaluation, we use the optimal hyperparameters determined during previous validation and optimization phases and keep them fixed across all \textit{k-shot} experiments. 
This approach ensures consistent and fair comparison between models as the number of labeled examples per class, \( k \), varies.
We systematically vary \( k \) over a wide range of values—from extremely low-resource settings (e.g., \( k=1 \)) to moderately sized subsets (up to \( k=600 \))—using a large dataset of over 40,000 observations. 
This allows us to analyze how models’ abilities to generalize evolve as more labeled data become available.

% Methodological note emphasizing comparability and consistency
By maintaining fixed hyperparameters and experimental conditions, we guarantee methodological consistency and ensure that observed differences in performance reflect the intrinsic generalization capacity of each model rather than artifacts of tuning or setup changes.

% Define the objective and experimental setup with k-values and dataset size
The objective is to evaluate the ability of different architectures to generalize from a very limited number of examples per class (\textit{k-shot}), a common scenario in the biomedical domain where annotated data are often scarce or imbalanced. 
In our experiments, we vary \( k \) across the values \{1, 5, 10, 20, 30, 50, 60, 70, 80, 100, 150, 200, 400, 600\} to thoroughly assess model performance from extremely low-resource to moderately sized data regimes. 
This is done on a dataset of over 40,000 observations, allowing us to observe the transition of model capabilities as the amount of labeled data per class increases. These results are illustrated in Figure~\ref{fig:fs_shot} ~\ref{tab:test_f1s}, ~\ref{tab:test_balanced_accs},~\ref{tab:test_recalls}, ~\ref{tab:test_precisions}.

% Description of the figure layout and metrics used for evaluation
Figure~\ref{fig:fs_shot} provides an overview of the comparative performances of several models across five metrics: \textit{F1-score}, \textit{balanced accuracy}, \textit{recall}, \textit{precision}, and \textit{accuracy}. 
Each row of the graph represents a different model, and each column corresponds to an evaluation metric. The analysis of these results highlights several notable trends.

% Commentary on poor performing models and explanation of their behavior
First, the \textit{GRU+FastText} and \textit{CNN+LSTM} models exhibit generally low and poorly evolving performances. 
Their metrics remain close to random values, even as the number of examples \( k \) increases, reflecting a clear inability to generalize in a few-shot learning context. 
In particular, their \textit{F1-score}, \textit{balanced accuracy}, and \textit{recall} curves stagnate below 0.3 without significant inflection, even for \( k = 400 \) or \( k = 600 \). 
This stagnation may be attributed to these architectures’ limited capacity to extract meaningful features from small training samples or to effectively leverage pre-trained embeddings in this setup.

% Slight improvement but instability of another model
The \textit{GRU+GloVe} model shows a slight improvement as \( k \) increases, but remains very unstable and overall underperforming. 
Its \textit{precision} curve, in particular, shows significant oscillations, indicating difficulty in maintaining consistency from one sample to another. 
This could stem from sensitivity to noise or overfitting in low-data regimes, where small sample variations heavily impact model predictions.

% Models with more consistent improvements and interpretation
Conversely, the \textit{GRU-BaselineFastText} model stands out with a more regular and coherent improvement in performance as \( k \) increases, notably in \textit{F1-score} and \textit{recall}, which increase monotonically from \( k = 10 \). 
The \textit{balanced accuracy} remains moderate but also follows this upward trend. 
This model makes better use of small data volumes, although it reaches a performance plateau earlier (around \( k = 200 \)), with average scores around 0.6–0.65.
This suggests that while the model benefits from additional data, it may be limited by the representational capacity of the FastText embeddings or the architecture itself.

% Best performing model details and interpretation of results
However, it is the \textit{GRU-BaselineGloVe} model that establishes itself as the best performing in this scenario. 
It displays a clear and steady improvement across all metrics as \( k \) increases. 
Its \textit{F1-score} curve follows an almost linear growth between \( k = 10 \) and \( k = 400 \), reaching values close to 0.8. 
The \textit{balanced accuracy} also exceeds 0.8 for large values of \( k \), reflecting a balanced generalization capacity across classes. 
Interestingly, \textit{precision} remains remarkably stable, indicating that false positives also decrease as the data volume grows.
This reflects the model’s robustness in handling class imbalance and maintaining reliability even with limited data.

% Discussion on increasing performance gaps and implications for few-shot learning
It is also noteworthy that performance gaps between models become increasingly visible as \( k \) increases: while suboptimal models stagnate, the more robust architectures continue to improve. 
This suggests that a model’s ability to leverage additional data is a key criterion in few-shot environments and that architecture choice significantly impacts learning efficacy in scarce data regimes.

% Specific analysis of the BiGRU + Bahdanau + GloVe model and stability observations
Figure~\ref{fig:fs_shot} more precisely illustrates the performance evolution of the \textit{BiGRU with Bahdanau attention} and GloVe embeddings model in the multi-class few-shot learning setting. 
We observe a steady decrease in the loss function, accompanied by a progressive and consistent improvement in \textit{F1-score} and \textit{balanced accuracy} on the test set as \( k \) increases. 
These curves are smoother, indicating greater inter-experiment stability and well-controlled optimizer behavior.
This highlights the benefits of combining attention mechanisms with recurrent models and quality embeddings to enhance generalization under low-resource conditions.

% Final remarks on the significance of architecture and embeddings in few-shot scenarios
Ultimately, these results emphasize how the combined choice of model architecture and lexical representations is critical in multi-class few-shot learning contexts. 
When data are limited, simple or ill-suited models struggle to generalize effectively. 
In contrast, more advanced configurations — such as \textit{BiGRU + Bahdanau + GloVe} — manage to maintain solid performance despite the scarcity of examples. 
This confirms the benefit of associating specialized pre-trained embeddings with a recurrent architecture enhanced by an attention mechanism, an approach particularly well-suited to low-resource, high-categorical-complexity scenarios.

In summary, these findings directly address the research question regarding model generalization in low-resource biomedical text classification scenarios. The results demonstrate that advanced architectures, particularly the BiGRU with Bahdanau attention combined with high-quality embeddings, can effectively leverage limited labeled data to improve classification performance on underrepresented disease categories. This confirms the promise of few-shot learning methods in mitigating data scarcity challenges in biomedical NLP.

\begin{table}[H]
    \centering
    \scriptsize
    \setlength{\tabcolsep}{2pt}
    \caption{Few-shot performance for \texttt{F1-score} across models.}
    \label{tab:test_f1s}
    \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
    \hline
    \textbf{Model} & 1 & 5 & 10 & 20 & 30 & 50 & 60 & 70 & 80 & 100 & 150 & 200 & 400 & 600 \\ \hline
    GRU-Bah-Glove & 1.16\% & 2.86\% & 5.52\% & 4.06\% & 7.58\% & 14.25\% & 86.54\% & 72.22\% & 20.79\% & 72.10\% & 83.76\% & 78.10\% & 84.42\% & 86.60\% \\
    GRU-Bah-FastText & 2.76\% & 5.33\% & 6.37\% & 18.24\% & 12.54\% & 4.41\% & 5.38\% & 12.71\% & 87.75\% & 85.00\% & 71.19\% & 78.82\% & 87.16\% & 80.27\% \\
    CNN\_LSTM & 2.59\% & 0.86\% & 0.86\% & 0.96\% & 0.92\% & 0.99\% & 1.08\% & 0.86\% & 0.96\% & 0.93\% & 0.86\% & 0.89\% & 0.96\% & 1.23\% \\
    GRU-Glove & 2.48\% & 0.96\% & 0.89\% & 0.96\% & 0.93\% & 0.86\% & 0.93\% & 0.93\% & 0.89\% & 0.93\% & 1.03\% & 1.00\% & 81.60\% & 0.90\% \\
    GRU-FastText & 0.93\% & 1.03\% & 0.93\% & 0.93\% & 0.96\% & 1.03\% & 2.05\% & 0.93\% & 4.72\% & 2.49\% & 0.93\% & 9.10\% & 0.96\% & 77.56\% \\
    \hline
    \end{tabular}
\end{table}


\begin{table}[H]
    \centering
    \scriptsize
    \setlength{\tabcolsep}{2pt}
    \caption{Few-shot performance for \texttt{balanced accuracies} across models.}
    \label{tab:test_balanced_accs}
    \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
    \hline
    \textbf{Model} & 1 & 5 & 10 & 20 & 30 & 50 & 60 & 70 & 80 & 100 & 150 & 200 & 400 & 600 \\ \hline
    GRU-Bah-Glove & 11.03\% & 11.12\% & 13.85\% & 11.79\% & 15.91\% & 20.85\% & 86.53\% & 72.92\% & 27.01\% & 71.57\% & 83.32\% & 78.31\% & 84.51\% & 86.78\% \\
    GRU-Bah-FastText & 11.07\% & 11.78\% & 14.07\% & 23.00\% & 19.16\% & 13.29\% & 13.42\% & 21.02\% & 87.52\% & 85.01\% & 71.81\% & 77.85\% & 87.23\% & 79.95\% \\
    CNN\_LSTM & 11.13\% & 11.11\% & 11.11\% & 11.07\% & 11.06\% & 11.06\% & 11.08\% & 11.05\% & 11.05\% & 11.12\% & 11.11\% & 11.02\% & 11.07\% & 11.15\% \\
    GRU-Glove & 11.16\% & 11.10\% & 11.07\% & 11.10\% & 11.09\% & 11.05\% & 11.06\% & 11.11\% & 11.01\% & 11.06\% & 11.11\% & 11.15\% & 82.34\% & 11.10\% \\
    GRU-FastText & 11.06\% & 11.11\% & 11.06\% & 11.14\% & 11.07\% & 11.11\% & 11.35\% & 11.09\% & 13.17\% & 11.99\% & 11.09\% & 16.36\% & 11.10\% & 80.94\% \\
    \hline
    \end{tabular}
\end{table}


\begin{table}[H]
    \centering
    \scriptsize
    \setlength{\tabcolsep}{2pt}
    \caption{Few-shot performance for \texttt{recalls} across models.}
    \label{tab:test_recalls}
    \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
    \hline
    \textbf{Model} & 1 & 5 & 10 & 20 & 30 & 50 & 60 & 70 & 80 & 100 & 150 & 200 & 400 & 600 \\ \hline
    GRU-Bah-Glove & 6.80\% & 12.28\% & 11.49\% & 8.14\% & 12.38\% & 17.20\% & 86.05\% & 71.99\% & 23.97\% & 70.47\% & 83.43\% & 77.63\% & 84.05\% & 86.37\% \\
    GRU-Bah-FastText & 11.54\% & 10.52\% & 10.47\% & 19.47\% & 15.56\% & 9.05\% & 9.35\% & 16.99\% & 87.17\% & 84.68\% & 70.83\% & 76.92\% & 86.91\% & 79.30\% \\
    CNN\_LSTM & 11.81\% & 6.78\% & 6.78\% & 6.78\% & 6.76\% & 6.78\% & 6.81\% & 6.74\% & 6.76\% & 6.80\% & 6.78\% & 6.73\% & 6.78\% & 6.88\% \\
    GRU-Glove & 11.45\% & 6.80\% & 6.76\% & 6.80\% & 6.78\% & 6.74\% & 6.76\% & 6.80\% & 6.73\% & 6.76\% & 6.81\% & 6.83\% & 81.92\% & 6.78\% \\
    GRU-FastText & 6.76\% & 6.81\% & 6.76\% & 6.81\% & 6.78\% & 6.81\% & 7.28\% & 6.78\% & 8.97\% & 7.85\% & 6.78\% & 12.54\% & 6.80\% & 79.67\% \\
    \hline
    \end{tabular}
\end{table}


\begin{table}[H]
    \centering
    \scriptsize
    \setlength{\tabcolsep}{2pt}  % réduit l'espace horizontal entre colonnes (par défaut 6pt)
    \caption{Few-shot performance for \texttt{precisions} across models.}
    \label{tab:test_precisions}
    \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
    \hline
    \textbf{Model} & 1 & 5 & 10 & 20 & 30 & 50 & 60 & 70 & 80 & 100 & 150 & 200 & 400 & 600 \\ \hline
    GRU-Bah-Glove & 23.05\% & 4.45\% & 11.57\% & 9.99\% & 27.77\% & 33.10\% & 87.78\% & 73.32\% & 28.11\% & 77.82\% & 84.64\% & 78.94\% & 85.46\% & 87.28\% \\
    GRU-Bah-FastText & 7.45\% & 10.13\% & 16.91\% & 32.28\% & 42.01\% & 21.84\% & 23.76\% & 53.49\% & 89.07\% & 86.12\% & 72.59\% & 82.49\% & 87.95\% & 83.24\% \\
    CNN\_LSTM & 3.70\% & 0.46\% & 0.46\% & 2.12\% & 1.50\% & 4.76\% & 1.55\% & 0.46\% & 4.69\% & 1.27\% & 0.46\% & 1.12\% & 9.32\% & 14.75\% \\
    GRU-Glove & 10.16\% & 13.25\% & 11.88\% & 8.76\% & 9.56\% & 0.46\% & 5.35\% & 14.72\% & 6.55\% & 7.45\% & 16.69\% & 14.53\% & 81.58\% & 4.39\% \\
    GRU-FastText & 8.32\% & 23.39\% & 15.62\% & 17.97\% & 19.48\% & 23.01\% & 5.29\% & 18.76\% & 28.67\% & 8.89\% & 3.95\% & 20.55\% & 8.93\% & 84.71\% \\
    \hline
    \end{tabular}
\end{table}

\begin{table}[H]
    \centering
    \scriptsize
    \setlength{\tabcolsep}{2pt}
    \caption{Few-shot performance for \texttt{accuracies} across models.}
    \label{tab:test_accuracies}
    \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
    \hline
    \textbf{Model} & 1 & 5 & 10 & 20 & 30 & 50 & 60 & 70 & 80 & 100 & 150 & 200 & 400 & 600 \\ \hline
    GRU-Bah-Glove & 6.80\% & 12.28\% & 11.49\% & 8.14\% & 12.38\% & 17.20\% & 86.05\% & 71.99\% & 23.97\% & 70.47\% & 83.43\% & 77.63\% & 84.05\% & 86.37\% \\
    GRU-Bah-FastText & 11.54\% & 10.52\% & 10.47\% & 19.47\% & 15.56\% & 9.05\% & 9.35\% & 16.99\% & 87.17\% & 84.68\% & 70.83\% & 76.92\% & 86.91\% & 79.30\% \\
    CNN\_LSTM & 11.81\% & 6.78\% & 6.78\% & 6.78\% & 6.76\% & 6.78\% & 6.81\% & 6.74\% & 6.76\% & 6.80\% & 6.78\% & 6.73\% & 6.78\% & 6.88\% \\
    GRU-Glove & 11.45\% & 6.80\% & 6.76\% & 6.80\% & 6.78\% & 6.74\% & 6.76\% & 6.80\% & 6.73\% & 6.76\% & 6.81\% & 6.83\% & 81.92\% & 6.78\% \\
    GRU-FastText & 6.76\% & 6.81\% & 6.76\% & 6.81\% & 6.78\% & 6.81\% & 7.28\% & 6.78\% & 8.97\% & 7.85\% & 6.78\% & 12.54\% & 6.80\% & 79.67\% \\
    \hline
    \end{tabular}
\end{table}


\newpage

\chapter{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}

Automatic classification of biomedical texts remains a significant challenge in natural language processing due to the complexity of scientific language, class imbalance, and data scarcity for rare diseases. This thesis explored several deep learning architectures—LSTM, GRU, and bidirectional GRU with Bahdanau attention—on two classification tasks using abstracts from PubMed: a binary classification task (Malaria vs. Non-Malaria) and a multiclass task involving nine disease categories.

The results demonstrate that bidirectional GRUs with attention consistently outperform simpler architectures, particularly in terms of accuracy and the ability to handle complex sequences. However, these performance gains come at the cost of increased computational demands during both training and inference, raising concerns about their scalability. This highlights the importance of balancing predictive performance with computational efficiency, especially in resource-constrained environments.

Furthermore, the comparison of textual representations reveals that contextual embeddings pre-trained on biomedical corpora, such as BioBERT and PubMedBERT, significantly outperform general-purpose embeddings like GloVe and FastText. These domain-specific models better capture the semantic and terminological nuances of biomedical literature, though they require greater computational resources.

Finally, the evaluation of few-shot learning techniques demonstrates their potential to improve classification in low-resource settings, particularly for underrepresented diseases. Data augmentation and model adaptation to imbalanced scenarios prove essential for achieving robust generalization.

In summary, this study shows that combining GRU-based architectures with attention mechanisms and biomedical contextual embeddings represents a promising approach for biomedical text classification. However, practical deployment requires careful consideration of trade-offs between performance, computational cost, and data availability. Future research should prioritize optimizing these models for lightweight, interpretable, and adaptable use, enabling integration into clinical decision support systems and scientific monitoring platforms.

\newpage

\chapter{Perspectives}
\addcontentsline{toc}{chapter}{Perspectives}

\section*{Perspectives: BioWordVec, Prompt Engineering, Clinical Applications}
\addcontentsline{toc}{section}{Perspectives: BioWordVec, Prompt Engineering, Clinical Applications}

Several future research directions emerge to enhance and extend this work:

\paragraph{Integration of \gls{biowordvec}}  
The use of domain-specific embeddings such as \gls{biowordvec}, pretrained on large biomedical corpora, represents a promising path for enriching semantic representations. These word vectors, designed to capture domain-specific semantic relationships, offer a potential balance between traditional static embeddings and heavier contextual models. This approach could deliver strong performance while remaining computationally efficient, particularly in resource-constrained environments, without compromising the understanding of specialized biomedical language.

\paragraph{Prompt Engineering with Generative Models}  
The rise of large generative language models, such as GPT or T5, opens new avenues for biomedical text classification through prompt engineering. Instead of retraining complex models, these architectures can be rapidly and efficiently adapted by formulating specific instructions (prompts). This technique holds promise for significantly reducing the need for annotated data and computational resources, while enabling flexible and customizable classification aligned with clinical or research needs.

\paragraph{Collaborative Clinical Applications}  
The development of biomedical text classification systems must be accompanied by rigorous validation in collaboration with healthcare professionals. Such partnerships will help assess the relevance and reliability of models in real-world scenarios and ensure the integration of critical criteria such as interpretability and transparency. Tailoring models to the specific requirements of clinical practice—especially for decision-making and medical surveillance—will promote their adoption and impact within the healthcare domain.

\paragraph{Semi-supervised Learning and Multitask Transfer}  
To address the scarcity of annotated biomedical data, exploring semi-supervised learning and multitask transfer learning is an important direction. By leveraging unlabeled data and transferring knowledge across related tasks, these methods can improve model robustness and generalization, particularly for rare or emerging disease categories. This helps reduce dependency on costly annotated datasets while expanding the model’s applicability.

\paragraph{Model Compression and Optimization}  
To enable deployment in resource-constrained environments—such as embedded devices or healthcare facilities with limited infrastructure—techniques such as model compression, quantization, and distillation are essential. These approaches reduce model size and energy consumption with minimal impact on performance, facilitating wide-scale and practical deployment of biomedical NLP systems.

\paragraph{Ethics and Bias Mitigation}  
Finally, detecting, analyzing, and mitigating biases in both data and models remains a priority. Ensuring fairness and reliability in classification is critical, especially in the biomedical domain, where decisions can have direct consequences on patient health. Future work must follow a rigorous ethical framework to ensure transparency, accountability, and compliance with relevant standards, thereby fostering trust in automated systems.

This work thus lays the foundation for biomedical NLP systems that are powerful, practical, and ethically responsible—capable of supporting researchers and clinicians in the management and analysis of biomedical literature.




























%% Si besoin d'annexes
\newpage
\lhead{APPENDICES}
\chapter*{Appendices}
\addcontentsline{toc}{chapter}{Appendices} 

\section{Additional Tables}

\begin{table}[H]
\centering
\begin{tabular}{|l|p{10cm}|}
\hline
\textbf{Variable}       & \textbf{Description} \\ \hline
\texttt{PMID}           & Unique identifier of the article in the PubMed database. \\ \hline
\texttt{Title}          & Title of the scientific article. \\ \hline
\texttt{Abstract}       & Summary of the article containing key information about the study. \\ \hline
\texttt{Keywords}       & List of keywords associated with the article. \\ \hline
\texttt{PublicationYear}& Year of publication of the article. \\ \hline
\texttt{MeSH Terms}     & Terms from the medical thesaurus MeSH, enabling thematic indexing. \\ \hline
\texttt{Label}          & Class label associated with the article (used for classification). \\ \hline
\end{tabular}
\caption{Description of variables present in the datasets used.}
\label{tab:variables}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{|p{3.5cm}|c|p{8cm}|}
\hline
\textbf{Class} & \textbf{Label} & \textbf{Description} \\ \hline
Malaria & 1 & Articles related to malaria. \\ \hline
Non-Malaria (Alzheimer’s, Dengue) & 0 & Articles about diseases other than malaria (Alzheimer’s, Dengue). \\ \hline
\end{tabular}
\caption{Details of classes for the binary classification task.}
\label{tab:binary_classes}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Label} & \textbf{Disease}      & \textbf{Type}                              \\ \hline
0              & Tuberculosis          & Infectious                                 \\ \hline
1              & Cholera               & Infectious                                 \\ \hline
2              & Leprosy               & Infectious                                 \\ \hline
3              & Ebola                 & Infectious                                 \\ \hline
4              & Leukemia              & Non-infectious (Cancer)                    \\ \hline
5              & Asthma                & Non-infectious (Chronic Inflammatory)      \\ \hline
6              & Parkinson's disease   & Non-infectious (Neurodegenerative)         \\ \hline
7              & Lupus                 & Non-infectious (Autoimmune)                \\ \hline
8              & Cystic Fibrosis       & Non-infectious (Genetic)                   \\ \hline
\end{tabular}
\caption{Class details for the multiclass classification task.}
\label{tab:multiclass_classes}
\end{table}


\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.2}
\resizebox{1.05\textwidth}{!}{
\begin{tabular}{|l|l|c|c|l|c|r|}
\hline
\textbf{Model} & \textbf{Embedding} & \textbf{Hidden Dim} & \textbf{Dropout} & \textbf{Optimizer} & \textbf{LR} & \textbf{Params} \\
\hline
LSTM & Learned (200d) & 350 & 0.8 & RMSprop & 1e-3 & 8,773,551 \\
\hline
GRU  & Learned (200d) & 350 & 0.8 & Adam & 1e-3 & 8,580,351 \\
\hline
LSTM & FastText (300d) & 500 & 0.5 & AdamW & 5e-3 & 13,605,101 \\
\hline
GRU & FastText (300d) & 350 & 0.5 & RMSprop & 1e-3 & 12,685,551 \\
\hline
LSTM & GloVe (300d) & 450 & 0.3 & RMSprop & 4e-4 & 13,354,651 \\
\hline
GRU & GloVe (300d) & 350 & 0.8 & Adam & 1e-3 & 12,685,551 \\
\hline
BiGRU + Bahdanau & GloVe (300d) & 300 & 0.9 & RMSprop & 1e-4 & 13,265,401 \\
\hline
BiGRU + Bahdanau  & FastText (300d) & 350 & 0.8 & RMSprop & 1e-3 & 13,616,201 \\
\hline
BiGRU + Bahdanau & PubMedBERT (768d) & 256 & 0.6 & AdamW & 1e-4 & 1,708,033 \\
\hline
BiGRU + Bahdanau  & BioBERT (768d) & 256 & 0.6 & RMSprop & 1e-4 & 110,018,305 \\
\hline
\end{tabular}
}
\caption{Hyperparameters used for each model in the binary classification task. Loss function: Binary Cross-Entropy. Batch size: 16.}
\label{tab:model-config}
\end{table}

\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|c|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Embedding Type} & \textbf{Embedding Dim} & \textbf{Hidden Dim} & \textbf{Dropout} & \textbf{Num Layers} & \textbf{Optimizer} & \textbf{LR} & \textbf{Params} \\ \hline

CNN+LSTM       & Learned (from scratch)  & 250                    & 167                 & 0.3              & 1                   & AdamW              & 7e-4                        & 8,773,551 \\ \hline
CNN+GRU        & Learned (from scratch)  & 250                    & 167                 & 0.5              & 1                   & Adam               & 7e-4                        & 8,580,351 \\ \hline
LSTM           & Learned (from scratch)  & 250                    & 167                 & 0.5              & 1                   & Adam               & 9e-4                        & 8,773,551 \\ \hline
GRU            & Learned (from scratch)  & 250                    & 167                 & 0.5              & 1                   & Adam               & 1e-4                        & 8,580,351 \\ \hline

CNN+LSTM       & FastText (300d)         & 300                    & 450                 & 0.3              & 1                   & RMSprop            & 1e-3                        & 13,605,101 \\ \hline
CNN+GRU        & FastText (300d)         & 300                    & 300                 & 0.5              & 1                   & AdamW              & 1e-3                        & 12,685,551 \\ \hline
LSTM           & FastText (300d)         & 300                    & 450                 & 0.3              & 1                   & RMSprop            & 7e-3                        & 13,354,651 \\ \hline
GRU            & FastText (300d)         & 300                    & 300                 & 0.5              & 1                   & Adam               & 1e-3                        & 12,685,551 \\ \hline

CNN+LSTM       & GloVe (300d)            & 300                    & 250                 & 0.3              & 2                   & Adam               & 8e-4                        & 13,354,651 \\ \hline
CNN+GRU        & GloVe (300d)            & 300                    & 250                 & 0.3              & 1                   & Adam               & 7e-4                        & 12,685,551 \\ \hline
LSTM           & GloVe (300d)            & 300                    & 250                 & 0.3              & 1                   & Adam               & 7e-3                        & 13,354,651 \\ \hline
GRU            & GloVe (300d)            & 300                    & 250                 & 0.3              & 1                   & Adam               & 7e-4                        & 12,685,551 \\ \hline

BiGRU+Bahdanau & GloVe (300d)            & 300                    & 300                 & 0.9              & 1                   & RMSprop            & 9e-5                        & 13,265,401 \\ \hline
BiGRU+Bahdanau & FastText (300d)         & 300                    & 350                 & 0.8              & 1                   & RMSprop            & 1e-3                        & 13,616,201 \\ \hline
BiGRU+Bahdanau & PubMedBERT (768d)       & 768 (BERT)             & 256                 & 0.5              & 1                   & Adam               & 9e-5                        & 1,708,033 \\ \hline
BiGRU+Bahdanau & BioBERT (768d)          & 768 (BERT)             & 256                 & 0.5              & 1                   & AdamW              & 1e-4                        & 110,018,305 \\ \hline

\end{tabular}
}
\caption{Hyperparameters used for each model in the multi-class classification task. Loss function: CrossEntropyLoss. Batch size: 16.}
\label{tab:hyperparams_multiclass}
\end{table}

\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|c|c|c|c|c|c|c|}
\hline
\textbf{Model}             & \textbf{Embedding Type}   & \textbf{Accuracy} & \textbf{F1 Score} & \textbf{Balanced Acc} & \textbf{Precision} & \textbf{Recall} & \textbf{Training Time} & \textbf{Inference Time} \\ \hline
\textbf{CNN-LSTM}           & Learned (from scratch)    & 88.13\%           & 88.36\%           & 88.40\%              & 89.34\%            & 88.13\%         & 22.85                  & 0.04                   \\ \hline
\textbf{CNN+GRU}            & Learned (from scratch)    & 86.77\%           & 87.11\%           & 87.09\%              & 88.35\%            & 86.77\%         & 13.38                  & 0.02                   \\ \hline
\textbf{LSTM}               & Learned (from scratch)    & 88.13\%           & 88.36\%           & 88.46\%              & 88.88\%            & 88.13\%         & 9.03                   & 0.07                   \\ \hline
\textbf{GRU}                & Learned (from scratch)    & 87.74\%           & 87.78\%           & 88.00\%              & 88.00\%            & 87.74\%         & 9.09                   & 0.03                   \\ \hline
\textbf{CNN-LSTM}           & FastText (300d)           & \textbf{90.50\%}  & \textbf{90.49\%}  & \textbf{90.73\%}     & \textbf{90.58\%}   & \textbf{90.50\%}& \textbf{5.12}          & 0.06                   \\ \hline
\textbf{CNN-GRU}            & FastText (300d)           & 88.91\%           & 88.96\%           & 89.15\%              & 89.15\%            & 88.91\%         & 8.25                   & 0.03                   \\ \hline
\textbf{LSTM}               & FastText (300d)           & 80.49\%           & 80.65\%           & 81.67\%              & 89.21\%            & 80.49\%         & $\sim$11.0             & 0.09                   \\ \hline
\textbf{GRU}                & FastText (300d)           & 92.26\%           & 92.26\%           & 92.46\%              & 92.31\%            & 92.26\%         & 2.67                   & \textbf{0.06}          \\ \hline
\textbf{CNN-LSTM}           & GloVe (300d)              & 87.19\%           & 87.60\%           & 87.43\%              & 89.16\%            & 87.19\%         & 19.28                  & 0.06                   \\ \hline
\textbf{CNN-GRU}            & GloVe (300d)              & 87.62\%           & 87.72\%           & 87.89\%              & 88.33\%            & 87.62\%         & 12.74                  & 0.03                   \\ \hline
\textbf{LSTM}               & GloVe (300d)              & 89.17\%           & 89.25\%           & 89.51\%              & 89.51\%            & 89.17\%         & 41.73                  & 0.05                   \\ \hline
\textbf{GRU}                & GloVe (300d)              & 92.15\%           & 92.14\%           & 92.35\%              & 92.13\%            & 92.15\%         & \textbf{2.33}          & 0.05                   \\ \hline
\textbf{BiGRU + Bahdanau}   & GloVe (300d)              & 92.17\%           & 92.21\%           & 92.35\%              & 92.40\%            & 92.17\%         & 10.56                  & 0.21                   \\ \hline
\textbf{BiGRU + Bahdanau}   & FastText (300d)           & 92.80\%           & 92.89\%           & 93.04\%              & 93.14\%            & 92.80\%         & 9.96                   & 0.22                   \\ \hline
\textbf{BiGRU + Bahdanau}   & PubMedBERT                & 91.89\%           & \textbf{91.9\%}   & 92.16\%              & 91.96\%            & 91.89\%         & 33.33                  & 0.50                   \\ \hline
\textbf{BiGRU + Bahdanau}   & BioBERT                   & \textbf{92.23\%}  & \textbf{92.33\%}  & \textbf{92.01\%}     & \textbf{92.13\%}   & \textbf{92.21\%}& 35.66                  & 0.49                   \\ \hline
\end{tabular}
}
\caption{Performance metrics of multi-class classification models on the test set: Accuracy, F1 Score, Balanced Accuracy, Precision, Recall, Training and Inference Times.}
\label{tab:model_performance_multiclass}
\end{table}

\begin{table}[H]
\centering
\scriptsize
\caption{Performance of LSTM and GRU models with different resampling and weighting strategies (binary classification)}
\begin{tabular}{|l|l|c|c|c|c|}
\hline
\textbf{Training Strategy} & \textbf{Model} & \textbf{Accuracy} & \textbf{F1-score} & \textbf{Balanced Acc} & \textbf{Recall} \\
\hline
No resampling & LSTM & 90.70\% & 87.23\% & 90.46\% & 89.64\% \\
No resampling & GRU  & 91.74\% & 88.76\% & 91.81\% & 92.02\% \\
\hline
SMOTE & LSTM & 89.66\% & 86.40\% & 90.35\% & 92.72\% \\
SMOTE & GRU  & \textbf{92.04\%} & \textbf{89.09\%} & \textbf{91.96\%} & 91.67\% \\
\hline
Borderline-SMOTE & LSTM & 90.45\% & 87.25\% & 90.84\% & 92.16\% \\
Borderline-SMOTE & GRU  & 90.92\% & 87.87\% & 91.35\% & 92.79\% \\
\hline
Class Weights & LSTM & 90.13\% & 87.32\% & 91.43\% & \textbf{95.87\%} \\
Class Weights & GRU  & 91.97\% & 88.83\% & 91.55\% & 90.13\% \\
\hline
\end{tabular}
\vspace{0.5em}
\caption*{\textit{Note: All models trained with batch size = 16 and use GloVe (300d) embeddings.}}
\label{tab:performance_resampling_multi}
\end{table}

\section{Additional Figures}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{word_distribution_per_abstract_binary_class.png}
\caption{Distribution of word counts in abstracts for the binary classification task.}
\label{fig:word_distribution_binary_class}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{word_distribution_per_abstract_multi_class.png}
\caption{Distribution of word counts in abstracts for the multi-class classification task.}
\label{fig:word_distribution_multi_class}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fs_shot.png}
    \caption{Performance comparison of several models on multi-class few-shot learning tasks across varying numbers of examples per class \(k\). Metrics include F1-score, balanced accuracy, recall, precision, and accuracy.}
    \label{fig:fs_shot}
\end{figure}


\newpage
\begin{thebibliography}{99}

\bibitem{bahdanau2015neural}
D. Bahdanau, K. Cho, and Y. Bengio, 
\textit{Neural Machine Translation by Jointly Learning to Align and Translate}, 
International Conference on Learning Representations (ICLR), 2015. 
\url{https://arxiv.org/abs/1409.0473}

\bibitem{bengio1994learning}
Y. Bengio, et al., 
\textit{Learning long-term dependencies with gradient descent is difficult}, 
IEEE Transactions on Neural Networks, vol. 5, no. 2, pp. 157–166, 1994.

\bibitem{bojanowski2017enriching}
P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, 
\textit{Enriching word vectors with subword information}, 
Transactions of the Association for Computational Linguistics, vol. 5, pp. 135–146, 2017. 
\url{https://aclanthology.org/Q17-1010/}

\bibitem{brodersen2010balanced}
K. H. Brodersen, C. S. Ong, K. E. Stephan, and J. M. Buhmann, 
\textit{The balanced accuracy and its posterior distribution}, 
20th International Conference on Pattern Recognition, 2010.

\bibitem{cho2014learning}
K. Cho, B. van Merriënboer, D. Bahdanau, and Y. Bengio, 
\textit{Learning phrase representations using RNN encoder-decoder for statistical machine translation}, 
arXiv preprint arXiv:1406.1078, 2014. 
\url{https://arxiv.org/abs/1406.1078}

\bibitem{devlin2019bert}
J. Devlin, M. W. Chang, K. Lee, and K. Toutanova, 
\textit{BERT: Pre-training of deep bidirectional transformers for language understanding}, 
NAACL-HLT 2019, pp. 4171–4186, 2019. 
\url{https://arxiv.org/abs/1810.04805}

\bibitem{elman1990finding}
J. L. Elman, 
\textit{Finding structure in time}, 
Cognitive Science, vol. 14, no. 2, pp. 179–211, 1990.

\bibitem{finn2017model}
C. Finn, P. Abbeel, and S. Levine, 
\textit{Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks}, 
ICML 2017, pp. 1126–1135, 2017. 
\url{https://arxiv.org/abs/1703.03400}

\bibitem{gupta2021pubmedbert}
A. Gupta and A. Agarwal, 
\textit{PubMedBERT: A pre-trained biomedical language representation model for PubMed abstracts}, 
arXiv preprint arXiv:2102.09714, 2021. 
\url{https://arxiv.org/abs/2102.09714}

\bibitem{he2009learning}
H. He, X. L. Chen, and D. W. Song, 
\textit{Learning from imbalanced data}, 
Proceedings of the IEEE International Conference on Data Mining, 2009.

\bibitem{hochreiter1997long}
S. Hochreiter and J. Schmidhuber, 
\textit{Long short-term memory}, 
Neural Computation, vol. 9, no. 8, pp. 1735–1780, 1997.

\bibitem{lu2020pubmedbert}
Z. Lu et al., 
\textit{PubMedBERT: Pre-trained language model for biomedical text mining}, 
[Journal/Conference], 2020. 
\url{https://arxiv.org/abs/2007.XXX}

\bibitem{jia2019deep}
L. Jia, et al., 
\textit{Deep learning for medical text mining: A survey}, 
Journal of Biomedical Informatics, vol. 96, pp. 103–118, 2019.

\bibitem{jin2019recurrent}
Q. Jin, B. Dhingra, X. Liu, W. Cohen, and E. Hovy, 
\textit{Recurrent neural network models for disease name recognition using domain-invariant features}, 
In Proceedings of the BioNLP 2019 Workshop, pp. 1–10, 2019.

\bibitem{lee2020biobert}
J. Lee, W. Yoon, S. Kim, D. Kim, and C. H. So, 
\textit{BioBERT: A pre-trained biomedical language representation model for biomedical text mining}, 
Bioinformatics, vol. 36, no. 4, pp. 1234–1240, 2020. 
\url{https://doi.org/10.1093/bioinformatics/btz682}

\bibitem{mikolov2010recurrent}
T. Mikolov, M. Karafiát, L. Burget, J. Černocký, and S. Khudanpur, 
\textit{Recurrent neural network based language model}, 
In Interspeech, vol. 2, pp. 1045–1048, 2010.

\bibitem{mikolov2018advances}
T. Mikolov and others, 
\textit{Advances in pretraining models}, 
Journal of Machine Learning, vol. 10, pp. 1–10, 2018.

\bibitem{pennington2014glove}
J. Pennington, R. Socher, and C. D. Manning, 
\textit{GloVe: Global Vectors for Word Representation}, 
In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1532–1543, 2014. 
\url{https://aclanthology.org/D14-1162/}

\bibitem{snell2017prototypical}
J. Snell, K. Swersky, and R. Zemel, 
\textit{Prototypical Networks for Few-shot Learning}, 
NeurIPS 2017, pp. 4077–4087, 2017. 
\url{https://arxiv.org/abs/1703.05175}

\bibitem{vinyals2016matching}
O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, and D. Silver, 
\textit{Matching Networks for One-Shot Learning}, 
NeurIPS 2016, pp. 3630–3638, 2016. 
\url{https://arxiv.org/abs/1606.04080}

\bibitem{yin2017comparative}
W. Yin, K. Kann, M. Yu, and H. Schütze, 
\textit{Comparative study of CNN and RNN for natural language processing}, 
arXiv preprint arXiv:1702.01923, 2017. 
\url{https://arxiv.org/abs/1702.01923}

\bibitem{zhang2020biowordvec}
Y. Zhang, Q. Chen, Z. Yang, H. Lin, and Z. Lu, 
\textit{BioWordVec, improving biomedical word embeddings with subword information and MeSH}, 
Scientific Data, vol. 6, no. 52, 2020. 
\url{https://doi.org/10.1038/s41597-019-0055-0}

\bibitem{saito2015precision}
T. Saito and M. Rehmsmeier, 
\textit{The Precision-Recall Plot is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets}, 
PLOS ONE, vol. 10, no. 3, 2015.

\bibitem{japkowicz2002class}
N. Japkowicz and S. Stephen, 
\textit{The class imbalance problem: A systematic study}, 
Intelligent Data Analysis, vol. 6, no. 5, pp. 429–449, 2002.

\bibitem{chawla2002smote}
Chawla, N.V., Bowyer, K.W., Hall, L.O., \& Kegelmeyer, W.P. (2002).
SMOTE: Synthetic Minority Over-sampling Technique.
\textit{Journal of Artificial Intelligence Research}, \textbf{16}, 321--357.

\bibitem{han2005borderline}
Han, H., Wang, W.Y., \& Mao, B.H. (2005).
Borderline-SMOTE: A New Over-Sampling Method in Imbalanced Data Sets Learning.
In \textit{Proceedings of the International Conference on Intelligent Computing}, Springer, 878--887.

\bibitem{Chawla2002SMOTE} Chawla, N. V., \textit{SMOTE: Synthetic Minority Over-sampling Technique}, Journal of Artificial Intelligence Research, 2002.

\bibitem{Han2005Borderline} Han, H., Wang, W. Y., and Mao, B. H., \textit{Borderline-SMOTE: A New Over-Sampling Method in Imbalanced Data Sets Learning}, Proceedings of the International Conference on Intelligent Computing, 2005.

\bibitem{King2001Logit}
G. King and L. Zeng, 
\textit{Logistic Regression in Rare Events Data}, 
Political Analysis, vol. 9, no. 2, pp. 137–163, 2001.

\bibitem{Lin2017Focal}
T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár,
\textit{Focal Loss for Dense Object Detection}, 
Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 2980–2988, 2017.

\bibitem{van2008visualizing}
L.J.P. van der Maaten and G.E. Hinton,
\textit{Visualizing data using t-SNE},
Journal of Machine Learning Research, vol. 9, pp. 2579–2605, 2008.

\bibitem{zhou2015text}
P. Zhou, Z. Qi, S. Zheng, J. Xu, H. Bao, and B. Xu, 
\textit{Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling}, 
Proceedings of the 26th International Conference on Computational Linguistics (COLING), 2016, pp. 3485–3495. 
\url{https://aclanthology.org/C16-1329}

\bibitem{kim2014convolutional}
Y. Kim,
"Convolutional Neural Networks for Sentence Classification,"
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014.
\url{https://arxiv.org/abs/1408.5882}

\bibitem{zhou2015clstm}
C. Zhou, C. Sun, Z. Liu, and F. Lau,
"A C-LSTM Neural Network for Text Classification,"
arXiv preprint, 2015.
\url{https://arxiv.org/abs/1511.08630}

\bibitem{gu2020domain}
Y. Gu, R. Tinn, H. Cheng, M. Lucas, N. Usuyama, X. Liu, T. Naumann, J. Gao, H. Poon, "Domain-specific language model pretraining for biomedical natural language processing," ACM Transactions on Computing for Healthcare (HEALTH), vol. 3, no. 1, pp. 1–23, 2020. \url{https://arxiv.org/abs/2007.15779}

\bibitem{geurts2006} 
Geurts, P., \& Wehenkel, L. (2006). \textit{Learning with Decision Trees and Random Forests}. Institut Montefiore, University of Liège, Belgium.

\bibitem{prechelt1998early}
L. Prechelt,
\textit{Early Stopping — But When?},
In Neural Networks: Tricks of the Trade, Springer, 1998, pp. 55–69.
\url{https://doi.org/10.1007/3-540-49430-8_3}

\bibitem{yao2020automl}
Q. Yao, M. Wang, Y. Chen, W. Dai, Y. Li, and Q. Yang,
\textit{Taking Human Out of Learning Applications: A Survey on Automated Machine Learning},
ACM Computing Surveys (CSUR), 2020, vol. 53, no. 6, pp. 1–37.
\url{https://doi.org/10.1145/3398031}


\end{thebibliography}

\end{document}
